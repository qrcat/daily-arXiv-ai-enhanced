{"id": "2508.09235", "categories": ["cs.GR", "cs.CG"], "pdf": "https://arxiv.org/pdf/2508.09235", "abs": "https://arxiv.org/abs/2508.09235", "authors": ["Nathaniel Gorski", "Xin Liang", "Hanqi Guo", "Bei Wang"], "title": "TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields", "comment": "29 pages, 27 figures, to be presented at IEEE Vis 2025 (and published\n  in IEEE TVCG 2026)", "summary": "In this paper, we present a novel compression framework, TFZ, that preserves\nthe topology of 2D symmetric and asymmetric second-order tensor fields defined\non flat triangular meshes. A tensor field assigns a tensor - a\nmulti-dimensional array of numbers - to each point in space. Tensor fields,\nsuch as the stress and strain tensors, and the Riemann curvature tensor, are\nessential to both science and engineering. The topology of tensor fields\ncaptures the core structure of data, and is useful in various disciplines, such\nas graphics (for manipulating shapes and textures) and neuroscience (for\nanalyzing brain structures from diffusion MRI). Lossy data compression may\ndistort the topology of tensor fields, thus hindering downstream analysis and\nvisualization tasks. TFZ ensures that certain topological features are\npreserved during lossy compression. Specifically, TFZ preserves degenerate\npoints essential to the topology of symmetric tensor fields and retains\neigenvector and eigenvalue graphs that represent the topology of asymmetric\ntensor fields. TFZ scans through each cell, preserving the local topology of\neach cell, and thereby ensuring certain global topological guarantees. We\nshowcase the effectiveness of our framework in enhancing the lossy scientific\ndata compressors SZ3 and SPERR.", "AI": {"tldr": "TFZ\u662f\u4e00\u79cd\u65b0\u578b\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e8c\u7ef4\u5bf9\u79f0\u548c\u975e\u5bf9\u79f0\u4e8c\u9636\u5f20\u91cf\u573a\u4e2d\u4fdd\u7559\u62d3\u6251\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u3002", "motivation": "\u5f20\u91cf\u573a\u7684\u62d3\u6251\u7ed3\u6784\u5bf9\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u53ef\u80fd\u7834\u574f\u8fd9\u4e9b\u7ed3\u6784\u3002", "method": "TFZ\u901a\u8fc7\u9010\u5355\u5143\u626b\u63cf\u5e76\u4fdd\u7559\u5c40\u90e8\u62d3\u6251\u7279\u5f81\uff0c\u786e\u4fdd\u5168\u5c40\u62d3\u6251\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u3002", "result": "TFZ\u6210\u529f\u63d0\u5347\u4e86SZ3\u548cSPERR\u7b49\u79d1\u5b66\u6570\u636e\u538b\u7f29\u5668\u7684\u6027\u80fd\u3002", "conclusion": "TFZ\u6846\u67b6\u5728\u4fdd\u7559\u5f20\u91cf\u573a\u62d3\u6251\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.09610", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.09610", "abs": "https://arxiv.org/abs/2508.09610", "authors": ["Jiachen Li", "Guangzhi Han", "Jin Wan", "Yuan Gao", "Delong Han"], "title": "DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction", "comment": "12 pages, 4 figures", "summary": "In 3D reconstruction of underwater scenes, traditional methods based on\natmospheric optical models cannot effectively deal with the selective\nattenuation of light wavelengths and the effect of suspended particle\nscattering, which are unique to the water medium, and lead to color distortion,\ngeometric artifacts, and collapsing phenomena at long distances. We propose the\nDualPhys-GS framework to achieve high-quality underwater reconstruction through\na dual-path optimization mechanism. Our approach further develops a dual\nfeature-guided attenuation-scattering modeling mechanism, the RGB-guided\nattenuation optimization model combines RGB features and depth information and\ncan handle edge and structural details. In contrast, the multi-scale\ndepth-aware scattering model captures scattering effects at different scales\nusing a feature pyramid network and an attention mechanism. Meanwhile, we\ndesign several special loss functions. The attenuation scattering consistency\nloss ensures physical consistency. The water body type adaptive loss\ndynamically adjusts the weighting coefficients. The edge-aware scattering loss\nis used to maintain the sharpness of structural edges. The multi-scale feature\nloss helps to capture global and local structural information. In addition, we\ndesign a scene adaptive mechanism that can automatically identify the\nwater-body-type characteristics (e.g., clear coral reef waters or turbid\ncoastal waters) and dynamically adjust the scattering and attenuation\nparameters and optimization strategies. Experimental results show that our\nmethod outperforms existing methods in several metrics, especially in suspended\nmatter-dense regions and long-distance scenes, and the reconstruction quality\nis significantly improved.", "AI": {"tldr": "DualPhys-GS\u6846\u67b6\u901a\u8fc7\u53cc\u8def\u5f84\u4f18\u5316\u673a\u5236\u89e3\u51b3\u6c34\u4e0b\u573a\u666f3D\u91cd\u5efa\u4e2d\u7684\u989c\u8272\u5931\u771f\u548c\u51e0\u4f55\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5927\u6c14\u5149\u5b66\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6c34\u4e0b\u5149\u6ce2\u957f\u9009\u62e9\u6027\u8870\u51cf\u548c\u60ac\u6d6e\u7c92\u5b50\u6563\u5c04\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u7279\u5f81\u5f15\u5bfc\u7684\u8870\u51cf-\u6563\u5c04\u5efa\u6a21\u673a\u5236\uff0c\u7ed3\u5408RGB\u7279\u5f81\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u8bbe\u8ba1\u7279\u6b8a\u635f\u5931\u51fd\u6570\u548c\u573a\u666f\u81ea\u9002\u5e94\u673a\u5236\u3002", "result": "\u5728\u60ac\u6d6e\u7269\u5bc6\u96c6\u533a\u57df\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\u4e2d\uff0c\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DualPhys-GS\u6846\u67b6\u6709\u6548\u63d0\u5347\u6c34\u4e0b3D\u91cd\u5efa\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.09830", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09830", "abs": "https://arxiv.org/abs/2508.09830", "authors": ["Shenxing Wei", "Jinxi Li", "Yafei Yang", "Siyuan Zhou", "Bo Yang"], "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians", "comment": "ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code\n  and data are available at: https://github.com/vLAR-group/RayletDF", "summary": "In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRayletDF\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u70b9\u4e91\u62163DGS\u9884\u4f30\u8ba1\u76843D\u9ad8\u65af\u5206\u5e03\u4e2d\u91cd\u5efa3D\u8868\u9762\uff0c\u901a\u8fc7\u5c04\u7ebf\u8ddd\u79bb\u573a\u76f4\u63a5\u9884\u6d4b\u8868\u9762\u70b9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5750\u6807\u7684\u65b9\u6cd5\u5728\u6e32\u67d3\u663e\u5f0f\u8868\u9762\u65f6\u8ba1\u7b97\u91cf\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c04\u7ebf\u7279\u5f81\u63d0\u53d6\u5668\u3001\u5c04\u7ebf\u8ddd\u79bb\u573a\u9884\u6d4b\u5668\u548c\u591a\u5c04\u7ebf\u6df7\u5408\u5668\u4e09\u4e2a\u6a21\u5757\uff0c\u63d0\u53d6\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\u3001\u9884\u6d4b\u8ddd\u79bb\u5e76\u805a\u5408\u7ed3\u679c\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5355\u6b21\u524d\u5411\u4f20\u64ad\u91cd\u5efa3D\u8868\u9762\u3002", "conclusion": "RayletDF\u57283D\u8868\u9762\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09638", "categories": ["cs.CG", "cs.DC", "F.2.2"], "pdf": "https://arxiv.org/pdf/2508.09638", "abs": "https://arxiv.org/abs/2508.09638", "authors": ["Irina Kostitsyna", "David Liedtke", "Christian Scheideler"], "title": "Distributed Diamond Formation of Sliding Squares", "comment": "29 pages, 11 figures, 34 subfigures", "summary": "The sliding square model is a widely used abstraction for studying\nself-reconfigurable robotic systems, where modules are square-shaped robots\nthat move by sliding or rotating over one another. In this paper, we propose a\nnovel distributed algorithm that allows a group of modules to reconfigure into\na diamond shape, starting from an arbitrary side-connected configuration. It is\nconnectivity-preserving and operates under minimal assumptions: one leader\nmodule, common chirality, constant memory per module, and visibility and\ncommunication restricted to immediate neighbors. Unlike prior work, which\nrelaxes the original sliding square move-set, our approach uses the unmodified\nmove-set, addressing the additional challenge of handling locked\nconfigurations. Our algorithm is sequential in nature and operates with a\nworst-case time complexity of $\\mathcal{O}(n^2)$ rounds, which is optimal for\nsequential algorithms. To improve runtime, we introduce two parallel variants\nof the algorithm. Both rely on a spanning tree data structure, allowing modules\nto make decisions based on local connectivity. Our experimental results show a\nsignificant speedup for the first variant, and linear average runtime for the\nsecond variant, which is worst-case optimal for parallel algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u4f7f\u65b9\u5f62\u6a21\u5757\u673a\u5668\u4eba\u80fd\u5728\u4fdd\u6301\u8fde\u901a\u6027\u7684\u60c5\u51b5\u4e0b\u91cd\u65b0\u914d\u7f6e\u4e3a\u83f1\u5f62\uff0c\u4f7f\u7528\u539f\u59cb\u6ed1\u52a8\u79fb\u52a8\u65b9\u5f0f\uff0c\u5e76\u4f18\u5316\u4e86\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5e76\u884c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6a21\u5757\u7684\u91cd\u65b0\u914d\u7f6e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u8fde\u901a\u6027\u548c\u4f7f\u7528\u539f\u59cb\u79fb\u52a8\u65b9\u5f0f\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u9501\u5b9a\u914d\u7f6e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u57fa\u4e8e\u4e00\u4e2a\u9886\u5bfc\u6a21\u5757\u3001\u5171\u540c\u624b\u6027\u3001\u6052\u5b9a\u5185\u5b58\u548c\u5c40\u90e8\u53ef\u89c1\u6027/\u901a\u4fe1\u7684\u5047\u8bbe\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u5e76\u884c\u53d8\u4f53\u4ee5\u63d0\u9ad8\u8fd0\u884c\u6548\u7387\u3002", "result": "\u7b97\u6cd5\u5728\u4fdd\u6301\u8fde\u901a\u6027\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u5c06\u6a21\u5757\u91cd\u65b0\u914d\u7f6e\u4e3a\u83f1\u5f62\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n\u00b2)\uff0c\u5e76\u884c\u53d8\u4f53\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u4fdd\u6301\u539f\u59cb\u79fb\u52a8\u65b9\u5f0f\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u9501\u5b9a\u914d\u7f6e\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u5316\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6027\u80fd\uff0c\u4e3a\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09983", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09983", "abs": "https://arxiv.org/abs/2508.09983", "authors": ["David Dinkevich", "Matan Levy", "Omri Avrahami", "Dvir Samuel", "Dani Lischinski"], "title": "Story2Board: A Training-Free Approach for Expressive Storyboard Generation", "comment": "Project page is available at\n  https://daviddinkevich.github.io/Story2Board/", "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.", "AI": {"tldr": "Story2Board\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6545\u4e8b\u677f\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u673a\u5236\u63d0\u5347\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u4e3b\u9898\u8eab\u4efd\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u53d9\u4e8b\u7684\u5173\u952e\u65b9\u9762\uff08\u5982\u7a7a\u95f4\u6784\u56fe\u3001\u80cc\u666f\u6f14\u53d8\u548c\u53d9\u4e8b\u8282\u594f\uff09\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u5305\u62ec\u6f5c\u5728\u9762\u677f\u951a\u5b9a\u548c\u4e92\u6ce8\u610f\u529b\u503c\u6df7\u5408\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u6216\u5fae\u8c03\u3002", "result": "\u5728\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u751f\u6210\u66f4\u5177\u52a8\u6001\u6027\u548c\u53d9\u4e8b\u5438\u5f15\u529b\u7684\u6545\u4e8b\u677f\u3002", "conclusion": "Story2Board\u5728\u89c6\u89c9\u53d9\u4e8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6545\u4e8b\u677f\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.09734", "categories": ["cs.CG"], "pdf": "https://arxiv.org/pdf/2508.09734", "abs": "https://arxiv.org/abs/2508.09734", "authors": ["Sarita de Berg", "Jacobus Conradi", "Ivor van der Hoog", "Frank Staals"], "title": "Simpler and Faster Contiguous Art Gallery", "comment": null, "summary": "The contiguous art gallery problem was introduced at SoCG'25 in a merged\npaper that combined three simultaneous results, each achieving a\npolynomial-time algorithm for the problem. This problem is a variant of the\nclassical art gallery problem, first introduced by Klee in 1973. In the\ncontiguous art gallery problem, we are given a polygon P and asked to determine\nthe minimum number of guards needed, where each guard is assigned a contiguous\nportion of the boundary of P that it can see, such that all assigned portions\ntogether cover the boundary of P. The classical art gallery problem is NP-hard\nand ER-complete, and the three independent works investigated whether this\nvariant admits a polynomial-time solution. Each of these works indeed presented\nsuch a solution, with the fastest running in O(k n^5 log n) time, where n\ndenotes the number of vertices of P and k is the size of a minimum guard set\ncovering the boundary of P. We present a solution that is both considerably\nsimpler and significantly faster, yielding a concise and almost entirely\nself-contained O(k n^2 log^2 n)-time algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u66f4\u5feb\u7684\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u827a\u672f\u753b\u5eca\u95ee\u9898\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(k n^2 log^2 n)\u3002", "motivation": "\u8fde\u7eed\u827a\u672f\u753b\u5eca\u95ee\u9898\u662f\u7ecf\u5178\u827a\u672f\u753b\u5eca\u95ee\u9898\u7684\u53d8\u4f53\uff0c\u7ecf\u5178\u95ee\u9898\u5df2\u88ab\u8bc1\u660e\u662fNP\u96be\u548cER\u5b8c\u5168\u7684\u3002\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u8be5\u53d8\u4f53\u662f\u5426\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u6d01\u4e14\u51e0\u4e4e\u5b8c\u5168\u81ea\u5305\u542b\u7684\u7b97\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(k n^2 log^2 n)\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u6bd4\u4e4b\u524d\u66f4\u7b80\u5355\u4e14\u66f4\u5feb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fde\u7eed\u827a\u672f\u753b\u5eca\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.09175", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09175", "abs": "https://arxiv.org/abs/2508.09175", "authors": ["Mohammad Zia Ur Rehman", "Sufyaan Zahoor", "Areeb Manzoor", "Musharaf Maqbool", "Nagendra Kumar"], "title": "A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection", "comment": "Published in Information Processing & Management", "summary": "A substantial portion of offensive content on social media is directed\ntowards women. Since the approaches for general offensive content detection\nface a challenge in detecting misogynistic content, it requires solutions\ntailored to address offensive content against women. To this end, we propose a\nnovel multimodal framework for the detection of misogynistic and sexist\ncontent. The framework comprises three modules: the Multimodal Attention module\n(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the\nContent-specific Features Learning Module (CFLM). The MANM employs adaptive\ngating-based multimodal context-aware attention, enabling the model to focus on\nrelevant visual and textual information and generating contextually relevant\nfeatures. The GFRM module utilizes graphs to refine features within individual\nmodalities, while the CFLM focuses on learning text and image-specific features\nsuch as toxicity features and caption features. Additionally, we curate a set\nof misogynous lexicons to compute the misogyny-specific lexicon score from the\ntext. We apply test-time augmentation in feature space to better generalize the\npredictions on diverse inputs. The performance of the proposed approach has\nbeen evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and\n13,494 samples, respectively. The proposed method demonstrates an average\nimprovement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI\nand MMHS150K datasets, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u9488\u5bf9\u5973\u6027\u7684\u538c\u5973\u548c\u6027\u522b\u6b67\u89c6\u5185\u5bb9\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\uff08MANM\u3001GFRM\u3001CFLM\uff09\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u9488\u5bf9\u5973\u6027\u7684\u653b\u51fb\u6027\u5185\u5bb9\u8f83\u591a\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u538c\u5973\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5305\u542bMANM\uff08\u591a\u6a21\u6001\u6ce8\u610f\u529b\uff09\u3001GFRM\uff08\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u91cd\u6784\uff09\u548cCFLM\uff08\u5185\u5bb9\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\uff09\uff0c\u5e76\u7ed3\u5408\u538c\u5973\u8bcd\u5178\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728MAMI\u548cMMHS150K\u6570\u636e\u96c6\u4e0a\uff0c\u5b8fF1\u5206\u522b\u5e73\u5747\u63d0\u534710.17%\u548c8.88%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u538c\u5973\u548c\u6027\u522b\u6b67\u89c6\u5185\u5bb9\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.09304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09304", "abs": "https://arxiv.org/abs/2508.09304", "authors": ["Kelen C. Teixeira Vivaldini", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Decision-Making-Based Path Planning for Autonomous UAVs: A Survey", "comment": null, "summary": "One of the most critical features for the successful operation of autonomous\nUAVs is the ability to make decisions based on the information acquired from\ntheir surroundings. Each UAV must be able to make decisions during the flight\nin order to deal with uncertainties in its system and the environment, and to\nfurther act upon the information being received. Such decisions influence the\nfuture behavior of the UAV, which is expressed as the path plan. Thus,\ndecision-making in path planning is an enabling technique for deploying\nautonomous UAVs in real-world applications. This survey provides an overview of\nexisting studies that use aspects of decision-making in path planning,\npresenting the research strands for Exploration Path Planning and Informative\nPath Planning, and focusing on characteristics of how data have been modeled\nand understood. Finally, we highlight the existing challenges for relevant\ntopics in this field.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u81ea\u4e3b\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u51b3\u7b56\u65b9\u6cd5\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u63a2\u7d22\u8def\u5f84\u89c4\u5212\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u3002", "motivation": "\u81ea\u4e3b\u65e0\u4eba\u673a\u9700\u57fa\u4e8e\u73af\u5883\u4fe1\u606f\u505a\u51fa\u51b3\u7b56\u4ee5\u5e94\u5bf9\u7cfb\u7edf\u4e0e\u73af\u5883\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51b3\u7b56\u80fd\u529b\u662f\u5176\u6210\u529f\u8fd0\u884c\u7684\u5173\u952e\u3002", "method": "\u7efc\u8ff0\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u51b3\u7b56\u5728\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6570\u636e\u5efa\u6a21\u548c\u7406\u89e3\u7684\u7279\u70b9\u3002", "result": "\u603b\u7ed3\u4e86\u63a2\u7d22\u8def\u5f84\u89c4\u5212\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u6307\u51fa\u4e86\u76f8\u5173\u9886\u57df\u7684\u6311\u6218\u3002", "conclusion": "\u51b3\u7b56\u80fd\u529b\u662f\u81ea\u4e3b\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u7684\u6838\u5fc3\u6280\u672f\uff0c\u672a\u6765\u9700\u89e3\u51b3\u73b0\u6709\u6311\u6218\u4ee5\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.09909", "categories": ["cs.CG"], "pdf": "https://arxiv.org/pdf/2508.09909", "abs": "https://arxiv.org/abs/2508.09909", "authors": ["Gabriele Paolini", "Claudio Tortorici", "Stefano Berretti", "Ahmed Hazem Youssef", "Halim Benhabiles", "Adnane Cabani", "Ruiwen He", "Karim Hammoudi", "Iyyakutti Iyappan Ganapathi", "Syed Sadaf Ali", "Divya Velayudhan", "Maregu Assefa", "Naoufel Werghi"], "title": "SHREC'25 Track on Multiple Relief Patterns: Report and Analysis", "comment": "12 pages, 8 figures", "summary": "This SHREC 2025 track focuses on the recognition and segmentation of relief\npatterns embedded on the surface of a set of synthetically generated triangle\nmeshes. We report the methods proposed by the participants, whose performance\nhighlights the inherent complexity of solving the problem, which is still open.\nThen, we discuss the critical aspects of the proposed tasks, highlight the\nlimitations of current techniques, and outline possible directions for future\nresearch. All resources and track details are available at the official track\nwebpage: https://sites.google.com/unifi.it/shrec25-relief-pattern.", "AI": {"tldr": "SHREC 2025 \u8d5b\u9053\u4e13\u6ce8\u4e8e\u8bc6\u522b\u548c\u5206\u5272\u5d4c\u5165\u5728\u5408\u6210\u4e09\u89d2\u5f62\u7f51\u683c\u8868\u9762\u4e0a\u7684\u6d6e\u96d5\u56fe\u6848\uff0c\u603b\u7ed3\u4e86\u53c2\u4e0e\u8005\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u6d6e\u96d5\u56fe\u6848\u8bc6\u522b\u548c\u5206\u5272\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u53c2\u4e0e\u8005\u63d0\u51fa\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u5177\u4f53\u65b9\u6cd5\u672a\u8be6\u7ec6\u8bf4\u660e\u3002", "result": "\u53c2\u4e0e\u8005\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u660e\u8be5\u95ee\u9898\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4efb\u52a1\u7684\u5173\u952e\u70b9\u3001\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.09178", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09178", "abs": "https://arxiv.org/abs/2508.09178", "authors": ["Yanhui Li", "Yunkang Cao", "Chengliang Liu", "Yuan Xiong", "Xinghui Dong", "Chao Huang"], "title": "IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection is a critical component of modern manufacturing,\nyet the scarcity of defective samples restricts traditional detection methods\nto scenario-specific applications. Although Vision-Language Models (VLMs)\ndemonstrate significant advantages in generalization capabilities, their\nperformance in industrial anomaly detection remains limited. To address this\nchallenge, we propose IAD-R1, a universal post-training framework applicable to\nVLMs of different architectures and parameter scales, which substantially\nenhances their anomaly detection capabilities. IAD-R1 employs a two-stage\ntraining strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)\nstage utilizes a meticulously constructed high-quality Chain-of-Thought dataset\n(Expert-AD) for training, enhancing anomaly perception capabilities and\nestablishing reasoning-to-answer correlations; the Structured Control Group\nRelative Policy Optimization (SC-GRPO) stage employs carefully designed reward\nfunctions to achieve a capability leap from \"Anomaly Perception\" to \"Anomaly\nInterpretation\". Experimental results demonstrate that IAD-R1 achieves\nsignificant improvements across 7 VLMs, attaining up to 43.3% enhancement in\naverage accuracy on 6 industrial anomaly detection benchmark datasets. Notably,\nthe 0.5B parameter model trained with IAD-R1 surpasses commercial models\nincluding GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the\neffectiveness and superiority of IAD-R1. The dataset, code, and all model\nweights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.", "AI": {"tldr": "IAD-R1\u662f\u4e00\u4e2a\u901a\u7528\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e0d\u540c\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7f3a\u9677\u6837\u672c\u7a00\u7f3a\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u6709\u4f18\u52bf\u4f46\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1aPA-SFT\u9636\u6bb5\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u589e\u5f3a\u5f02\u5e38\u611f\u77e5\u80fd\u529b\uff1bSC-GRPO\u9636\u6bb5\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u4ece\u611f\u77e5\u5230\u89e3\u91ca\u7684\u98de\u8dc3\u3002", "result": "\u57287\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534743.3%\uff0c\u5c0f\u6a21\u578b\u8868\u73b0\u8d85\u8d8a\u5546\u4e1a\u5927\u6a21\u578b\u3002", "conclusion": "IAD-R1\u6846\u67b6\u6709\u6548\u4e14\u4f18\u8d8a\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.09346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09346", "abs": "https://arxiv.org/abs/2508.09346", "authors": ["Zhenjiang Mao", "Mrinall Eashaan Umasudhan", "Ivan Ruchkin"], "title": "How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy", "comment": null, "summary": "Autonomous robots that rely on deep neural network controllers pose critical\nchallenges for safety prediction, especially under partial observability and\ndistribution shift. Traditional model-based verification techniques are limited\nin scalability and require access to low-dimensional state models, while\nmodel-free methods often lack reliability guarantees. This paper addresses\nthese limitations by introducing a framework for calibrated safety prediction\nin end-to-end vision-controlled systems, where neither the state-transition\nmodel nor the observation model is accessible. Building on the foundation of\nworld models, we leverage variational autoencoders and recurrent predictors to\nforecast future latent trajectories from raw image sequences and estimate the\nprobability of satisfying safety properties. We distinguish between monolithic\nand composite prediction pipelines and introduce a calibration mechanism to\nquantify prediction confidence. In long-horizon predictions from\nhigh-dimensional observations, the forecasted inputs to the safety evaluator\ncan deviate significantly from the training distribution due to compounding\nprediction errors and changing environmental conditions, leading to\nmiscalibrated risk estimates. To address this, we incorporate unsupervised\ndomain adaptation to ensure robustness of safety evaluation under distribution\nshift in predictions without requiring manual labels. Our formulation provides\ntheoretical calibration guarantees and supports practical evaluation across\nlong prediction horizons. Experimental results on three benchmarks show that\nour UDA-equipped evaluators maintain high accuracy and substantially lower\nfalse positive rates under distribution shift. Similarly, world model-based\ncomposite predictors outperform their monolithic counterparts on long-horizon\ntasks, and our conformal calibration provides reliable statistical bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u6821\u51c6\u5b89\u5168\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u89c6\u89c9\u63a7\u5236\u7cfb\u7edf\uff0c\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u4f9d\u8d56\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u89e3\u51b3\u5b89\u5168\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5faa\u73af\u9884\u6d4b\u5668\u9884\u6d4b\u6f5c\u5728\u8f68\u8ff9\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u57df\u9002\u5e94\u786e\u4fdd\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUDA-equipped\u8bc4\u4f30\u5668\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u8bef\u62a5\u7387\uff0c\u4e16\u754c\u6a21\u578b\u590d\u5408\u9884\u6d4b\u5668\u4f18\u4e8e\u5355\u5757\u9884\u6d4b\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u7406\u8bba\u6821\u51c6\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u957f\u9884\u6d4b\u8303\u56f4\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09185", "abs": "https://arxiv.org/abs/2508.09185", "authors": ["Rongqian Chen", "Allison Andreyev", "Yanming Xiu", "Mahdi Imani", "Bin Li", "Maria Gorlatova", "Gang Tan", "Tian Lan"], "title": "A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality", "comment": null, "summary": "Augmented Reality (AR) enriches perception by overlaying virtual elements on\nthe physical world. Due to its growing popularity, cognitive attacks that alter\nAR content to manipulate users' semantic perception have received increasing\nattention. Existing detection methods often focus on visual changes, which are\nrestricted to pixel- or image-level processing and lack semantic reasoning\ncapabilities, or they rely on pre-trained vision-language models (VLMs), which\nfunction as black-box approaches with limited interpretability. In this paper,\nwe present CADAR, a novel neurosymbolic approach for cognitive attack detection\nin AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a\nsymbolic perception-graph representation, incorporating prior knowledge,\nsalience weighting, and temporal correlations. The model then enables\nparticle-filter based statistical reasoning -- a sequential Monte Carlo method\n-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of\npre-trained VLM and the interpretability and reasoning rigor of particle\nfiltering. Experiments on an extended AR cognitive attack dataset show accuracy\nimprovements of up to 10.7% over strong baselines on challenging AR attack\nscenarios, underscoring the promise of neurosymbolic methods for effective and\ninterpretable cognitive attack detection.", "AI": {"tldr": "CADAR\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bAR\u4e2d\u7684\u8ba4\u77e5\u653b\u51fb\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7c92\u5b50\u6ee4\u6ce2\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5c40\u9650\u4e8e\u50cf\u7d20\u7ea7\u5904\u7406\u6216\u4f9d\u8d56\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "CADAR\u878d\u5408\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\uff0c\u751f\u6210\u7b26\u53f7\u611f\u77e5\u56fe\uff0c\u5229\u7528\u7c92\u5b50\u6ee4\u6ce2\u8fdb\u884c\u7edf\u8ba1\u63a8\u7406\u3002", "result": "\u5728\u6269\u5c55\u7684AR\u8ba4\u77e5\u653b\u51fb\u6570\u636e\u96c6\u4e0a\uff0cCADAR\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe10.7%\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728AR\u8ba4\u77e5\u653b\u51fb\u68c0\u6d4b\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09354", "abs": "https://arxiv.org/abs/2508.09354", "authors": ["Kejun Li", "Zachary Olkin", "Yisong Yue", "Aaron D. Ames"], "title": "CLF-RL: Control Lyapunov Function Guided Reinforcement Learning", "comment": "8 pages; 8 figures", "summary": "Reinforcement learning (RL) has shown promise in generating robust locomotion\npolicies for bipedal robots, but often suffers from tedious reward design and\nsensitivity to poorly shaped objectives. In this work, we propose a structured\nreward shaping framework that leverages model-based trajectory generation and\ncontrol Lyapunov functions (CLFs) to guide policy learning. We explore two\nmodel-based planners for generating reference trajectories: a reduced-order\nlinear inverted pendulum (LIP) model for velocity-conditioned motion planning,\nand a precomputed gait library based on hybrid zero dynamics (HZD) using\nfull-order dynamics. These planners define desired end-effector and joint\ntrajectories, which are used to construct CLF-based rewards that penalize\ntracking error and encourage rapid convergence. This formulation provides\nmeaningful intermediate rewards, and is straightforward to implement once a\nreference is available. Both the reference trajectories and CLF shaping are\nused only during training, resulting in a lightweight policy at deployment. We\nvalidate our method both in simulation and through extensive real-world\nexperiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved\nrobustness relative to the baseline RL policy and better performance than a\nclassic tracking reward RL formulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u8f68\u8ff9\u751f\u6210\u548c\u63a7\u5236Lyapunov\u51fd\u6570\uff08CLF\uff09\u7684\u7ed3\u6784\u5316\u5956\u52b1\u6846\u67b6\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u751f\u6210\u3002", "motivation": "\u4f20\u7edfRL\u5728\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u751f\u6210\u4e2d\u5b58\u5728\u5956\u52b1\u8bbe\u8ba1\u7e41\u7410\u548c\u76ee\u6807\u654f\u611f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5012\u7acb\u6446\uff08LIP\uff09\u6a21\u578b\u548c\u6df7\u5408\u96f6\u52a8\u529b\u5b66\uff08HZD\uff09\u9884\u8ba1\u7b97\u6b65\u6001\u5e93\u751f\u6210\u53c2\u8003\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7CLF\u6784\u5efa\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cCLF-RL\u8868\u73b0\u51fa\u6bd4\u57fa\u7ebfRL\u7b56\u7565\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86RL\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4e14\u90e8\u7f72\u65f6\u4ec5\u9700\u8f7b\u91cf\u7ea7\u7b56\u7565\u3002"}}
{"id": "2508.09499", "categories": ["cs.CV", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09499", "abs": "https://arxiv.org/abs/2508.09499", "authors": ["Liyan Jia", "Chuan-Xian Ren", "Hong Yan"], "title": "CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking", "comment": null, "summary": "Accurately predicting the binding conformation of small-molecule ligands to\nprotein targets is a critical step in rational drug design. Although recent\ndeep learning-based docking surpasses traditional methods in speed and\naccuracy, many approaches rely on graph representations and language\nmodel-inspired encoders while neglecting critical geometric information,\nresulting in inaccurate pocket localization and unrealistic binding\nconformations. In this study, we introduce CWFBind, a weighted, fast, and\naccurate docking method based on local curvature features. Specifically, we\nintegrate local curvature descriptors during the feature extraction phase to\nenrich the geometric representation of both proteins and ligands, complementing\nexisting chemical, sequence, and structural features. Furthermore, we embed\ndegree-aware weighting mechanisms into the message passing process, enhancing\nthe model's ability to capture spatial structural distinctions and interaction\nstrengths. To address the class imbalance challenge in pocket prediction,\nCWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced\nloss function, facilitating more precise identification of binding regions and\nkey residues. Comprehensive experimental evaluations demonstrate that CWFBind\nachieves competitive performance across multiple docking benchmarks, offering a\nbalanced trade-off between accuracy and efficiency.", "AI": {"tldr": "CWFBind\u662f\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u66f2\u7387\u7279\u5f81\u7684\u5feb\u901f\u3001\u51c6\u786e\u7684\u5206\u5b50\u5bf9\u63a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u4fe1\u606f\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5b50\u5bf9\u63a5\u4e2d\u5ffd\u7565\u4e86\u5173\u952e\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u53e3\u888b\u5b9a\u4f4d\u548c\u7ed3\u5408\u6784\u8c61\u4e0d\u51c6\u786e\u3002", "method": "CWFBind\u7ed3\u5408\u5c40\u90e8\u66f2\u7387\u63cf\u8ff0\u7b26\u548c\u5ea6\u611f\u77e5\u52a0\u6743\u673a\u5236\uff0c\u589e\u5f3a\u51e0\u4f55\u8868\u793a\u548c\u7a7a\u95f4\u7ed3\u6784\u6355\u6349\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u534a\u5f84\u7b56\u7565\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u5bf9\u63a5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCWFBind\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "CWFBind\u901a\u8fc7\u51e0\u4f55\u4fe1\u606f\u589e\u5f3a\u548c\u52a8\u6001\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u5bf9\u63a5\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09186", "abs": "https://arxiv.org/abs/2508.09186", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak", "Mahboobeh Haghparast"], "title": "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System", "comment": null, "summary": "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nfundamental right to privacy. Existing privacy-preserving mechanisms, such as\nblurring or encryption, are often insufficient, creating an undesirable\ntrade-off where either privacy is compromised against advanced reconstruction\nattacks or data utility is critically degraded. To resolve this impasse, we\npropose RL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.", "AI": {"tldr": "RL-MoE\u6846\u67b6\u901a\u8fc7\u5c06\u654f\u611f\u89c6\u89c9\u6570\u636e\u8f6c\u6362\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u89e3\u51b3\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u9690\u79c1\u4e0e\u6570\u636e\u6548\u7528\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2dAI\u6444\u50cf\u5934\u7684\u666e\u53ca\u5bfc\u81f4\u9690\u79c1\u4e0e\u6570\u636e\u9700\u6c42\u4e4b\u95f4\u7684\u4e25\u91cd\u51b2\u7a81\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408Mixture-of-Experts\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u751f\u6210\u517c\u5177\u8bed\u4e49\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRL-MoE\u5c06\u91cd\u653e\u653b\u51fb\u6210\u529f\u7387\u964d\u81f39.4%\uff0c\u540c\u65f6\u751f\u6210\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u4e30\u5bcc\u7684\u6587\u672c\u5185\u5bb9\u3002", "conclusion": "RL-MoE\u4e3a\u9690\u79c1\u654f\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u66f4\u5b89\u5168\u7684\u667a\u80fd\u57ce\u5e02\u548c\u81ea\u52a8\u9a7e\u9a76\u7f51\u7edc\u3002"}}
{"id": "2508.09444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09444", "abs": "https://arxiv.org/abs/2508.09444", "authors": ["Haoxiang Shi", "Xiang Deng", "Zaijing Li", "Gongwei Chen", "Yaowei Wang", "Liqiang Nie"], "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to follow natural language instructions through free-form 3D spaces.\nExisting VLN-CE approaches typically use a two-stage waypoint planning\nframework, where a high-level waypoint predictor generates the navigable\nwaypoints, and then a navigation planner suggests the intermediate goals in the\nhigh-level action space. However, this two-stage decomposition framework\nsuffers from: (1) global sub-optimization due to the proxy objective in each\nstage, and (2) a performance bottleneck caused by the strong reliance on the\nquality of the first-stage predicted waypoints. To address these limitations,\nwe propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE\npolicy that unifies the traditional two stages, i.e. waypoint generation and\nplanning, into a single diffusion policy. Notably, DifNav employs a conditional\ndiffusion policy to directly model multi-modal action distributions over future\nactions in continuous navigation space, eliminating the need for a waypoint\npredictor while enabling the agent to capture multiple possible\ninstruction-following behaviors. To address the issues of compounding error in\nimitation learning and enhance spatial reasoning in long-horizon navigation\ntasks, we employ DAgger for online policy training and expert trajectory\naugmentation, and use the aggregated data to further fine-tune the policy. This\napproach significantly improves the policy's robustness and its ability to\nrecover from error states. Extensive experiments on benchmark datasets\ndemonstrate that, even without a waypoint predictor, the proposed method\nsubstantially outperforms previous state-of-the-art two-stage waypoint-based\nmodels in terms of navigation performance. Our code is available at:\nhttps://github.com/Tokishx/DifNav.", "AI": {"tldr": "VLN-CE\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u4e24\u9636\u6bb5\u5bfc\u822a\u6846\u67b6\u5b58\u5728\u5168\u5c40\u4f18\u5316\u4e0d\u8db3\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002DifNav\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6269\u6563\u7b56\u7565\uff0c\u7edf\u4e00\u4e86\u8def\u5f84\u70b9\u751f\u6210\u4e0e\u89c4\u5212\uff0c\u5e76\u901a\u8fc7DAgger\u589e\u5f3a\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4e24\u9636\u6bb5\u5bfc\u822a\u6846\u67b6\u7684\u5168\u5c40\u4f18\u5316\u4e0d\u8db3\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faDifNav\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6269\u6563\u7b56\u7565\uff0c\u76f4\u63a5\u5efa\u6a21\u8fde\u7eed\u5bfc\u822a\u7a7a\u95f4\u4e2d\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u5e76\u5229\u7528DAgger\u8fdb\u884c\u5728\u7ebf\u8bad\u7ec3\u548c\u4e13\u5bb6\u8f68\u8ff9\u589e\u5f3a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDifNav\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u65e0\u9700\u8def\u5f84\u70b9\u9884\u6d4b\u5668\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "DifNav\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u548cDAgger\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.09188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09188", "abs": "https://arxiv.org/abs/2508.09188", "authors": ["Seyed Muhammad Hossein Mousavi", "S. Younes Mirinezhad"], "title": "Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation", "comment": null, "summary": "Affective computing faces a major challenge: the lack of high-quality,\ndiverse depth facial datasets for recognizing subtle emotional expressions. We\npropose a framework for synthetic depth face generation using an optimized GAN\nwith Knowledge Distillation (EMA teacher models) to stabilize training, improve\nquality, and prevent mode collapse. We also apply Genetic Algorithms to evolve\nGAN latent vectors based on image statistics, boosting diversity and visual\nquality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in\nboth diversity and quality. For classification, we extract and concatenate LBP,\nHOG, Sobel edge, and intensity histogram features, achieving 94% and 96%\naccuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows\nconsistent improvement over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6f5c\u5728\u5411\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u60c5\u611f\u8ba1\u7b97\u4e2d\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u6df1\u5ea6\u9762\u90e8\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684GAN\u4e0e\u77e5\u8bc6\u84b8\u998f\uff08EMA\u6559\u5e08\u6a21\u578b\uff09\u751f\u6210\u5408\u6210\u6df1\u5ea6\u4eba\u8138\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6f5c\u5728\u5411\u91cf\uff0c\u5e76\u63d0\u53d6\u591a\u79cd\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8eGAN\u3001VAE\u3001GMM\u548cKDE\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94%\u548c96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.09502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09502", "abs": "https://arxiv.org/abs/2508.09502", "authors": ["Junheon Yoon", "Woo-Jeong Baek", "Jaeheung Park"], "title": "Reactive Model Predictive Contouring Control for Robot Manipulators", "comment": "8 pages, 7 figures, 3 tables, conference paper, Accepted for\n  publication at IEEE/RSJ International Conference on Intelligent Robots and\n  Systems(IROS) 2025", "summary": "This contribution presents a robot path-following framework via Reactive\nModel Predictive Contouring Control (RMPCC) that successfully avoids obstacles,\nsingularities and self-collisions in dynamic environments at 100 Hz. Many\npath-following methods rely on the time parametrization, but struggle to handle\ncollision and singularity avoidance while adhering kinematic limits or other\nconstraints. Specifically, the error between the desired path and the actual\nposition can become large when executing evasive maneuvers. Thus, this paper\nderives a method that parametrizes the reference path by a path parameter and\nperforms the optimization via RMPCC. In particular, Control Barrier Functions\n(CBFs) are introduced to avoid collisions and singularities in dynamic\nenvironments. A Jacobian-based linearization and Gauss-Newton Hessian\napproximation enable solving the nonlinear RMPCC problem at 100 Hz,\noutperforming state-of-the-art methods by a factor of 10. Experiments confirm\nthat the framework handles dynamic obstacles in real-world settings with low\ncontouring error and low robot acceleration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRMPCC\u7684\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2a\u6846\u67b6\uff0c\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4ee5100 Hz\u9891\u7387\u907f\u5f00\u969c\u788d\u7269\u3001\u5947\u5f02\u70b9\u548c\u81ea\u78b0\u649e\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u65f6\u95f4\u53c2\u6570\u5316\uff0c\u96be\u4ee5\u540c\u65f6\u5904\u7406\u78b0\u649e\u3001\u5947\u5f02\u70b9\u907f\u514d\u548c\u8fd0\u52a8\u5b66\u9650\u5236\uff0c\u5bfc\u81f4\u8def\u5f84\u8bef\u5dee\u8f83\u5927\u3002", "method": "\u901a\u8fc7\u8def\u5f84\u53c2\u6570\u5316\u53c2\u8003\u8def\u5f84\uff0c\u5229\u7528RMPCC\u8fdb\u884c\u4f18\u5316\uff0c\u5f15\u5165CBF\u907f\u514d\u78b0\u649e\u548c\u5947\u5f02\u70b9\uff0c\u91c7\u7528\u96c5\u53ef\u6bd4\u7ebf\u6027\u5316\u548c\u9ad8\u65af-\u725b\u987f\u8fd1\u4f3c\u6c42\u89e3\u975e\u7ebf\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8def\u5f84\u8bef\u5dee\u548c\u52a0\u901f\u5ea6\u4f4e\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb10\u500d\u3002", "conclusion": "RMPCC\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u7a33\u5b9a\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2a\u3002"}}
{"id": "2508.09199", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09199", "abs": "https://arxiv.org/abs/2508.09199", "authors": ["Jucheng Hu", "Suorong Yang", "Dongzhan Zhou"], "title": "$\u0394$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "comment": null, "summary": "Visual Instruction Finetuning (VIF) is pivotal for post-training\nVision-Language Models (VLMs). Unlike unimodal instruction finetuning in\nplain-text large language models, which mainly requires instruction datasets to\nenable model instruction-following ability, VIF also requires multimodal data\nto enable joint visual and textual understanding; therefore, it typically\nrequires more data. Consequently, VIF imposes stricter data selection\nchallenges: the method must scale efficiently to handle larger data demands\nwhile ensuring the quality of both visual and textual content, as well as their\nalignment. Despite its critical impact on performance, data selection for VIF\nremains an understudied area. In this paper, we propose $\\Delta$-AttnMask. This\ndata-efficient framework quantifies sample quality through attention-guided\nmasking of the model's hidden states, jointly evaluating image-text pairs\nwithout requiring domain labels, auxiliary models, or extra training. By\ncomputing loss differences ($\\Delta$) between the original states and states\nmasked using high-attention regions, $\\Delta$-AttnMask intrinsically assesses\nsample quality. Experiments across multiple VLMs and datasets show that\n$\\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,\naccelerating training by 5x while surpassing full-dataset baselines by +10.1%\nin overall accuracy. Its model-agnostic and data-agnostic design ensures broad\napplicability across modalities and architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u0394-AttnMask\u7684\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u85cf\u72b6\u6001\u63a9\u7801\u91cf\u5316\u6837\u672c\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\uff08VIF\uff09\u9700\u8981\u591a\u6a21\u6001\u6570\u636e\u4ee5\u652f\u6301\u89c6\u89c9\u548c\u6587\u672c\u8054\u5408\u7406\u89e3\uff0c\u4f46\u6570\u636e\u9009\u62e9\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u0394-AttnMask\u901a\u8fc7\u8ba1\u7b97\u539f\u59cb\u72b6\u6001\u4e0e\u9ad8\u6ce8\u610f\u529b\u533a\u57df\u63a9\u7801\u72b6\u6001\u7684\u635f\u5931\u5dee\u5f02\uff08\u0394\uff09\uff0c\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u8bad\u7ec3\u5373\u53ef\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u0394-AttnMask\u4ec5\u970020%\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc7\u5168\u6570\u636e\u96c6\u57fa\u7ebf10.1%\u3002", "conclusion": "\u0394-AttnMask\u7684\u6a21\u578b\u65e0\u5173\u548c\u6570\u636e\u65e0\u5173\u8bbe\u8ba1\u4f7f\u5176\u5728\u591a\u6a21\u6001\u548c\u67b6\u6784\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.09508", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09508", "abs": "https://arxiv.org/abs/2508.09508", "authors": ["Reema Raval", "Shalabh Gupta"], "title": "SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents", "comment": null, "summary": "Typical marine environments are highly complex with spatio-temporally varying\ncurrents and dynamic obstacles, presenting significant challenges to Unmanned\nSurface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need\nto continuously adapt their paths with real-time information to avoid\ncollisions and follow the path of least resistance to the goal via exploiting\nocean currents. In this regard, we introduce a novel algorithm, called\nSelf-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents\n(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic\nenvironments. SMART-OC integrates the obstacle risks along a path with the time\ncost to reach the goal to find the time-risk optimal path. The effectiveness of\nSMART-OC is validated by simulation experiments, which demonstrate that the USV\nperforms fast replannings to avoid dynamic obstacles and exploit ocean currents\nto successfully reach the goal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMART-OC\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u6c34\u9762\u8247\uff08USV\uff09\u5728\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u7ed3\u5408\u969c\u788d\u7269\u98ce\u9669\u548c\u5230\u8fbe\u76ee\u6807\u7684\u65f6\u95f4\u6210\u672c\uff0c\u5b9e\u73b0\u65f6\u95f4-\u98ce\u9669\u6700\u4f18\u8def\u5f84\u3002", "motivation": "\u6d77\u6d0b\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u5b58\u5728\u52a8\u6001\u969c\u788d\u7269\u548c\u6d0b\u6d41\uff0cUSV\u9700\u8981\u5b9e\u65f6\u8c03\u6574\u8def\u5f84\u4ee5\u786e\u4fdd\u5b89\u5168\u548c\u9ad8\u6548\u5bfc\u822a\u3002", "method": "SMART-OC\u7b97\u6cd5\u6574\u5408\u8def\u5f84\u4e0a\u7684\u969c\u788d\u7269\u98ce\u9669\u548c\u5230\u8fbe\u76ee\u6807\u7684\u65f6\u95f4\u6210\u672c\uff0c\u8fdb\u884c\u5b9e\u65f6\u65f6\u95f4-\u98ce\u9669\u6700\u4f18\u8def\u5f84\u89c4\u5212\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SMART-OC\u7684\u6709\u6548\u6027\uff0cUSV\u80fd\u591f\u5feb\u901f\u91cd\u65b0\u89c4\u5212\u8def\u5f84\u4ee5\u907f\u5f00\u969c\u788d\u7269\u5e76\u5229\u7528\u6d0b\u6d41\u6210\u529f\u5230\u8fbe\u76ee\u6807\u3002", "conclusion": "SMART-OC\u4e3aUSV\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09202", "abs": "https://arxiv.org/abs/2508.09202", "authors": ["Masoumeh Sharafi", "Soufiane Belharbi", "Houssem Ben Salem", "Ali Etemad", "Alessandro Lameiras Koerich", "Marco Pedersoli", "Simon Bacon", "Eric Granger"], "title": "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method", "comment": null, "summary": "Facial expression recognition (FER) models are employed in many video-based\naffective computing applications, such as human-computer interaction and\nhealthcare monitoring. However, deep FER models often struggle with subtle\nexpressions and high inter-subject variability, limiting their performance in\nreal-world applications. To improve their performance, source-free domain\nadaptation (SFDA) methods have been proposed to personalize a pretrained source\nmodel using only unlabeled target domain data, thereby avoiding data privacy,\nstorage, and transmission constraints. This paper addresses a challenging\nscenario where source data is unavailable for adaptation, and only unlabeled\ntarget data consisting solely of neutral expressions is available. SFDA methods\nare not typically designed to adapt using target data from only a single class.\nFurther, using models to generate facial images with non-neutral expressions\ncan be unstable and computationally intensive. In this paper, personalized\nfeature translation (PFT) is proposed for SFDA. Unlike current image\ntranslation methods for SFDA, our lightweight method operates in the latent\nspace. We first pre-train the translator on the source domain data to transform\nthe subject-specific style features from one source subject into another.\nExpression information is preserved by optimizing a combination of expression\nconsistency and style-aware objectives. Then, the translator is adapted on\nneutral target data, without using source data or image synthesis. By\ntranslating in the latent space, PFT avoids the complexity and noise of face\nexpression generation, producing discriminative embeddings optimized for\nclassification. Using PFT eliminates the need for image synthesis, reduces\ncomputational overhead (using a lightweight translator), and only adapts part\nof the model, making the method efficient compared to image-based translation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e2a\u6027\u5316\u7279\u5f81\u7ffb\u8bd1\uff08PFT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6e90\u81ea\u7531\u57df\u9002\u5e94\uff08SFDA\uff09\uff0c\u4ee5\u89e3\u51b3\u4ec5\u542b\u4e2d\u6027\u8868\u60c5\u7684\u65e0\u6807\u7b7e\u76ee\u6807\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SFDA\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u9002\u5e94\u4ec5\u542b\u5355\u4e00\u7c7b\u522b\u7684\u76ee\u6807\u6570\u636e\uff0c\u4e14\u751f\u6210\u975e\u4e2d\u6027\u8868\u60c5\u56fe\u50cf\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "PFT\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u7ffb\u8bd1\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7ffb\u8bd1\u5668\u4fdd\u7559\u8868\u60c5\u4fe1\u606f\uff0c\u5e76\u5728\u76ee\u6807\u6570\u636e\u4e0a\u9002\u5e94\uff0c\u65e0\u9700\u6e90\u6570\u636e\u6216\u56fe\u50cf\u5408\u6210\u3002", "result": "PFT\u907f\u514d\u4e86\u8868\u60c5\u751f\u6210\u7684\u590d\u6742\u6027\uff0c\u751f\u6210\u4f18\u5316\u7684\u5206\u7c7b\u5d4c\u5165\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "PFT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002"}}
{"id": "2508.09558", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09558", "abs": "https://arxiv.org/abs/2508.09558", "authors": ["Jiahui Zuo", "Boyang Zhang", "Fumin Zhang"], "title": "CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail", "comment": null, "summary": "The manipulation of deformable linear flexures has a wide range of\napplications in industry, such as cable routing in automotive manufacturing and\ntextile production. Cable routing, as a complex multi-stage robot manipulation\nscenario, is a challenging task for robot automation. Common parallel\ntwo-finger grippers have the risk of over-squeezing and over-tension when\ngrasping and guiding cables. In this paper, a novel eagle-inspired fingernail\nis designed and mounted on the gripper fingers, which helps with cable grasping\non planar surfaces and in-hand cable guiding operations. Then we present a\nsingle-grasp end-to-end 3D cable routing framework utilizing the proposed\nfingernails, instead of the common pick-and-place strategy. Continuous control\nis achieved to efficiently manipulate cables through vision-based state\nestimation of task configurations and offline trajectory planning based on\nmotion primitives. We evaluate the effectiveness of the proposed framework with\na variety of cables and channel slots, significantly outperforming the\npick-and-place manipulation process under equivalent perceptual conditions. Our\nreconfigurable task setting and the proposed framework provide a reference for\nfuture cable routing manipulations in 3D space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u9e70\u722a\u6307\u7532\u8bbe\u8ba1\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6293\u53d6\u548c\u5f15\u5bfc\u7535\u7f06\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7aef\u5230\u7aef3D\u7535\u7f06\u5e03\u7ebf\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u62fe\u653e\u7b56\u7565\u3002", "motivation": "\u7535\u7f06\u5e03\u7ebf\u662f\u5de5\u4e1a\u4e2d\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u4f20\u7edf\u5e73\u884c\u4e24\u6307\u5939\u5177\u6709\u6324\u538b\u548c\u5f20\u529b\u8fc7\u5927\u7684\u98ce\u9669\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4eff\u9e70\u722a\u6307\u7532\u5e76\u5b89\u88c5\u5728\u5939\u6307\u4e0a\uff0c\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u72b6\u6001\u4f30\u8ba1\u548c\u79bb\u7ebf\u8f68\u8ff9\u89c4\u5212\u7684\u7aef\u5230\u7aef3D\u7535\u7f06\u5e03\u7ebf\u6846\u67b6\u3002", "result": "\u5728\u591a\u79cd\u7535\u7f06\u548c\u69fd\u9053\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u62fe\u653e\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u67653D\u7a7a\u95f4\u7535\u7f06\u5e03\u7ebf\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5177\u6709\u53ef\u91cd\u6784\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.09207", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09207", "abs": "https://arxiv.org/abs/2508.09207", "authors": ["Tai Vu", "Robert Yang"], "title": "GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning", "comment": null, "summary": "The process of generating fully colorized drawings from sketches is a large,\nusually costly bottleneck in the manga and anime industry. In this study, we\nexamine multiple models for image-to-image translation between anime characters\nand their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By\nassessing them qualitatively and quantitatively, we find that C-GAN is the most\neffective model that is able to produce high-quality and high-resolution images\nclose to those created by humans.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\u5728\u52a8\u6f2b\u89d2\u8272\u4e0e\u8349\u56fe\u4e4b\u95f4\u7684\u6548\u679c\uff0c\u53d1\u73b0C-GAN\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u52a8\u6f2b\u884c\u4e1a\u4e2d\u4ece\u8349\u56fe\u751f\u6210\u5168\u5f69\u7ed8\u7684\u9ad8\u6210\u672c\u74f6\u9888\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86Neural Style Transfer\u3001C-GAN\u548cCycleGAN\u7b49\u6a21\u578b\u3002", "result": "C-GAN\u80fd\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u521b\u4f5c\u7684\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "C-GAN\u662f\u89e3\u51b3\u52a8\u6f2b\u884c\u4e1a\u8349\u56fe\u7740\u8272\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.09581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09581", "abs": "https://arxiv.org/abs/2508.09581", "authors": ["Junkai Jiang", "Yihe Chen", "Yibin Yang", "Ruochen Li", "Shaobing Xu", "Jianqiang Wang"], "title": "ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots", "comment": null, "summary": "Multi-vehicle trajectory planning (MVTP) is one of the key challenges in\nmulti-robot systems (MRSs) and has broad applications across various fields.\nThis paper presents ESCoT, an enhanced step-based coordinate trajectory\nplanning method for multiple car-like robots. ESCoT incorporates two key\nstrategies: collaborative planning for local robot groups and replanning for\nduplicate configurations. These strategies effectively enhance the performance\nof step-based MVTP methods. Through extensive experiments, we show that ESCoT\n1) in sparse scenarios, significantly improves solution quality compared to\nbaseline step-based method, achieving up to 70% improvement in typical conflict\nscenarios and 34% in randomly generated scenarios, while maintaining high\nsolving efficiency; and 2) in dense scenarios, outperforms all baseline\nmethods, maintains a success rate of over 50% even in the most challenging\nconfigurations. The results demonstrate that ESCoT effectively solves MVTP,\nfurther extending the capabilities of step-based methods. Finally, practical\nrobot tests validate the algorithm's applicability in real-world scenarios.", "AI": {"tldr": "ESCoT\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u8f66\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u4f5c\u89c4\u5212\u548c\u91cd\u590d\u914d\u7f6e\u91cd\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u548c\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u8f66\u8f68\u8ff9\u89c4\u5212\uff08MVTP\uff09\u662f\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08MRS\uff09\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e94\u7528\u5e7f\u6cdb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faESCoT\u65b9\u6cd5\uff0c\u7ed3\u5408\u534f\u4f5c\u89c4\u5212\u548c\u91cd\u590d\u914d\u7f6e\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u4f18\u5316\u6b65\u8fdb\u5f0fMVTP\u65b9\u6cd5\u3002", "result": "\u7a00\u758f\u573a\u666f\u4e2d\uff0cESCoT\u663e\u8457\u63d0\u5347\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff08\u6700\u9ad870%\uff09\uff0c\u5bc6\u96c6\u573a\u666f\u4e2d\u4fdd\u630150%\u4ee5\u4e0a\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ESCoT\u6709\u6548\u89e3\u51b3MVTP\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u6b65\u8fdb\u5f0f\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u673a\u5668\u4eba\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09210", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09210", "abs": "https://arxiv.org/abs/2508.09210", "authors": ["Fan Zhang", "Zebang Cheng", "Chong Deng", "Haoxuan Li", "Zheng Lian", "Qian Chen", "Huadai Liu", "Wen Wang", "Yi-Fan Zhang", "Renrui Zhang", "Ziyu Guo", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Yefeng Zheng", "Xiaojiang Peng", "Xian Wu", "Kun Wang", "Xiangang Li", "Jieping Ye", "Pheng-Ann Heng"], "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have catalyzed\ntransformative progress in affective computing, enabling models to exhibit\nemergent emotional intelligence. Despite substantial methodological progress,\ncurrent emotional benchmarks remain limited, as it is still unknown: (a) the\ngeneralization abilities of MLLMs across distinct scenarios, and (b) their\nreasoning capabilities to identify the triggering factors behind emotional\nstates. To bridge these gaps, we present \\textbf{MME-Emotion}, a systematic\nbenchmark that assesses both emotional understanding and reasoning capabilities\nof MLLMs, enjoying \\textit{scalable capacity}, \\textit{diverse settings}, and\n\\textit{unified protocols}. As the largest emotional intelligence benchmark for\nMLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific\nquestioning-answering (QA) pairs, spanning broad scenarios to formulate eight\nemotional tasks. It further incorporates a holistic evaluation suite with\nhybrid metrics for emotion recognition and reasoning, analyzed through a\nmulti-agent system framework. Through a rigorous evaluation of 20 advanced\nMLLMs, we uncover both their strengths and limitations, yielding several key\ninsights: \\ding{182} Current MLLMs exhibit unsatisfactory emotional\nintelligence, with the best-performing model achieving only $39.3\\%$\nrecognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark.\n\\ding{183} Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional\nintelligence from generalized multimodal understanding capabilities, while\nspecialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance\nthrough domain-specific post-training adaptation. By introducing MME-Emotion,\nwe hope that it can serve as a foundation for advancing MLLMs' emotional\nintelligence in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MME-Emotion\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u60c5\u611f\u60c5\u611f\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u8bc4\u4f30MLLMs\u5728\u591a\u6837\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u60c5\u611f\u89e6\u53d1\u56e0\u7d20\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMME-Emotion\u57fa\u51c6\uff0c\u5305\u542b6,000\u591a\u4e2a\u89c6\u9891\u7247\u6bb5\u548c\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u516b\u79cd\u60c5\u611f\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u6307\u6807\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f3020\u4e2a\u5148\u8fdbMLLMs\uff0c\u53d1\u73b0\u5176\u60c5\u611f\u60c5\u611f\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u9ad8\u8bc6\u522b\u5f97\u520639.3%\uff0c\u63a8\u7406\u5f97\u520656.0%\uff09\uff0c\u901a\u7528\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "MME-Emotion\u4e3a\u672a\u6765\u63d0\u5347MLLMs\u7684\u60c5\u611f\u60c5\u611f\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09595", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09595", "abs": "https://arxiv.org/abs/2508.09595", "authors": ["Michael Fennel", "Markus Walker", "Dominik Pikos", "Uwe D. Hanebeck"], "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control", "comment": "Final Version - Accepted on IEEE Transactions on Haptics", "summary": "Research in virtual reality and haptic technologies has consistently aimed to\nenhance immersion. While advanced head-mounted displays are now commercially\navailable, kinesthetic haptic interfaces still face challenges such as limited\nworkspaces, insufficient degrees of freedom, and kinematics not matching the\nhuman arm. In this paper, we present HapticGiant, a novel large-scale\nkinesthetic haptic interface designed to match the properties of the human arm\nas closely as possible and to facilitate natural user locomotion while\nproviding full haptic feedback. The interface incorporates a novel\nadmittance-type force control scheme, leveraging hierarchical optimization to\nrender both arbitrary serial kinematic chains and Cartesian admittances.\nNotably, the proposed control scheme natively accounts for system limitations,\nincluding joint and Cartesian constraints, as well as singularities.\nExperimental results demonstrate the effectiveness of HapticGiant and its\ncontrol scheme, paving the way for highly immersive virtual reality\napplications.", "AI": {"tldr": "HapticGiant\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5927\u89c4\u6a21\u52a8\u89c9\u89e6\u89c9\u63a5\u53e3\uff0c\u65e8\u5728\u5339\u914d\u4eba\u7c7b\u624b\u81c2\u7279\u6027\u5e76\u63d0\u4f9b\u81ea\u7136\u7528\u6237\u8fd0\u52a8\u4e0e\u5b8c\u6574\u89e6\u89c9\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u52a8\u89c9\u89e6\u89c9\u63a5\u53e3\u5b58\u5728\u5de5\u4f5c\u7a7a\u95f4\u6709\u9650\u3001\u81ea\u7531\u5ea6\u4e0d\u8db3\u53ca\u8fd0\u52a8\u5b66\u4e0d\u5339\u914d\u4eba\u7c7b\u624b\u81c2\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u865a\u62df\u73b0\u5b9e\u7684\u6c89\u6d78\u611f\u3002", "method": "\u63d0\u51faHapticGiant\u63a5\u53e3\uff0c\u91c7\u7528\u65b0\u578b\u5bfc\u7eb3\u578b\u529b\u63a7\u5236\u65b9\u6848\uff0c\u5229\u7528\u5206\u5c42\u4f18\u5316\u5b9e\u73b0\u4efb\u610f\u4e32\u884c\u8fd0\u52a8\u94fe\u548c\u7b1b\u5361\u5c14\u5bfc\u7eb3\u7684\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHapticGiant\u53ca\u5176\u63a7\u5236\u65b9\u6848\u6709\u6548\uff0c\u80fd\u591f\u5904\u7406\u7cfb\u7edf\u9650\u5236\uff08\u5982\u5173\u8282\u548c\u7b1b\u5361\u5c14\u7ea6\u675f\u53ca\u5947\u70b9\uff09\u3002", "conclusion": "HapticGiant\u4e3a\u9ad8\u5ea6\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.09218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09218", "abs": "https://arxiv.org/abs/2508.09218", "authors": ["Zuoou Li", "Weitong Zhang", "Jingyuan Wang", "Shuyuan Zhang", "Wenjia Bai", "Bernhard Kainz", "Mengyun Qiao"], "title": "Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity", "comment": null, "summary": "Multimodal large language models (MLLMs) are widely used in vision-language\nreasoning tasks. However, their vulnerability to adversarial prompts remains a\nserious concern, as safety mechanisms often fail to prevent the generation of\nharmful outputs. Although recent jailbreak strategies report high success\nrates, many responses classified as \"successful\" are actually benign, vague, or\nunrelated to the intended malicious goal. This mismatch suggests that current\nevaluation standards may overestimate the effectiveness of such attacks. To\naddress this issue, we introduce a four-axis evaluation framework that\nconsiders input on-topicness, input out-of-distribution (OOD) intensity, output\nharmfulness, and output refusal rate. This framework identifies truly effective\njailbreaks. In a substantial empirical study, we reveal a structural trade-off:\nhighly on-topic prompts are frequently blocked by safety filters, whereas those\nthat are too OOD often evade detection but fail to produce harmful content.\nHowever, prompts that balance relevance and novelty are more likely to evade\nfilters and trigger dangerous output. Building on this insight, we develop a\nrecursive rewriting strategy called Balanced Structural Decomposition (BSD).\nThe approach restructures malicious prompts into semantically aligned\nsub-tasks, while introducing subtle OOD signals and visual cues that make the\ninputs harder to detect. BSD was tested across 13 commercial and open-source\nMLLMs, where it consistently led to higher attack success rates, more harmful\noutputs, and fewer refusals. Compared to previous methods, it improves success\nrates by $67\\%$ and harmfulness by $21\\%$, revealing a previously\nunderappreciated weakness in current multimodal safety systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\uff08Balanced Structural Decomposition, BSD\uff09\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u6548\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u5b89\u5168\u7cfb\u7edf\u7684\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u53ef\u80fd\u9ad8\u4f30\u4e86\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u8bb8\u591a\u88ab\u6807\u8bb0\u4e3a\u201c\u6210\u529f\u201d\u7684\u54cd\u5e94\u5b9e\u9645\u4e0a\u662f\u826f\u6027\u7684\u3001\u6a21\u7cca\u7684\u6216\u4e0e\u6076\u610f\u76ee\u6807\u65e0\u5173\u3002", "method": "\u5f15\u5165\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\uff08\u8f93\u5165\u76f8\u5173\u6027\u3001\u8f93\u5165\u5206\u5e03\u5916\u5f3a\u5ea6\u3001\u8f93\u51fa\u5371\u5bb3\u6027\u3001\u8f93\u51fa\u62d2\u7edd\u7387\uff09\uff0c\u5e76\u63d0\u51fa\u9012\u5f52\u91cd\u5199\u7b56\u7565BSD\uff0c\u901a\u8fc7\u5e73\u8861\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u6765\u4f18\u5316\u653b\u51fb\u6548\u679c\u3002", "result": "BSD\u572813\u4e2a\u5546\u4e1a\u548c\u5f00\u6e90MLLMs\u4e0a\u6d4b\u8bd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad867%\uff0c\u5371\u5bb3\u6027\u63d0\u9ad821%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u62d2\u7edd\u7387\u3002", "conclusion": "BSD\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5b89\u5168\u7cfb\u7edf\u4e2d\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u5f31\u70b9\uff0c\u4e3a\u6539\u8fdb\u5b89\u5168\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.09606", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09606", "abs": "https://arxiv.org/abs/2508.09606", "authors": ["Alejandro Posadas-Nava", "Alejandro Carrasco", "Richard Linares"], "title": "BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots", "comment": "Accepted for presentation on ICCR Kyoto 2025", "summary": "\\textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality\n(VR) teleoperation system for robots, designed to unify real-time control, data\nrecording, and policy learning across heterogeneous robotic platforms. BEAVR\nenables real-time, dexterous teleoperation using commodity VR hardware,\nsupports modular integration with robots ranging from 7-DoF manipulators to\nfull-body humanoids, and records synchronized multi-modal demonstrations\ndirectly in the LeRobot dataset schema. Our system features a zero-copy\nstreaming architecture achieving $\\leq$35\\,ms latency, an asynchronous\n``think--act'' control loop for scalable inference, and a flexible network API\noptimized for real-time, multi-robot operation. We benchmark BEAVR across\ndiverse manipulation tasks and demonstrate its compatibility with leading\nvisuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is\npublicly available, and datasets are released on Hugging Face\\footnote{Code,\ndatasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.", "AI": {"tldr": "BEAVR\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u53cc\u624b\u673a\u5668\u4eba\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u5e73\u53f0\u7edf\u4e00\u63a7\u5236\u3001\u6570\u636e\u8bb0\u5f55\u548c\u653f\u7b56\u5b66\u4e60\u3002", "motivation": "\u8bbe\u8ba1BEAVR\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u7edf\u4e00\u5f02\u6784\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5b9e\u65f6\u63a7\u5236\u3001\u6570\u636e\u8bb0\u5f55\u548c\u653f\u7b56\u5b66\u4e60\uff0c\u63d0\u9ad8\u8fdc\u7a0b\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "BEAVR\u91c7\u7528\u96f6\u62f7\u8d1d\u6d41\u67b6\u6784\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\uff08\u226435ms\uff09\uff0c\u652f\u6301\u5f02\u6b65\u201c\u601d\u8003-\u884c\u52a8\u201d\u63a7\u5236\u5faa\u73af\u548c\u7075\u6d3b\u7684\u7f51\u7edcAPI\uff0c\u517c\u5bb9\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e0e\u4e3b\u6d41\u89c6\u89c9\u8fd0\u52a8\u653f\u7b56\uff08\u5982ACT\u3001DiffusionPolicy\u3001SmolVLA\uff09\u517c\u5bb9\u3002", "conclusion": "BEAVR\u4e3a\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.09220", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09220", "abs": "https://arxiv.org/abs/2508.09220", "authors": ["Haoyang Li", "Jiaqing Li", "Jialun Cao", "Zongyuan Yang", "Yongping Xiong"], "title": "Towards Scalable Training for Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Large foundation models have achieved significant performance gains through\nscalable training on massive datasets. However, the field of\n\\textbf{H}andwritten \\textbf{M}athematical \\textbf{E}xpression\n\\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily\ndue to the arduous and costly process of manual annotation. To bridge this gap,\nwe propose a novel method integrating limited handwritten formulas with\nlarge-scale LaTeX-rendered formulas by developing a scalable data engine to\ngenerate complex and consistent LaTeX sequences. With this engine, we built the\nlargest formula dataset to date, termed \\texttt{Tex80M}, comprising over 80\nmillion high-quality training instances. Then we propose \\texttt{TexTeller},\nthe first HMER model trained at scale, by mix-training \\texttt{Tex80M} with a\nrelatively small HME dataset. The expansive training dataset and our refined\npipeline have equipped \\texttt{TexTeller} with state-of-the-art (SOTA)\nperformance across nearly all benchmarks. To advance the field, we will openly\nrelease our complete model, entire dataset, and full codebase, enabling further\nresearch building upon our contributions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u624b\u5199\u516c\u5f0f\u4e0eLaTeX\u6e32\u67d3\u516c\u5f0f\u7684\u65b0\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6Tex80M\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u8bad\u7ec3\u7684HMER\u6a21\u578bTexTeller\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u9886\u57df\u56e0\u6570\u636e\u7a00\u7f3a\u800c\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u5f15\u64ce\uff0c\u7ed3\u5408\u6709\u9650\u624b\u5199\u516c\u5f0f\u548c\u5927\u89c4\u6a21LaTeX\u6e32\u67d3\u516c\u5f0f\uff0c\u6784\u5efa\u4e86Tex80M\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u5f00\u53d1\u4e86TexTeller\u6a21\u578b\u3002", "result": "TexTeller\u5728\u51e0\u4e4e\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u5c06\u516c\u5f00\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u63a8\u52a8HMER\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.09621", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09621", "abs": "https://arxiv.org/abs/2508.09621", "authors": ["Ingrid Ma\u00e9va Chekam", "Ines Pastor-Martinez", "Ali Tourani", "Jose Andres Millan-Romera", "Laura Ribeiro", "Pedro Miguel Bastos Soares", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Interpretable Robot Control via Structured Behavior Trees and Large Language Models", "comment": "15 pages, 5 figures, 3 tables", "summary": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u884c\u4e3a\u6811\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u8ba4\u77e5\u5230\u6267\u884c\u7684\u51c6\u786e\u7387\u7ea6\u4e3a94%\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u7528\u6237\u9002\u5e94\u754c\u9762\u6216\u8bb0\u5fc6\u547d\u4ee4\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u884c\u4e3a\u6811\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u63d2\u4ef6\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u6837\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e73\u5747\u8ba4\u77e5\u5230\u6267\u884c\u51c6\u786e\u7387\u4e3a94%\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u7684\u81ea\u7136\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.09239", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09239", "abs": "https://arxiv.org/abs/2508.09239", "authors": ["Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Jia-Chen Zhang", "Hong-Jian Zhan"], "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting", "comment": null, "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced\nnovel view synthesis through explicit scene representation, enabling real-time\nphotorealistic rendering. However, existing approaches manifest two critical\nlimitations in complex scenarios: (1) Over-reconstruction occurs when\npersistent large Gaussians cannot meet adaptive splitting thresholds during\ndensity control. This is exacerbated by conflicting gradient directions that\nprevent effective splitting of these Gaussians; (2) Over-densification of\nGaussians occurs in regions with aligned gradient aggregation, leading to\nredundant component proliferation. This redundancy significantly increases\nmemory overhead due to unnecessary data retention. We present\nGradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware\nadaptive density control framework to address these challenges. Our key\ninnovations: the gradient coherence ratio (GCR), computed through normalized\ngradient vector norms, which explicitly discriminates Gaussians with concordant\nversus conflicting gradient directions; and a nonlinear dynamic weighting\nmechanism leverages the GCR to enable gradient-direction-aware density control.\nSpecifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting\noperations to enhance geometric details while suppressing redundant\nconcordant-direction Gaussians. Conversely, in cloning processes, GDAGS\npromotes concordant-direction Gaussian densification for structural completion\nwhile preventing conflicting-direction Gaussian overpopulation. Comprehensive\nevaluations across diverse real-world benchmarks demonstrate that GDAGS\nachieves superior rendering quality while effectively mitigating\nover-reconstruction, suppressing over-densification, and constructing compact\nscene representations with 50\\% reduced memory consumption through optimized\nGaussians utilization.", "AI": {"tldr": "GDAGS\u901a\u8fc7\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u4e2d\u7684\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u589e\u52a0\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faGDAGS\u6846\u67b6\uff0c\u5f15\u5165\u68af\u5ea6\u4e00\u81f4\u6027\u6bd4\uff08GCR\uff09\u548c\u975e\u7ebf\u6027\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u5b9e\u73b0\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u5bc6\u5ea6\u63a7\u5236\u3002", "result": "GDAGS\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6e32\u67d3\u8d28\u91cf\u63d0\u5347\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1150%\u3002", "conclusion": "GDAGS\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09700", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09700", "abs": "https://arxiv.org/abs/2508.09700", "authors": ["Mahdi Hejrati", "Jouni Mattila"], "title": "Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions", "comment": "This work has been accepted for presentation at the 2025 IEEE\n  Conference on Telepresence, to be held in Leiden, Netherlands", "summary": "Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents\nunique challenges that differ fundamentally from conventional human-scale\nsystems. As these platforms gain relevance in industrial domains such as\nconstruction, mining, and disaster response, immersive interfaces must be\nrethought to support scalable, safe, and effective human-robot collaboration.\nThis paper investigates the control, cognitive, and interface-level challenges\nof immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,\nminimizing sensorimotor mismatch, and enhancing the sense of embodiment. We\nanalyze design trade-offs in haptic and visual feedback systems, supported by\nearly experimental comparisons of exoskeleton- and joystick-based control\nsetups. Finally, we outline key research directions for developing new\nevaluation tools, scaling strategies, and human-centered safety models tailored\nto large-scale robotic telepresence.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8d85\u5927\u578b\u673a\u68b0\u81c2\uff08BHSRMs\uff09\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u63a7\u5236\u3001\u8ba4\u77e5\u548c\u754c\u9762\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u64cd\u4f5c\u8005\u5b89\u5168\u3001\u51cf\u5c11\u611f\u77e5\u8fd0\u52a8\u4e0d\u5339\u914d\u548c\u589e\u5f3a\u4f53\u611f\u3002", "motivation": "\u968f\u7740BHSRMs\u5728\u5de5\u4e1a\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u6c89\u6d78\u5f0f\u754c\u9762\u4ee5\u652f\u6301\u5b89\u5168\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u5206\u6790\u4e86\u89e6\u89c9\u548c\u89c6\u89c9\u53cd\u9988\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u5916\u9aa8\u9abc\u548c\u6447\u6746\u63a7\u5236\u8bbe\u7f6e\u7684\u5b9e\u9a8c\u6bd4\u8f83\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u5927\u578b\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7684\u65b0\u8bc4\u4f30\u5de5\u5177\u3001\u6269\u5c55\u7b56\u7565\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b89\u5168\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u4e3aBHSRMs\u7684\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u4f18\u5316\u3002"}}
{"id": "2508.09241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09241", "abs": "https://arxiv.org/abs/2508.09241", "authors": ["Fengxian Ji", "Jingpu Yang", "Zirui Song", "Yuanxi Wang", "Zhexuan Cui", "Yuke Li", "Qian Jiang", "Miao Fang", "Xiuying Chen"], "title": "FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents", "comment": "submit/6682470 (Fengxian Ji)", "summary": "With the rapid advancement of generative artificial intelligence technology,\nGraphical User Interface (GUI) agents have demonstrated tremendous potential\nfor autonomously managing daily tasks through natural language instructions.\nHowever, current evaluation frameworks for GUI agents suffer from fundamental\nflaws: existing benchmarks overly focus on coarse-grained task completion while\nneglecting fine-grained control capabilities crucial for real-world\napplications. To address this, we introduce FineState-Bench, the first\nevaluation and diagnostic standard for fine-grained GUI proxy operations,\ndesigned to quantify fine-grained control. This multi-platform (desktop, Web,\nmobile) framework includes 2257 task benchmarks in four components and uses a\nfour-phase indicator for comprehensive perception-to-control assessment. To\nanalyze perception and positioning for refined operations, we developed the\nplug-and-play Visual Diagnostic Assistant (VDA), enabling the first\nquantitative decoupling analysis of these capabilities. Experimental results on\nour benchmark show that the most advanced models achieve only 32.8%\nfine-grained interaction accuracy. Using our VDA in controlled experiments,\nquantifying the impact of visual capabilities, we showed that ideal visual\nlocalization boosts Gemini-2.5-Flash's success rate by 14.9\\%. Our diagnostic\nframework confirms for the first time that the primary bottleneck for current\nGUI proxies is basic visual positioning capability.All resources are fully\nopen-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench\nhuggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FineState-Bench\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7ec6\u7c92\u5ea6GUI\u4ee3\u7406\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e73\u53f0\u4efb\u52a1\u57fa\u51c6\u548c\u89c6\u89c9\u8bca\u65ad\u5de5\u5177VDA\uff0c\u63ed\u793a\u4e86\u5f53\u524dGUI\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\u662f\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u8bc4\u4f30\u6846\u67b6\u8fc7\u4e8e\u5173\u6ce8\u7c97\u7c92\u5ea6\u4efb\u52a1\u5b8c\u6210\uff0c\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u5f15\u5165FineState-Bench\uff0c\u5305\u542b2257\u4e2a\u591a\u5e73\u53f0\u4efb\u52a1\u57fa\u51c6\uff0c\u91c7\u7528\u56db\u9636\u6bb5\u6307\u6807\u8bc4\u4f30\u611f\u77e5\u5230\u63a7\u5236\u7684\u5168\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86VDA\u5de5\u5177\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u7684\u5b9a\u91cf\u89e3\u8026\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u51c6\u786e\u7387\u4ec5\u4e3a32.8%\uff0c\u7406\u60f3\u89c6\u89c9\u5b9a\u4f4d\u53ef\u5c06Gemini-2.5-Flash\u7684\u6210\u529f\u7387\u63d0\u534714.9%\u3002", "conclusion": "\u5f53\u524dGUI\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\u662f\u57fa\u7840\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0cFineState-Bench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2508.09797", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09797", "abs": "https://arxiv.org/abs/2508.09797", "authors": ["Dongcheng Cao", "Jin Zhou", "Xian Wang", "Shuo Li"], "title": "FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning", "comment": null, "summary": "Agile flight for the quadrotor cable-suspended payload system is a formidable\nchallenge due to its underactuated, highly nonlinear, and hybrid dynamics.\nTraditional optimization-based methods often struggle with high computational\ncosts and the complexities of cable mode transitions, limiting their real-time\napplicability and maneuverability exploitation. In this letter, we present\nFLARE, a reinforcement learning (RL) framework that directly learns agile\nnavigation policy from high-fidelity simulation. Our method is validated across\nthree designed challenging scenarios, notably outperforming a state-of-the-art\noptimization-based approach by a 3x speedup during gate traversal maneuvers.\nFurthermore, the learned policies achieve successful zero-shot sim-to-real\ntransfer, demonstrating remarkable agility and safety in real-world\nexperiments, running in real time on an onboard computer.", "AI": {"tldr": "FLARE\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56db\u65cb\u7ffc\u60ac\u6302\u8d1f\u8f7d\u7cfb\u7edf\u7684\u654f\u6377\u98de\u884c\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u56db\u65cb\u7ffc\u60ac\u6302\u8d1f\u8f7d\u7cfb\u7edf\u7684\u52a8\u6001\u7279\u6027\u590d\u6742\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5904\u7406\u5b9e\u65f6\u6027\u548c\u673a\u52a8\u6027\u3002", "method": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u76f4\u63a5\u5b66\u4e60\u654f\u6377\u5bfc\u822a\u7b56\u7565\uff0c\u5e76\u9a8c\u8bc1\u4e8e\u4e09\u79cd\u6311\u6218\u6027\u573a\u666f\u3002", "result": "\u5728\u95e8\u7a7f\u8d8a\u4efb\u52a1\u4e2d\u901f\u5ea6\u63d0\u53473\u500d\uff0c\u4e14\u5b9e\u73b0\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "FLARE\u5c55\u793a\u4e86\u5728\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\u4e0a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.09245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09245", "abs": "https://arxiv.org/abs/2508.09245", "authors": ["Jeffri Murrugarra-LLerena", "Haoran Niu", "K. Suzanne Barber", "Hal Daum\u00e9 III", "Yang Trista Cao", "Paola Cascante-Bonilla"], "title": "Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users", "comment": null, "summary": "As visual assistant systems powered by visual language models (VLMs) become\nmore prevalent, concerns over user privacy have grown, particularly for blind\nand low vision users who may unknowingly capture personal private information\nin their images. Existing privacy protection methods rely on coarse-grained\nsegmentation, which uniformly masks entire private objects, often at the cost\nof usability. In this work, we propose FiGPriv, a fine-grained privacy\nprotection framework that selectively masks only high-risk private information\nwhile preserving low-risk information. Our approach integrates fine-grained\nsegmentation with a data-driven risk scoring mechanism. We evaluate our\nframework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%\nof image content, enhancing the ability of VLMs to provide useful responses by\n11% and identify the image content by 45%, while ensuring privacy protection.\nProject Page: https://artcs1.github.io/VLMPrivacy/", "AI": {"tldr": "FiGPriv\u662f\u4e00\u79cd\u7ec6\u7c92\u5ea6\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5c4f\u853d\u9ad8\u98ce\u9669\u9690\u79c1\u4fe1\u606f\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u52a9\u624b\u7cfb\u7edf\u4e2d\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u53ef\u80fd\u65e0\u610f\u4e2d\u6355\u83b7\u9690\u79c1\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u7c97\u7c92\u5ea6\u5206\u5272\u5bfc\u81f4\u53ef\u7528\u6027\u964d\u4f4e\u3002", "method": "\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5206\u5272\u548c\u6570\u636e\u9a71\u52a8\u7684\u98ce\u9669\u8bc4\u5206\u673a\u5236\uff0c\u9009\u62e9\u6027\u5c4f\u853d\u9ad8\u98ce\u9669\u9690\u79c1\u4fe1\u606f\u3002", "result": "\u5728BIV-Priv-Seg\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cFiGPriv\u4fdd\u755926%\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u63d0\u5347\u6a21\u578b\u54cd\u5e94\u80fd\u529b11%\uff0c\u8bc6\u522b\u80fd\u529b45%\u3002", "conclusion": "FiGPriv\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u7528\u6027\u548c\u529f\u80fd\u3002"}}
{"id": "2508.09836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09836", "abs": "https://arxiv.org/abs/2508.09836", "authors": ["Anirvan Dutta", "Alexis WM Devillard", "Zhihuan Zhang", "Xiaoxiao Cheng", "Etienne Burdet"], "title": "Embodied Tactile Perception of Soft Objects Properties", "comment": null, "summary": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u68b0\u987a\u5e94\u6027\u3001\u591a\u6a21\u6001\u4f20\u611f\u548c\u4ea4\u4e92\u7b56\u7565\u5982\u4f55\u5171\u540c\u5f71\u54cd\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u8bc1\u660e\u591a\u6a21\u6001\u4f20\u611f\u4f18\u4e8e\u5355\u6a21\u6001\u4f20\u611f\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u50cf\u4eba\u7c7b\u4e00\u6837\u7684\u7cbe\u7ec6\u64cd\u4f5c\uff0c\u9700\u8981\u7406\u89e3\u673a\u68b0\u987a\u5e94\u6027\u3001\u591a\u6a21\u6001\u4f20\u611f\u548c\u6709\u76ee\u7684\u4ea4\u4e92\u5982\u4f55\u5171\u540c\u5851\u9020\u89e6\u89c9\u611f\u77e5\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316e-Skin\u7cfb\u7edf\uff0c\u7ed3\u5408\u53ef\u8c03\u673a\u68b0\u987a\u5e94\u6027\u548c\u591a\u6a21\u6001\u4f20\u611f\uff08\u6cd5\u5411\u529b\u3001\u526a\u5207\u529b\u548c\u632f\u52a8\uff09\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u89e6\u8bca\u52a8\u4f5c\uff08\u6309\u538b\u3001\u6ed1\u52a8\u7b49\uff09\u7814\u7a76\u611f\u77e5\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u52a8\u4f5c\u6761\u4ef6\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u6f5c\u5728\u6ee4\u6ce2\u5668\uff09\u3002", "result": "\u591a\u6a21\u6001\u4f20\u611f\u4f18\u4e8e\u5355\u6a21\u6001\u4f20\u611f\uff0c\u63ed\u793a\u4e86e-Skin\u673a\u68b0\u7279\u6027\u4e0e\u73af\u5883\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u9700\u7ed3\u5408\u65f6\u95f4\u52a8\u6001\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u5f3a\u8c03\u4e86\u673a\u68b0\u7279\u6027\u548c\u4ea4\u4e92\u7b56\u7565\u5728\u89e6\u89c9\u611f\u77e5\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.09262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09262", "abs": "https://arxiv.org/abs/2508.09262", "authors": ["Dongwoo Kang", "Akhil Perincherry", "Zachary Coalson", "Aiden Gabriel", "Stefan Lee", "Sanghyun Hong"], "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "comment": "Accepted to ICCV 2025 [Poster]", "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f93\u5165\u81ea\u9002\u5e94\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u7b97\u6cd5\u63d0\u5347\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u6a21\u578b\u7684\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u8f93\u5165\u81ea\u9002\u5e94\u673a\u5236\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u9009\u62e9\u6027\u5904\u7406\u5168\u666f\u89c6\u56fe\u63d0\u5347\u7a7a\u95f4\u6548\u7387\uff1b2. \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u81ea\u9002\u5e94\u9608\u503c\u7528\u4e8e\u65e9\u671f\u9000\u51fa\u65b9\u6cd5\uff1b3. \u7f13\u5b58\u673a\u5236\u907f\u514d\u91cd\u590d\u5904\u7406\u5df2\u89c2\u5bdf\u89c6\u56fe\u3002", "result": "\u5728\u4e03\u4e2aVLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u8d85\u8fc72\u500d\uff0c\u4e14\u6027\u80fd\u672a\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLN\u6a21\u578b\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2508.09846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09846", "abs": "https://arxiv.org/abs/2508.09846", "authors": ["Donghoon Baek", "Amartya Purushottam", "Jason J. Choi", "Joao Ramos"], "title": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation", "comment": null, "summary": "This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8f6e\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7269\u4f53\u611f\u77e5\u5168\u8eab\u53cc\u8fb9\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u5728\u7ebf\u591a\u9636\u6bb5\u7269\u4f53\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\u6a21\u5757\uff0c\u63d0\u5347\u52a8\u6001\u540c\u6b65\u548c\u64cd\u4f5c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8f6e\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u5728\u642c\u8fd0\u4efb\u52a1\u4e2d\u56e0\u7269\u4f53\u52a8\u6001\u7279\u6027\uff08\u5982\u60ef\u6027\u53c2\u6570\uff09\u672a\u77e5\u800c\u5bfc\u81f4\u7684\u9065\u64cd\u4f5c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u7a0b\uff08\u89c6\u89c9\u5c3a\u5bf8\u4f30\u8ba1\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u731c\u6d4b\u3001\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff09\u5728\u7ebf\u4f30\u8ba1\u7269\u4f53\u60ef\u6027\u53c2\u6570\uff0c\u5e76\u5b9e\u65f6\u66f4\u65b0\u673a\u5668\u4eba\u5e73\u8861\u70b9\u3002", "result": "\u5728\u5b9a\u5236\u8f6e\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u6267\u884c\u642c\u8fd0\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u8d1f\u8f7d\u7ea6\u4e3a\u673a\u5668\u4eba\u91cd\u91cf\u7684\u4e09\u5206\u4e4b\u4e00\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8865\u507f\u7269\u4f53\u60ef\u6027\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u64cd\u4f5c\u7684\u52a8\u6001\u6027\u80fd\u548c\u64cd\u4f5c\u7cbe\u5ea6\u3002"}}
{"id": "2508.09325", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09325", "abs": "https://arxiv.org/abs/2508.09325", "authors": ["Alexandre Brown", "Glen Berseth"], "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning", "comment": null, "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks.", "AI": {"tldr": "SegDAC\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408SAM\u548cYOLO-World\u5b9e\u73b0\u5bf9\u8c61\u4e2d\u5fc3\u5206\u89e3\u548c\u8bed\u4e49\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u4ece\u9ad8\u7ef4\u8f93\u5165\u548c\u566a\u58f0\u5956\u52b1\u4e2d\u5b66\u4e60\u611f\u77e5\u548c\u52a8\u4f5c\uff0c\u73b0\u6709\u5927\u6a21\u578b\u5728RL\u4e2d\u7684\u6574\u5408\u6548\u679c\u4e0d\u660e\u786e\uff0cSegDAC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SegDAC\u7ed3\u5408SAM\u8fdb\u884c\u5bf9\u8c61\u4e2d\u5fc3\u5206\u89e3\uff0cYOLO-World\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8bed\u4e49\u6807\u6ce8\uff0c\u91c7\u7528\u65b0\u578bTransformer\u67b6\u6784\u52a8\u6001\u9009\u62e9\u5173\u6ce8\u7684\u5206\u5272\u533a\u57df\u3002", "result": "\u5728Maniskill3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSegDAC\u5728\u89c6\u89c9\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u96be\u4efb\u52a1\u6027\u80fd\u7ffb\u500d\uff0c\u6837\u672c\u6548\u7387\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SegDAC\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u5206\u5272\u548c\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2508.09855", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.09855", "abs": "https://arxiv.org/abs/2508.09855", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u901a\u8fc7RGB\u56fe\u50cf\u8bad\u7ec3\u4eba\u673a\u534f\u4f5c\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u573a\u666f\u751f\u6210\u6f14\u793a\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u907f\u514d\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u3002", "method": "\u5229\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4eba\u673a\u4ea4\u63a5\u573a\u666f\uff0c\u751f\u6210\u56fe\u50cf-\u52a8\u4f5c\u5bf9\u4f5c\u4e3a\u6f14\u793a\u3002", "result": "\u5728\u91cd\u5efa\u573a\u666f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u826f\u597d\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u63a5\u7684\u7a33\u5b9a\u6027\u548c\u907f\u969c\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u63d0\u4f9b\u65b0\u8868\u5f81\uff0c\u589e\u5f3a\u534f\u4f5c\u7684\u6d41\u7545\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09327", "abs": "https://arxiv.org/abs/2508.09327", "authors": ["Yifan Jiang", "Ahmad Shariftabrizi", "Venkata SK. Manem"], "title": "Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model", "comment": null, "summary": "Generative artificial intelligence (AI) has been playing an important role in\nvarious domains. Leveraging its high capability to generate high-fidelity and\ndiverse synthetic data, generative AI is widely applied in diagnostic tasks,\nsuch as lung cancer diagnosis using computed tomography (CT). However, existing\ngenerative models for lung cancer diagnosis suffer from low efficiency and\nanatomical imprecision, which limit their clinical applicability. To address\nthese drawbacks, we propose Lung-DDPM+, an improved version of our previous\nmodel, Lung-DDPM. This novel approach is a denoising diffusion probabilistic\nmodel (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary\nDPM-solver, enabling the method to focus on lesion areas while achieving a\nbetter trade-off between sampling efficiency and quality. Evaluation results on\nthe public LIDC-IDRI dataset suggest that the proposed method achieves\n8$\\times$ fewer FLOPs (floating point operations per second), 6.8$\\times$ lower\nGPU memory consumption, and 14$\\times$ faster sampling compared to Lung-DDPM.\nMoreover, it maintains comparable sample quality to both Lung-DDPM and other\nstate-of-the-art (SOTA) generative models in two downstream segmentation tasks.\nWe also conducted a Visual Turing Test by an experienced radiologist, showing\nthe advanced quality and fidelity of synthetic samples generated by the\nproposed method. These experimental results demonstrate that Lung-DDPM+ can\neffectively generate high-quality thoracic CT images with lung nodules,\nhighlighting its potential for broader applications, such as general tumor\nsynthesis and lesion generation in medical imaging. The code and pretrained\nmodels are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.", "AI": {"tldr": "Lung-DDPM+\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u80ba\u90e8CT\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u6548\u7387\u4f4e\u548c\u89e3\u5256\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u80ba\u90e8\u764c\u75c7\u8bca\u65ad\u4e2d\u6548\u7387\u4f4e\u4e14\u89e3\u5256\u4e0d\u7cbe\u786e\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faLung-DDPM+\uff0c\u4e00\u79cd\u57fa\u4e8e\u7ed3\u8282\u8bed\u4e49\u5e03\u5c40\u7684DDPM\u6a21\u578b\uff0c\u901a\u8fc7\u80ba\u90e8DPM-solver\u52a0\u901f\uff0c\u4f18\u5316\u91c7\u6837\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "result": "\u5728LIDC-IDRI\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51cf\u5c11\u4e868\u500dFLOPs\u30016.8\u500dGPU\u5185\u5b58\u6d88\u8017\uff0c\u91c7\u6837\u901f\u5ea6\u63d0\u534714\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6837\u672c\u8d28\u91cf\u3002", "conclusion": "Lung-DDPM+\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u80ba\u90e8CT\u56fe\u50cf\uff0c\u5177\u6709\u5e7f\u6cdb\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09876", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.09876", "abs": "https://arxiv.org/abs/2508.09876", "authors": ["Xiaowei Tan", "Weizhong Jiang", "Bi Zhang", "Wanxin Chen", "Yiwen Zhao", "Ning Li", "Lianqing Liu", "Xingang Zhao"], "title": "A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion", "comment": "49 pages, 20 figures, 4 tables", "summary": "Exoskeletons have been shown to effectively assist humans during steady\nlocomotion. However, their effects on non-steady locomotion, characterized by\nnonlinear phase progression within a gait cycle, remain insufficiently\nexplored, particularly across diverse activities. This work presents a shank\nangle-based control system that enables the exoskeleton to maintain real-time\ncoordination with human gait, even under phase perturbations, while dynamically\nshaping assistance profiles to match the biological ankle moment patterns\nacross walking, running, stair negotiation tasks. The control system consists\nof an assistance profile online generation method and a model-based feedforward\ncontrol method. The assistance profile is formulated as a dual-Gaussian model\nwith the shank angle as the independent variable. Leveraging only IMU\nmeasurements, the model parameters are updated online each stride to adapt to\ninter- and intra-individual biomechanical variability. The profile tracking\ncontrol employs a human-exoskeleton kinematics and stiffness model as a\nfeedforward component, reducing reliance on historical control data due to the\nlack of clear and consistent periodicity in non-steady locomotion. Three\nexperiments were conducted using a lightweight soft exoskeleton with multiple\nsubjects. The results validated the effectiveness of each individual method,\ndemonstrated the robustness of the control system against gait perturbations\nacross various activities, and revealed positive biomechanical and\nphysiological responses of human users to the exoskeleton's mechanical\nassistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u817f\u89d2\u5ea6\u7684\u5916\u9aa8\u9abc\u63a7\u5236\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u975e\u7a33\u6001\u6b65\u6001\uff08\u5982\u884c\u8d70\u3001\u8dd1\u6b65\u3001\u4e0a\u4e0b\u697c\u68af\uff09\u4e2d\u5b9e\u65f6\u534f\u8c03\u5e76\u63d0\u4f9b\u8f85\u52a9\uff0c\u901a\u8fc7\u53cc\u9ad8\u65af\u6a21\u578b\u52a8\u6001\u8c03\u6574\u8f85\u52a9\u6a21\u5f0f\u3002", "motivation": "\u7814\u7a76\u5916\u9aa8\u9abc\u5728\u975e\u7ebf\u6027\u6b65\u6001\uff08\u5982\u884c\u8d70\u3001\u8dd1\u6b65\u3001\u4e0a\u4e0b\u697c\u68af\uff09\u4e2d\u7684\u5b9e\u65f6\u534f\u8c03\u4e0e\u8f85\u52a9\u6548\u679c\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c0f\u817f\u89d2\u5ea6\u7684\u53cc\u9ad8\u65af\u6a21\u578b\u751f\u6210\u8f85\u52a9\u6a21\u5f0f\uff0c\u7ed3\u5408\u6a21\u578b\u524d\u9988\u63a7\u5236\uff0c\u4ec5\u9700IMU\u6570\u636e\u5b9e\u65f6\u66f4\u65b0\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63a7\u5236\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\uff0c\u5e76\u663e\u793a\u5916\u9aa8\u9abc\u8f85\u52a9\u5bf9\u7528\u6237\u7684\u751f\u7269\u529b\u5b66\u548c\u751f\u7406\u53cd\u5e94\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8be5\u63a7\u5236\u7cfb\u7edf\u80fd\u9002\u5e94\u591a\u6837\u5316\u7684\u975e\u7a33\u6001\u6b65\u6001\uff0c\u4e3a\u5916\u9aa8\u9abc\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09339", "abs": "https://arxiv.org/abs/2508.09339", "authors": ["Aqsa Sultana", "Nordin Abouzahra", "Ahmed Rahu", "Brian Shula", "Brandon Combs", "Derrick Forchetti", "Theus Aspiras", "Vijayan K. Asari"], "title": "UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas", "comment": null, "summary": "Identification of precancerous polyps during routine colonoscopy screenings\nis vital for their excision, lowering the risk of developing colorectal cancer.\nAdvanced deep learning algorithms enable precise adenoma classification and\nstratification, improving risk assessment accuracy and enabling personalized\nsurveillance protocols that optimize patient outcomes. Ultralight Med-Vision\nMamba, a state-space based model (SSM), has excelled in modeling long- and\nshort-range dependencies and image generalization, critical factors for\nanalyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's\nefficient architecture offers advantages in both computational speed and\nscalability, making it a promising tool for real-time clinical deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684Ultralight Med-Vision Mamba\uff0c\u7528\u4e8e\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u764c\u524d\u606f\u8089\u7684\u8bc6\u522b\u548c\u5206\u7c7b\uff0c\u63d0\u9ad8\u98ce\u9669\u8bc4\u4f30\u51c6\u786e\u6027\u5e76\u4f18\u5316\u60a3\u8005\u968f\u8bbf\u65b9\u6848\u3002", "motivation": "\u8bc6\u522b\u548c\u5206\u7c7b\u764c\u524d\u606f\u8089\u5bf9\u964d\u4f4e\u7ed3\u76f4\u80a0\u764c\u98ce\u9669\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u957f\u77ed\u671f\u4f9d\u8d56\u548c\u56fe\u50cf\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Ultralight Med-Vision Mamba\u6a21\u578b\uff0c\u5229\u7528\u5176\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7279\u6027\uff0c\u9ad8\u6548\u5904\u7406\u957f\u77ed\u671f\u4f9d\u8d56\u548c\u56fe\u50cf\u6cdb\u5316\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u5b9e\u65f6\u4e34\u5e8a\u90e8\u7f72\u3002", "conclusion": "Ultralight Med-Vision Mamba\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u53ef\u63d0\u5347\u7ed3\u80a0\u955c\u68c0\u67e5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09950", "abs": "https://arxiv.org/abs/2508.09950", "authors": ["Bida Ma", "Nuo Xu", "Chenkun Qi", "Xin Liu", "Yule Mo", "Jinkai Wang", "Chunpeng Lu"], "title": "PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces", "comment": null, "summary": "The legged locomotion in spatially constrained structures (called crawl\nspaces) is challenging. In crawl spaces, current exteroceptive locomotion\nlearning methods are limited by large noises and errors of the sensors in\npossible low visibility conditions, and current proprioceptive locomotion\nlearning methods are difficult in traversing crawl spaces because only ground\nfeatures are inferred. In this study, a point cloud supervised proprioceptive\nlocomotion reinforcement learning method for legged robots in crawl spaces is\nproposed. A state estimation network is designed to estimate the robot's\nsurrounding ground and spatial features as well as the robot's collision states\nusing historical proprioceptive sensor data. The point cloud is represented in\npolar coordinate frame and a point cloud processing method is proposed to\nefficiently extract the ground and spatial features that are used to supervise\nthe state estimation network learning. Comprehensive reward functions that\nguide the robot to traverse through crawl spaces after collisions are designed.\nExperiments demonstrate that, compared to existing methods, our method exhibits\nmore agile locomotion in crawl spaces. This study enhances the ability of\nlegged robots to traverse spatially constrained environments without requiring\nexteroceptive sensors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u76d1\u7763\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u611f\u77e5\u4f20\u611f\u5668\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u56e0\u4f20\u611f\u5668\u566a\u58f0\u548c\u4f4e\u53ef\u89c1\u6027\u6761\u4ef6\u53d7\u9650\uff0c\u6216\u4ec5\u80fd\u63a8\u65ad\u5730\u9762\u7279\u5f81\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8fd0\u52a8\u3002", "method": "\u8bbe\u8ba1\u4e86\u72b6\u6001\u4f30\u8ba1\u7f51\u7edc\uff0c\u5229\u7528\u5386\u53f2\u672c\u4f53\u611f\u77e5\u6570\u636e\u4f30\u8ba1\u5468\u56f4\u73af\u5883\u7279\u5f81\u548c\u78b0\u649e\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u70b9\u4e91\u5904\u7406\u65b9\u6cd5\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u66f4\u654f\u6377\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u65e0\u5916\u90e8\u4f20\u611f\u5668\u6761\u4ef6\u4e0b\u7684\u72ed\u7a84\u73af\u5883\u7a7f\u8d8a\u80fd\u529b\u3002"}}
{"id": "2508.09344", "categories": ["cs.CV", "68T45, 92C55", "H.5.2; I.2.10; J.3"], "pdf": "https://arxiv.org/pdf/2508.09344", "abs": "https://arxiv.org/abs/2508.09344", "authors": ["Anushka Bhatt"], "title": "Blink-to-code: real-time Morse code communication via eye blink detection and classification", "comment": "4 pages, 4 figures. Preprint on blink-based Morse code communication\n  via webcam for assistive technology. Relevant to computer vision and\n  human-computer interaction", "summary": "This study proposes a real-time system that translates voluntary eye blinks\ninto Morse code, enabling communication for individuals with severe motor\nimpairments. Using a standard webcam and computer vision, the system detects\nand classifies blinks as short (dot) or long (dash), then decodes them into\nalphanumeric characters. Experiments with five participants show 62% decoding\naccuracy and 18-20 seconds response times, demonstrating a viable, low-cost\nassistive communication method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u7cfb\u7edf\uff0c\u5c06\u7728\u773c\u52a8\u4f5c\u8f6c\u6362\u4e3a\u6469\u5c14\u65af\u7535\u7801\uff0c\u5e2e\u52a9\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u8fdb\u884c\u4ea4\u6d41\u3002", "motivation": "\u4e3a\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u53ef\u884c\u7684\u8f85\u52a9\u901a\u4fe1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u6444\u50cf\u5934\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u68c0\u6d4b\u5e76\u5206\u7c7b\u7728\u773c\u4e3a\u77ed\uff08\u70b9\uff09\u6216\u957f\uff08\u5212\uff09\uff0c\u518d\u89e3\u7801\u4e3a\u5b57\u6bcd\u6570\u5b57\u5b57\u7b26\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u89e3\u7801\u51c6\u786e\u7387\u4e3a62%\uff0c\u54cd\u5e94\u65f6\u95f4\u4e3a18-20\u79d2\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u4e00\u79cd\u53ef\u884c\u7684\u4f4e\u6210\u672c\u8f85\u52a9\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09960", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09960", "abs": "https://arxiv.org/abs/2508.09960", "authors": ["Yifei Yao", "Chengyuan Luo", "Jiaheng Du", "Wentao He", "Jun-Guo Lu"], "title": "GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation", "comment": null, "summary": "The creation of human-like humanoid robots is hindered by a fundamental\nfragmentation: data processing and learning algorithms are rarely universal\nacross different robot morphologies. This paper introduces the Generalized\nBehavior Cloning (GBC) framework, a comprehensive and unified solution designed\nto solve this end-to-end challenge. GBC establishes a complete pathway from\nhuman motion to robot action through three synergistic innovations. First, an\nadaptive data pipeline leverages a differentiable IK network to automatically\nretarget any human MoCap data to any humanoid. Building on this foundation, our\nnovel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,\nhigh-fidelity imitation policies. To complete the ecosystem, the entire\nframework is delivered as an efficient, open-source platform based on Isaac\nLab, empowering the community to deploy the full workflow via simple\nconfiguration scripts. We validate the power and generality of GBC by training\npolicies on multiple heterogeneous humanoids, demonstrating excellent\nperformance and transfer to novel motions. This work establishes the first\npractical and unified pathway for creating truly generalized humanoid\ncontrollers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u884c\u4e3a\u514b\u9686\uff08GBC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u9053\u3001DAgger-MMPPO\u7b97\u6cd5\u548c\u5f00\u6e90\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u52a8\u4f5c\u5230\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u5f00\u53d1\u4e2d\uff0c\u6570\u636e\u5904\u7406\u548c\u5b66\u4e60\u7b97\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u963b\u788d\u4e86\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002", "method": "GBC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u9053\u3001DAgger-MMPPO\u7b97\u6cd5\u548c\u57fa\u4e8eIsaac Lab\u7684\u5f00\u6e90\u5e73\u53f0\u3002", "result": "\u5728\u591a\u79cd\u5f02\u6784\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86GBC\u7684\u4f18\u5f02\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GBC\u4e3a\u521b\u5efa\u901a\u7528\u4eba\u5f62\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u7528\u4e14\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09362", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09362", "abs": "https://arxiv.org/abs/2508.09362", "authors": ["Md. Milon Islam", "Md Rezwanul Haque", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition", "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Accurate recognition of sign language in healthcare communication poses a\nsignificant challenge, requiring frameworks that can accurately interpret\ncomplex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,\na novel attention-based ensemble of spatiotemporal networks that dynamically\nfuses visual and motion data to enhance recognition accuracy. The proposed\napproach processes RGB video and range Doppler map radar modalities\nsynchronously through four different spatiotemporal networks. For each network,\nfeatures from both modalities are continuously fused using an attention-based\nfusion module before being fed into an ensemble of classifiers. Finally, the\noutputs of these four different fused channels are combined in an ensemble\nclassification head, thereby enhancing the model's robustness. Experiments\ndemonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches\nwith a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for\nItalian Sign Language. Our findings indicate that an ensemble of diverse\nspatiotemporal networks, unified by attention-based fusion, yields a robust and\naccurate framework for complex, multimodal isolated gesture recognition tasks.\nThe source code is available at:\nhttps://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.", "AI": {"tldr": "FusionEnsemble-Net\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65f6\u7a7a\u7f51\u7edc\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u878d\u5408\u89c6\u89c9\u548c\u8fd0\u52a8\u6570\u636e\u63d0\u5347\u624b\u8bed\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe99.44%\u3002", "motivation": "\u533b\u7597\u6c9f\u901a\u4e2d\u7684\u624b\u8bed\u8bc6\u522b\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u80fd\u51c6\u786e\u89e3\u91ca\u590d\u6742\u591a\u6a21\u6001\u624b\u52bf\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faFusionEnsemble-Net\uff0c\u540c\u6b65\u5904\u7406RGB\u89c6\u9891\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u901a\u8fc7\u56db\u4e2a\u65f6\u7a7a\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u52a8\u6001\u878d\u5408\u7279\u5f81\uff0c\u6700\u7ec8\u901a\u8fc7\u96c6\u6210\u5206\u7c7b\u5668\u8f93\u51fa\u7ed3\u679c\u3002", "result": "\u5728\u610f\u5927\u5229\u624b\u8bed\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.44%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6ce8\u610f\u529b\u878d\u5408\u7684\u591a\u6837\u5316\u65f6\u7a7a\u7f51\u7edc\u96c6\u6210\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u6a21\u6001\u624b\u52bf\u8bc6\u522b\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u6846\u67b6\u3002"}}
{"id": "2508.09971", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09971", "abs": "https://arxiv.org/abs/2508.09971", "authors": ["Zihan Wang", "Nina Mahmoudian"], "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model", "comment": "Submitted to Robotics and Autonomous Systems (RAS) journal", "summary": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is\ncritical for applications such as rescue, surveillance, and environmental\nmonitoring, particularly in dense riverine environments where GPS signals are\nunreliable. We formalize river following as a coverage control problem in which\nthe reward function is submodular, yielding diminishing returns as more unique\nriver segments are visited, thereby framing the task as a Submodular Markov\nDecision Process. First, we introduce Marginal Gain Advantage Estimation, which\nrefines the reward advantage function by using a sliding window baseline\ncomputed from historical episodic returns, thus aligning the advantage\nestimation with the agent's evolving recognition of action value in\nnon-Markovian settings. Second, we develop a Semantic Dynamics Model based on\npatchified water semantic masks that provides more interpretable and\ndata-efficient short-term prediction of future observations compared to latent\nvision dynamics models. Third, we present the Constrained Actor Dynamics\nEstimator architecture, which integrates the actor, the cost estimator, and SDM\nfor cost advantage estimation to form a model-based SafeRL framework capable of\nsolving partially observable Constrained Submodular Markov Decision Processes.\nSimulation results demonstrate that MGAE achieves faster convergence and\nsuperior performance over traditional critic-based methods like Generalized\nAdvantage Estimation. SDM provides more accurate short-term state predictions\nthat enable the cost estimator to better predict potential violations. Overall,\nCADE effectively integrates safety regulation into model-based RL, with the\nLagrangian approach achieving the soft balance of reward and safety during\ntraining, while the safety layer enhances performance during inference by hard\naction overlay.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6cb3\u6d41\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u6a21\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3001\u8fb9\u9645\u589e\u76ca\u4f18\u52bf\u4f30\u8ba1\u548c\u8bed\u4e49\u52a8\u6001\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "motivation": "\u5728GPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u5bc6\u96c6\u6cb3\u6d41\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u81ea\u4e3b\u6cb3\u6d41\u8ddf\u8e2a\u5bf9\u6551\u63f4\u3001\u76d1\u89c6\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u8fb9\u9645\u589e\u76ca\u4f18\u52bf\u4f30\u8ba1\uff08MGAE\uff09\u4f18\u5316\u5956\u52b1\u4f18\u52bf\u51fd\u6570\uff1b2. \u57fa\u4e8e\u8bed\u4e49\u63a9\u7801\u7684\u8bed\u4e49\u52a8\u6001\u6a21\u578b\uff08SDM\uff09\u63d0\u4f9b\u77ed\u671f\u9884\u6d4b\uff1b3. \u7ea6\u675f\u6f14\u5458\u52a8\u6001\u4f30\u8ba1\u5668\uff08CADE\uff09\u6574\u5408\u6a21\u578b\u548c\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3002", "result": "MGAE\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u66f4\u4f18\uff1bSDM\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u77ed\u671f\u72b6\u6001\u9884\u6d4b\uff1bCADE\u6210\u529f\u6574\u5408\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6cb3\u6d41\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2508.09372", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09372", "abs": "https://arxiv.org/abs/2508.09372", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition", "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Continuous Sign Language Recognition (CSLR) faces multiple challenges,\nincluding significant inter-signer variability and poor generalization to novel\nsentence structures. Traditional solutions frequently fail to handle these\nissues efficiently. For overcoming these constraints, we propose a\ndual-architecture framework. For the Signer-Independent (SI) challenge, we\npropose a Signer-Invariant Conformer that combines convolutions with multi-head\nself-attention to learn robust, signer-agnostic representations from pose-based\nskeletal keypoints. For the Unseen-Sentences (US) task, we designed a\nMulti-Scale Fusion Transformer with a novel dual-path temporal encoder that\ncaptures both fine-grained posture dynamics, enabling the model's ability to\ncomprehend novel grammatical compositions. Experiments on the challenging\nIsharah-1000 dataset establish a new standard for both CSLR benchmarks. The\nproposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on\nthe SI challenge, a reduction of 13.53% from the state-of-the-art. On the US\ntask, the transformer model scores a WER of 47.78%, surpassing previous work.\nIn the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th\nin the SI task, demonstrating the performance of these models. The findings\nvalidate our key hypothesis: that developing task-specific networks designed\nfor the particular challenges of CSLR leads to considerable performance\nimprovements and establishes a new baseline for further research. The source\ncode is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u67b6\u6784\u6846\u67b6\uff0c\u5206\u522b\u9488\u5bf9\u624b\u8bed\u8bc6\u522b\u4e2d\u7684\u72ec\u7acb\u4e8e\u8bf4\u8bdd\u8005\uff08SI\uff09\u548c\u672a\u89c1\u53e5\u5b50\uff08US\uff09\u95ee\u9898\uff0c\u901a\u8fc7Signer-Invariant Conformer\u548cMulti-Scale Fusion Transformer\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u4e2d\u8bf4\u8bdd\u8005\u95f4\u5dee\u5f02\u5927\u548c\u5bf9\u65b0\u53e5\u5b50\u7ed3\u6784\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u67b6\u6784\u6846\u67b6\uff1aSigner-Invariant Conformer\u7528\u4e8eSI\u4efb\u52a1\uff0c\u7ed3\u5408\u5377\u79ef\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff1bMulti-Scale Fusion Transformer\u7528\u4e8eUS\u4efb\u52a1\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u65f6\u95f4\u7f16\u7801\u5668\u3002", "result": "\u5728Isharah-1000\u6570\u636e\u96c6\u4e0a\uff0cSI\u4efb\u52a1WER\u964d\u81f313.07%\uff08\u63d0\u534713.53%\uff09\uff0cUS\u4efb\u52a1WER\u4e3a47.78%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7f51\u7edc\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u5347CSLR\u6027\u80fd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.09976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09976", "abs": "https://arxiv.org/abs/2508.09976", "authors": ["Marion Lepert", "Jiaying Fang", "Jeannette Bohg"], "title": "Masquerade: Learning from In-the-wild Human Videos using Data-Editing", "comment": "Project website at https://masquerade-robot.github.io/", "summary": "Robot manipulation research still suffers from significant data scarcity:\neven the largest robot datasets are orders of magnitude smaller and less\ndiverse than those that fueled recent breakthroughs in language and vision. We\nintroduce Masquerade, a method that edits in-the-wild egocentric human videos\nto bridge the visual embodiment gap between humans and robots and then learns a\nrobot policy with these edited videos. Our pipeline turns each human video into\nrobotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the\nhuman arms, and (iii) overlaying a rendered bimanual robot that tracks the\nrecovered end-effector trajectories. Pre-training a visual encoder to predict\nfuture 2-D robot keypoints on 675K frames of these edited clips, and continuing\nthat auxiliary loss while fine-tuning a diffusion policy head on only 50 robot\ndemonstrations per task, yields policies that generalize significantly better\nthan prior work. On three long-horizon, bimanual kitchen tasks evaluated in\nthree unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations\nshow that both the robot overlay and co-training are indispensable, and\nperformance scales logarithmically with the amount of edited human video. These\nresults demonstrate that explicitly closing the visual embodiment gap unlocks a\nvast, readily available source of data from human videos that can be used to\nimprove robot policies.", "AI": {"tldr": "Masquerade\u65b9\u6cd5\u901a\u8fc7\u7f16\u8f91\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u673a\u5668\u4eba\u6f14\u793a\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u73b0\u6709\u6570\u636e\u96c6\u8fdc\u5c0f\u4e8e\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc73D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u3001\u4eba\u7c7b\u624b\u81c2\u4fee\u590d\u548c\u673a\u5668\u4eba\u8986\u76d6\uff0c\u5c06\u4eba\u7c7b\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u6f14\u793a\uff0c\u5e76\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u672a\u89c1\u8fc7\u7684\u53a8\u623f\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u6bd4\u57fa\u7ebf\u9ad85-6\u500d\u3002", "conclusion": "\u901a\u8fc7\u7f29\u5c0f\u89c6\u89c9\u4f53\u73b0\u5dee\u8ddd\uff0c\u53ef\u4ee5\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u3002"}}
{"id": "2508.09381", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09381", "abs": "https://arxiv.org/abs/2508.09381", "authors": ["Kumar Abhishek", "Jeremy Kawahara", "Ghassan Hamarneh"], "title": "What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?", "comment": "Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2025; 12 pages, 4 tables, 3\n  figures", "summary": "Medical image segmentation exhibits intra- and inter-annotator variability\ndue to ambiguous object boundaries, annotator preferences, expertise, and\ntools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated\nor infiltrative nodules, or irregular borders per the ABCD rule, are\nparticularly prone to disagreement and are often associated with malignancy. In\nthis work, we curate IMA++, the largest multi-annotator skin lesion\nsegmentation dataset, on which we conduct an in-depth study of variability due\nto annotator, malignancy, tool, and skill factors. We find a statistically\nsignificant (p<0.001) association between inter-annotator agreement (IAA),\nmeasured using Dice, and the malignancy of skin lesions. We further show that\nIAA can be accurately predicted directly from dermoscopic images, achieving a\nmean absolute error of 0.108. Finally, we leverage this association by\nutilizing IAA as a \"soft\" clinical feature within a multi-task learning\nobjective, yielding a 4.2% improvement in balanced accuracy averaged across\nmultiple model architectures and across IMA++ and four public dermoscopic\ndatasets. The code is available at https://github.com/sfu-mial/skin-IAV.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6ce8\u91ca\u8005\u95f4\u7684\u53d8\u5f02\u6027\uff0c\u5c24\u5176\u662f\u76ae\u80a4\u75c5\u53d8\u7684\u6076\u6027\u7a0b\u5ea6\u4e0e\u6ce8\u91ca\u8005\u4e00\u81f4\u6027\uff08IAA\uff09\u7684\u5173\u8054\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528IAA\u4f5c\u4e3a\u8f6f\u7279\u5f81\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6ce8\u91ca\u8005\u95f4\u7684\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6a21\u7cca\u8fb9\u754c\u75c5\u53d8\uff08\u5982\u6076\u6027\u76ae\u80a4\u75c5\u53d8\uff09\u7684\u6ce8\u91ca\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efaIMA++\u6570\u636e\u96c6\uff0c\u7814\u7a76\u6ce8\u91ca\u8005\u3001\u6076\u6027\u7a0b\u5ea6\u3001\u5de5\u5177\u548c\u6280\u80fd\u5bf9\u53d8\u5f02\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528IAA\u9884\u6d4b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u6076\u6027\u7a0b\u5ea6\u4e0eIAA\u663e\u8457\u76f8\u5173\uff08p<0.001\uff09\uff0cIAA\u9884\u6d4b\u8bef\u5dee\u4e3a0.108\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u4f7f\u5e73\u8861\u51c6\u786e\u7387\u63d0\u53474.2%\u3002", "conclusion": "IAA\u53ef\u4f5c\u4e3a\u4e34\u5e8a\u7279\u5f81\u63d0\u5347\u5206\u5272\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6a21\u7cca\u8fb9\u754c\u75c5\u53d8\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09383", "abs": "https://arxiv.org/abs/2508.09383", "authors": ["Guoxian Song", "Hongyi Xu", "Xiaochen Zhao", "You Xie", "Tianpei Gu", "Zenan Li", "Chenxu Zhang", "Linjie Luo"], "title": "X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents", "comment": null, "summary": "We present X-UniMotion, a unified and expressive implicit latent\nrepresentation for whole-body human motion, encompassing facial expressions,\nbody poses, and hand gestures. Unlike prior motion transfer methods that rely\non explicit skeletal poses and heuristic cross-identity adjustments, our\napproach encodes multi-granular motion directly from a single image into a\ncompact set of four disentangled latent tokens -- one for facial expression,\none for body pose, and one for each hand. These motion latents are both highly\nexpressive and identity-agnostic, enabling high-fidelity, detailed\ncross-identity motion transfer across subjects with diverse identities, poses,\nand spatial configurations. To achieve this, we introduce a self-supervised,\nend-to-end framework that jointly learns the motion encoder and latent\nrepresentation alongside a DiT-based video generative model, trained on\nlarge-scale, diverse human motion datasets. Motion-identity disentanglement is\nenforced via 2D spatial and color augmentations, as well as synthetic 3D\nrenderings of cross-identity subject pairs under shared poses. Furthermore, we\nguide motion token learning with auxiliary decoders that promote fine-grained,\nsemantically aligned, and depth-aware motion embeddings. Extensive experiments\nshow that X-UniMotion outperforms state-of-the-art methods, producing highly\nexpressive animations with superior motion fidelity and identity preservation.", "AI": {"tldr": "X-UniMotion\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u9690\u5f0f\u6f5c\u5728\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5168\u8eab\u4eba\u4f53\u8fd0\u52a8\uff0c\u5305\u62ec\u9762\u90e8\u8868\u60c5\u3001\u8eab\u4f53\u59ff\u52bf\u548c\u624b\u52bf\u3002\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u8de8\u8eab\u4efd\u7684\u8fd0\u52a8\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u9aa8\u9abc\u59ff\u52bf\u548c\u542f\u53d1\u5f0f\u8de8\u8eab\u4efd\u8c03\u6574\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u8de8\u8eab\u4efd\u7684\u8fd0\u52a8\u8fc1\u79fb\u3002X-UniMotion\u65e8\u5728\u901a\u8fc7\u9690\u5f0f\u6f5c\u5728\u8868\u793a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5c06\u591a\u7c92\u5ea6\u8fd0\u52a8\u7f16\u7801\u4e3a\u56db\u4e2a\u89e3\u8026\u7684\u6f5c\u5728\u6807\u8bb0\uff08\u9762\u90e8\u3001\u8eab\u4f53\u3001\u53cc\u624b\uff09\u3002\u901a\u8fc72D\u548c3D\u589e\u5f3a\u6280\u672f\u5b9e\u73b0\u8fd0\u52a8\u4e0e\u8eab\u4efd\u7684\u5206\u79bb\uff0c\u5e76\u5229\u7528\u8f85\u52a9\u89e3\u7801\u5668\u4f18\u5316\u6f5c\u5728\u6807\u8bb0\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-UniMotion\u5728\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u8868\u8fbe\u7684\u52a8\u753b\u3002", "conclusion": "X-UniMotion\u901a\u8fc7\u9690\u5f0f\u6f5c\u5728\u8868\u793a\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u8de8\u8eab\u4efd\u7684\u8fd0\u52a8\u8fc1\u79fb\uff0c\u4e3a\u5168\u8eab\u8fd0\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09423", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09423", "abs": "https://arxiv.org/abs/2508.09423", "authors": ["Badi Li", "Ren-jie Lu", "Yu Zhou", "Jingke Meng", "Wei-shi Zheng"], "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation", "comment": null, "summary": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a\nspecified object in an unseen environment by imagining unobserved regions of\nthe scene. Prior approaches rely on deterministic and discriminative models to\ncomplete semantic maps, overlooking the inherent uncertainty in indoor layouts\nand limiting their ability to generalize to unseen environments. In this work,\nwe propose GOAL, a generative flow-based framework that models the semantic\ndistribution of indoor environments by bridging observed regions with\nLLM-enriched full-scene semantic maps. During training, spatial priors inferred\nfrom large language models (LLMs) are encoded as two-dimensional Gaussian\nfields and injected into target maps, distilling rich contextual knowledge into\nthe flow model and enabling more generalizable completions. Extensive\nexperiments demonstrate that GOAL achieves state-of-the-art performance on MP3D\nand Gibson, and shows strong generalization in transfer settings to HM3D. Codes\nand pretrained models are available at https://github.com/Badi-Li/GOAL.", "AI": {"tldr": "GOAL\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6d41\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u589e\u5f3a\u7684\u5168\u573a\u666f\u8bed\u4e49\u5730\u56fe\uff0c\u5efa\u6a21\u5ba4\u5185\u73af\u5883\u7684\u8bed\u4e49\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86ObjectNav\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u6a21\u578b\u5b8c\u6210\u8bed\u4e49\u5730\u56fe\uff0c\u5ffd\u7565\u4e86\u5ba4\u5185\u5e03\u5c40\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528LLM\u63a8\u65ad\u7a7a\u95f4\u5148\u9a8c\uff0c\u7f16\u7801\u4e3a\u4e8c\u7ef4\u9ad8\u65af\u573a\u5e76\u6ce8\u5165\u76ee\u6807\u5730\u56fe\uff0c\u901a\u8fc7\u751f\u6210\u6d41\u6a21\u578b\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u8865\u5168\u3002", "result": "\u5728MP3D\u548cGibson\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728HM3D\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GOAL\u901a\u8fc7\u751f\u6210\u6d41\u6a21\u578b\u548cLLM\u5148\u9a8c\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86ObjectNav\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09392", "abs": "https://arxiv.org/abs/2508.09392", "authors": ["Kang Ni", "Minrui Zou", "Yuxuan Li", "Xiang Li", "Kehua Guo", "Ming-Ming Cheng", "Yimian Dai"], "title": "DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection", "comment": null, "summary": "One of the primary challenges in Synthetic Aperture Radar (SAR) object\ndetection lies in the pervasive influence of coherent noise. As a common\npractice, most existing methods, whether handcrafted approaches or deep\nlearning-based methods, employ the analysis or enhancement of object\nspatial-domain characteristics to achieve implicit denoising. In this paper, we\npropose DenoDet V2, which explores a completely novel and different perspective\nto deconstruct and modulate the features in the transform domain via a\ncarefully designed attention architecture. Compared to DenoDet V1, DenoDet V2\nis a major advancement that exploits the complementary nature of amplitude and\nphase information through a band-wise mutual modulation mechanism, which\nenables a reciprocal enhancement between phase and amplitude spectra. Extensive\nexperiments on various SAR datasets demonstrate the state-of-the-art\nperformance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\\%\nimprovement on SARDet-100K dataset compared to DenoDet V1, while reducing the\nmodel complexity by half. The code is available at\nhttps://github.com/GrokCV/GrokSAR.", "AI": {"tldr": "DenoDet V2\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53d8\u6362\u57df\u7279\u5f81\u89e3\u6784\u548c\u8c03\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u67b6\u6784\u548c\u6ce2\u6bb5\u4e92\u8c03\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SAR\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SAR\u76ee\u6807\u68c0\u6d4b\u4e2d\u76f8\u5e72\u566a\u58f0\u7684\u666e\u904d\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u7a7a\u95f4\u57df\u7279\u5f81\u5206\u6790\u6216\u589e\u5f3a\uff0c\u7f3a\u4e4f\u5bf9\u53d8\u6362\u57df\u7279\u5f81\u7684\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u901a\u8fc7\u6ce2\u6bb5\u4e92\u8c03\u673a\u5236\u5229\u7528\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0c\u5b9e\u73b0\u76f8\u4f4d\u548c\u632f\u5e45\u8c31\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5728\u591a\u4e2aSAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cSARDet-100K\u6570\u636e\u96c6\u4e0a\u6bd4DenoDet V1\u63d0\u53470.8%\uff0c\u540c\u65f6\u6a21\u578b\u590d\u6742\u5ea6\u51cf\u534a\u3002", "conclusion": "DenoDet V2\u901a\u8fc7\u53d8\u6362\u57df\u7279\u5f81\u8c03\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86SAR\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u66f4\u9ad8\u6548\u3002"}}
{"id": "2508.09560", "categories": ["cs.CV", "cs.RO", "I.4.10"], "pdf": "https://arxiv.org/pdf/2508.09560", "abs": "https://arxiv.org/abs/2508.09560", "authors": ["Jiahao Wen", "Hang Yu", "Zhedong Zheng"], "title": "WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization", "comment": "13 pages, 4figures", "summary": "Visual geo-localization for drones faces critical degradation under weather\nperturbations, \\eg, rain and fog, where existing methods struggle with two\ninherent limitations: 1) Heavy reliance on limited weather categories that\nconstrain generalization, and 2) Suboptimal disentanglement of entangled\nscene-weather features through pseudo weather categories. We present\nWeatherPrompt, a multi-modality learning paradigm that establishes\nweather-invariant representations through fusing the image embedding with the\ntext context. Our framework introduces two key contributions: First, a\nTraining-free Weather Reasoning mechanism that employs off-the-shelf large\nmulti-modality models to synthesize multi-weather textual descriptions through\nhuman-like reasoning. It improves the scalability to unseen or complex weather,\nand could reflect different weather strength. Second, to better disentangle the\nscene and weather feature, we propose a multi-modality framework with the\ndynamic gating mechanism driven by the text embedding to adaptively reweight\nand fuse visual features across modalities. The framework is further optimized\nby the cross-modal objectives, including image-text contrastive learning and\nimage-text matching, which maps the same scene with different weather\nconditions closer in the respresentation space. Extensive experiments validate\nthat, under diverse weather conditions, our method achieves competitive recall\nrates compared to state-of-the-art drone geo-localization methods. Notably, it\nimproves Recall@1 by +13.37\\% under night conditions and by 18.69\\% under fog\nand snow conditions.", "AI": {"tldr": "WeatherPrompt\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u5d4c\u5165\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u5efa\u7acb\u5929\u6c14\u4e0d\u53d8\u7684\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u89c6\u89c9\u5730\u7406\u5b9a\u4f4d\u5728\u5929\u6c14\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5929\u6c14\u6270\u52a8\uff08\u5982\u96e8\u3001\u96fe\uff09\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u5bf9\u6709\u9650\u5929\u6c14\u7c7b\u522b\u7684\u4f9d\u8d56\u4ee5\u53ca\u573a\u666f-\u5929\u6c14\u7279\u5f81\u89e3\u8026\u4e0d\u8db3\u3002", "method": "1) \u63d0\u51faTraining-free Weather Reasoning\u673a\u5236\uff0c\u5229\u7528\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5408\u6210\u591a\u5929\u6c14\u6587\u672c\u63cf\u8ff0\uff1b2) \u8bbe\u8ba1\u52a8\u6001\u95e8\u63a7\u673a\u5236\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u81ea\u9002\u5e94\u878d\u5408\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\uff0cWeatherPrompt\u7684\u53ec\u56de\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u591c\u95f4\u6761\u4ef6\u4e0bRecall@1\u63d0\u534713.37%\uff0c\u96fe\u96ea\u6761\u4ef6\u4e0b\u63d0\u534718.69%\u3002", "conclusion": "WeatherPrompt\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u52a8\u6001\u95e8\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2508.09397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09397", "abs": "https://arxiv.org/abs/2508.09397", "authors": ["Zhengli Zhang", "Xinyu Luo", "Yuchen Sun", "Wenhua Ding", "Dongyu Huang", "Xinlei Chen"], "title": "Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety", "comment": null, "summary": "Drones operating in complex environments face a significant threat from thin\nobstacles, such as steel wires and kite strings at the submillimeter level,\nwhich are notoriously difficult for conventional sensors like RGB cameras,\nLiDAR, and depth cameras to detect. This paper introduces SkyShield, an\nevent-driven, end-to-end framework designed for the perception of submillimeter\nscale obstacles. Drawing upon the unique features that thin obstacles present\nin the event stream, our method employs a lightweight U-Net architecture and an\ninnovative Dice-Contour Regularization Loss to ensure precise detection.\nExperimental results demonstrate that our event-based approach achieves mean F1\nScore of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment\non edge and mobile platforms.", "AI": {"tldr": "SkyShield\u662f\u4e00\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u65e0\u4eba\u673a\u73af\u5883\u4e2d\u7684\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\uff0c\u5982\u94a2\u4e1d\u548c\u98ce\u7b5d\u7ebf\uff0c\u4f20\u7edf\u4f20\u611f\u5668\u96be\u4ee5\u68c0\u6d4b\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\uff08\u5982\u94a2\u4e1d\u548c\u98ce\u7b5d\u7ebf\uff09\u96be\u4ee5\u88ab\u4f20\u7edf\u4f20\u611f\u5668\uff08\u5982RGB\u76f8\u673a\u3001LiDAR\u548c\u6df1\u5ea6\u76f8\u673a\uff09\u68c0\u6d4b\u5230\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7U-Net\u67b6\u6784\u548c\u521b\u65b0\u7684Dice-Contour\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u4e8b\u4ef6\u6d41\u4e2d\u8584\u969c\u788d\u7269\u7684\u72ec\u7279\u7279\u5f81\u8fdb\u884c\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747F1\u5f97\u5206\u4e3a0.7088\uff0c\u5ef6\u8fdf\u4f4e\u81f321.2\u6beb\u79d2\uff0c\u9002\u5408\u8fb9\u7f18\u548c\u79fb\u52a8\u5e73\u53f0\u90e8\u7f72\u3002", "conclusion": "SkyShield\u6846\u67b6\u5728\u68c0\u6d4b\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.09625", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09625", "abs": "https://arxiv.org/abs/2508.09625", "authors": ["Daoxin Zhong", "Jun Li", "Meng Yee Michael Chuah"], "title": "Plane Detection and Ranking via Model Information Optimization", "comment": "Accepted as contributed paper in the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Plane detection from depth images is a crucial subtask with broad robotic\napplications, often accomplished by iterative methods such as Random Sample\nConsensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic\nguarantees, the ambiguity of its inlier threshold criterion makes it\nsusceptible to false positive plane detections. This issue is particularly\nprevalent in complex real-world scenes, where the true number of planes is\nunknown and multiple planes coexist. In this paper, we aim to address this\nlimitation by proposing a generalised framework for plane detection based on\nmodel information optimization. Building on previous works, we treat the\nobserved depth readings as discrete random variables, with their probability\ndistributions constrained by the ground truth planes. Various models containing\ndifferent candidate plane constraints are then generated through repeated\nrandom sub-sampling to explain our observations. By incorporating the physics\nand noise model of the depth sensor, we can calculate the information for each\nmodel, and the model with the least information is accepted as the most likely\nground truth. This information optimization process serves as an objective\nmechanism for determining the true number of planes and preventing false\npositive detections. Additionally, the quality of each detected plane can be\nranked by summing the information reduction of inlier points for each plane. We\nvalidate these properties through experiments with synthetic data and find that\nour algorithm estimates plane parameters more accurately compared to the\ndefault Open3D RANSAC plane segmentation. Furthermore, we accelerate our\nalgorithm by partitioning the depth map using neural network segmentation,\nwhich enhances its ability to generate more realistic plane parameters in\nreal-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4fe1\u606f\u4f18\u5316\u7684\u5e73\u9762\u68c0\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86RANSAC\u5728\u590d\u6742\u573a\u666f\u4e2d\u6613\u4ea7\u751f\u5047\u9633\u6027\u68c0\u6d4b\u7684\u95ee\u9898\u3002", "motivation": "RANSAC\u5728\u590d\u6742\u573a\u666f\u4e2d\u56e0\u5185\u70b9\u9608\u503c\u6a21\u7cca\u800c\u6613\u4ea7\u751f\u5047\u9633\u6027\u5e73\u9762\u68c0\u6d4b\uff0c\u9700\u4e00\u79cd\u66f4\u5ba2\u89c2\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6df1\u5ea6\u89c2\u6d4b\u89c6\u4e3a\u79bb\u6563\u968f\u673a\u53d8\u91cf\uff0c\u901a\u8fc7\u968f\u673a\u5b50\u91c7\u6837\u751f\u6210\u5019\u9009\u5e73\u9762\u6a21\u578b\uff0c\u5229\u7528\u4fe1\u606f\u4f18\u5316\u9009\u62e9\u6700\u53ef\u80fd\u7684\u5730\u9762\u771f\u5b9e\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u6bd4Open3D RANSAC\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u5e73\u9762\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5206\u5272\u52a0\u901f\u3002", "conclusion": "\u4fe1\u606f\u4f18\u5316\u6846\u67b6\u6709\u6548\u51cf\u5c11\u5047\u9633\u6027\u68c0\u6d4b\uff0c\u63d0\u5347\u5e73\u9762\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09398", "abs": "https://arxiv.org/abs/2508.09398", "authors": ["El Mustapha Mansouri"], "title": "Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring", "comment": "Preprint; 8 pages, 5 figures, 1 table; IEEEtran conference format.\n  Code: https://github.com/E-zClap/bird-classifier", "summary": "This paper presents a low cost, on premise system for autonomous backyard\nbird monitoring in Belgian urban gardens. A motion triggered IP camera uploads\nshort clips via FTP to a local server, where frames are sampled and birds are\nlocalized with Detectron2; cropped regions are then classified by an\nEfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a\nlarger Kaggle corpus. All processing runs on commodity hardware without a\ndiscrete GPU, preserving privacy and avoiding cloud fees. The physical feeder\nuses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.\nDetector-guided cropping improves classification accuracy over raw-frame\nclassification. The classifier attains high validation performance on the\ncurated subset (about 99.5 percent) and delivers practical field accuracy\n(top-1 about 88 percent) on held-out species, demonstrating feasibility for\ncitizen-science-grade biodiversity logging at home.", "AI": {"tldr": "\u4f4e\u6210\u672c\u3001\u672c\u5730\u5316\u7684\u6bd4\u5229\u65f6\u57ce\u5e02\u82b1\u56ed\u9e1f\u7c7b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u8fd0\u52a8\u89e6\u53d1\u7684IP\u6444\u50cf\u5934\u548c\u672c\u5730\u670d\u52a1\u5668\u8fdb\u884c\u9e1f\u7c7b\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u65e0\u9700GPU\uff0c\u4fdd\u62a4\u9690\u79c1\u5e76\u907f\u514d\u4e91\u8d39\u7528\u3002", "motivation": "\u4e3a\u516c\u6c11\u79d1\u5b66\u7ea7\u751f\u7269\u591a\u6837\u6027\u8bb0\u5f55\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e91\u670d\u52a1\u8d39\u7528\u3002", "method": "\u4f7f\u7528Detectron2\u8fdb\u884c\u9e1f\u7c7b\u5b9a\u4f4d\uff0cEfficientNet-B3\u6a21\u578b\u5206\u7c7b\uff0c\u57fa\u4e8e\u672c\u5730\u670d\u52a1\u5668\u5904\u7406\uff0c\u65e0\u9700GPU\u3002", "result": "\u5206\u7c7b\u5668\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523099.5%\u7684\u51c6\u786e\u7387\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u8fbe\u523088%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u9002\u5408\u5bb6\u5ead\u4f7f\u7528\uff0c\u80fd\u591f\u6709\u6548\u8bb0\u5f55\u751f\u7269\u591a\u6837\u6027\u3002"}}
{"id": "2508.09681", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09681", "abs": "https://arxiv.org/abs/2508.09681", "authors": ["Gerardo Loza", "Junlei Hu", "Dominic Jones", "Sharib Ali", "Pietro Valdastri"], "title": "Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision", "comment": "10 pages", "summary": "We proposed a novel test-time optimisation (TTO) approach framed by a\nNeRF-based architecture for long-term 3D point tracking. Most current methods\nin point tracking struggle to obtain consistent motion or are limited to 2D\nmotion. TTO approaches frame the solution for long-term tracking as optimising\na function that aggregates correspondences from other specialised\nstate-of-the-art methods. Unlike the state-of-the-art on TTO, we propose\nparametrising such a function with our new invertible Neural Radiance Field\n(InvNeRF) architecture to perform both 2D and 3D tracking in surgical\nscenarios. Our approach allows us to exploit the advantages of a\nrendering-based approach by supervising the reprojection of pixel\ncorrespondences. It adapts strategies from recent rendering-based methods to\nobtain a bidirectional deformable-canonical mapping, to efficiently handle a\ndefined workspace, and to guide the rays' density. It also presents our\nmulti-scale HexPlanes for fast inference and a new algorithm for efficient\npixel sampling and convergence criteria. We present results in the STIR and\nSCARE datasets, for evaluating point tracking and testing the integration of\nkinematic data in our pipeline, respectively. In 2D point tracking, our\napproach surpasses the precision and accuracy of the TTO state-of-the-art\nmethods by nearly 50% on average precision, while competing with other\napproaches. In 3D point tracking, this is the first TTO approach, surpassing\nfeed-forward methods while incorporating the benefits of a deformable\nNeRF-based reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u4f18\u5316\uff08TTO\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u957f\u671f3D\u70b9\u8ddf\u8e2a\uff0c\u7ed3\u5408\u4e86\u53ef\u9006\u795e\u7ecf\u8f90\u5c04\u573a\uff08InvNeRF\uff09\u67b6\u6784\uff0c\u57282D\u548c3D\u8ddf\u8e2a\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u957f\u671f\u4e00\u81f4\u6027\u62163D\u8fd0\u52a8\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u591a\u65b9\u6cd5\u4f18\u52bf\u5e76\u9002\u5e94\u624b\u672f\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528InvNeRF\u67b6\u6784\uff0c\u7ed3\u5408\u6e32\u67d3\u76d1\u7763\u548c\u53cc\u5411\u53ef\u53d8\u5f62-\u89c4\u8303\u6620\u5c04\uff0c\u63d0\u51fa\u591a\u5c3a\u5ea6HexPlanes\u548c\u9ad8\u6548\u50cf\u7d20\u91c7\u6837\u7b97\u6cd5\u3002", "result": "\u5728STIR\u548cSCARE\u6570\u636e\u96c6\u4e0a\uff0c2D\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u534750%\uff0c3D\u8ddf\u8e2a\u9996\u6b21\u5b9e\u73b0TTO\u65b9\u6cd5\u8d85\u8d8a\u524d\u9988\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57282D\u548c3D\u70b9\u8ddf\u8e2a\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u624b\u672f\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09404", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.09404", "abs": "https://arxiv.org/abs/2508.09404", "authors": ["Guangxun Zhu", "Shiyu Fan", "Hang Dai", "Edmond S. L. Ho"], "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving", "comment": "ACM Multimedia 2025 (Dataset Track) Paper", "summary": "Large-scale high-quality 3D motion datasets with multi-person interactions\nare crucial for data-driven models in autonomous driving to achieve\nfine-grained pedestrian interaction understanding in dynamic urban\nenvironments. However, existing datasets mostly rely on estimating 3D poses\nfrom monocular RGB video frames, which suffer from occlusion and lack of\ntemporal continuity, thus resulting in unrealistic and low-quality human\nmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale\ndataset providing high-quality, temporally coherent 3D skeletal motions with\nexplicit interaction semantics, derived from the Waymo Perception dataset. Our\nkey insight is to utilize 3D human body shape and motion priors to enhance the\nquality of the 3D pose sequences extracted from the raw LiDRA point clouds. The\ndataset covers over 14,000 seconds across more than 800 real driving scenarios,\nincluding rich interactions among an average of 27 agents per scene (with up to\n250 agents in the largest scene). Furthermore, we establish 3D pose forecasting\nbenchmarks under varying pedestrian densities, and the results demonstrate its\nvalue as a foundational resource for future research on fine-grained human\nbehavior understanding in complex urban environments. The dataset and code will\nbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo", "AI": {"tldr": "Waymo-3DSkelMo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf3D\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u884c\u4eba\u4ea4\u4e92\u7814\u7a76\uff0c\u901a\u8fc7LiDAR\u70b9\u4e91\u63d0\u53d63D\u9aa8\u9abc\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u56e0\u5355\u76eeRGB\u89c6\u9891\u5bfc\u81f4\u7684\u906e\u6321\u548c\u65f6\u95f4\u4e0d\u8fde\u7eed\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u8fd0\u52a8\u6570\u636e\u96c6\u4f9d\u8d56\u5355\u76eeRGB\u89c6\u9891\uff0c\u5b58\u5728\u906e\u6321\u548c\u65f6\u95f4\u4e0d\u8fde\u7eed\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u52a8\u6570\u636e\u8d28\u91cf\u4f4e\u3002", "method": "\u5229\u75283D\u4eba\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\u5148\u9a8c\uff0c\u4eceLiDAR\u70b9\u4e91\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u76843D\u9aa8\u9abc\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b14,000\u79d2\u4ee5\u4e0a\u7684\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\uff0c\u8986\u76d6800\u591a\u4e2a\u573a\u666f\uff0c\u5e73\u5747\u6bcf\u573a\u666f27\u4e2a\u884c\u4eba\uff0c\u6700\u5927\u573a\u666f\u8fbe250\u4e2a\u884c\u4eba\u3002", "conclusion": "Waymo-3DSkelMo\u4e3a\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7ec6\u7c92\u5ea6\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u5efa\u7acb\u4e863D\u59ff\u6001\u9884\u6d4b\u57fa\u51c6\u3002"}}
{"id": "2508.09732", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09732", "abs": "https://arxiv.org/abs/2508.09732", "authors": ["Romeo Valentin", "Sydney M. Katz", "Artur B. Carneiro", "Don Walker", "Mykel J. Kochenderfer"], "title": "Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System", "comment": "8 pages, 5 figures, accepted at DASC 2025", "summary": "Recent advances in data-driven computer vision have enabled robust autonomous\nnavigation capabilities for civil aviation, including automated landing and\nrunway detection. However, ensuring that these systems meet the robustness and\nsafety requirements for aviation applications remains a major challenge. In\nthis work, we present a practical vision-based pipeline for aircraft pose\nestimation from runway images that represents a step toward the ability to\ncertify these systems for use in safety-critical aviation applications. Our\napproach features three key innovations: (i) an efficient, flexible neural\narchitecture based on a spatial Soft Argmax operator for probabilistic keypoint\nregression, supporting diverse vision backbones with real-time inference; (ii)\na principled loss function producing calibrated predictive uncertainties, which\nare evaluated via sharpness and calibration metrics; and (iii) an adaptation of\nResidual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling\nruntime detection and rejection of faulty model outputs. We implement and\nevaluate our pose estimation pipeline on a dataset of runway images. We show\nthat our model outperforms baseline architectures in terms of accuracy while\nalso producing well-calibrated uncertainty estimates with sub-pixel precision\nthat can be used downstream for fault detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u98de\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3001\u6821\u51c6\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6545\u969c\u68c0\u6d4b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u822a\u7a7a\u5bfc\u822a\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u786e\u4fdd\u7cfb\u7edf\u6ee1\u8db3\u822a\u7a7a\u5b89\u5168\u8981\u6c42\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u7a7a\u95f4Soft Argmax\u7b97\u5b50\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3001\u6821\u51c6\u7684\u635f\u5931\u51fd\u6570\u548cRAIM\u6539\u8fdb\u7684\u6545\u969c\u68c0\u6d4b\u673a\u5236\u3002", "result": "\u6a21\u578b\u5728\u8dd1\u9053\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u67b6\u6784\uff0c\u63d0\u4f9b\u4e9a\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u7684\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u5173\u952e\u822a\u7a7a\u5e94\u7528\u4e2d\u7684\u7cfb\u7edf\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.09415", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2508.09415", "abs": "https://arxiv.org/abs/2508.09415", "authors": ["John S. O'Meara", "Jared Hwang", "Zeyu Wang", "Michael Saugstad", "Jon E. Froehlich"], "title": "RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata", "comment": "Accepted to the ICCV'25 Workshop on Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities", "summary": "Curb ramps are critical for urban accessibility, but robustly detecting them\nin images remains an open problem due to the lack of large-scale, high-quality\ndatasets. While prior work has attempted to improve data availability with\ncrowdsourced or manually labeled data, these efforts often fall short in either\nquality or scale. In this paper, we introduce and evaluate a two-stage pipeline\ncalled RampNet to scale curb ramp detection datasets and improve model\nperformance. In Stage 1, we generate a dataset of more than 210,000 annotated\nGoogle Street View (GSV) panoramas by auto-translating government-provided curb\nramp location data to pixel coordinates in panoramic images. In Stage 2, we\ntrain a curb ramp detection model (modified ConvNeXt V2) from the generated\ndataset, achieving state-of-the-art performance. To evaluate both stages of our\npipeline, we compare to manually labeled panoramas. Our generated dataset\nachieves 94.0% precision and 92.5% recall, and our detection model reaches\n0.9236 AP -- far exceeding prior work. Our work contributes the first\nlarge-scale, high-quality curb ramp detection dataset, benchmark, and model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRampNet\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u68c0\u6d4b\u8def\u7f18\u5761\u9053\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u81ea\u52a8\u8f6c\u6362\u653f\u5e9c\u6570\u636e\u751f\u6210\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u6539\u8fdb\u7684ConvNeXt V2\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u8def\u7f18\u5761\u9053\u5bf9\u57ce\u5e02\u65e0\u969c\u788d\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u9650\u5236\u4e86\u5176\u68c0\u6d4b\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u8d28\u91cf\u6216\u89c4\u6a21\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u901a\u8fc7\u81ea\u52a8\u8f6c\u6362\u653f\u5e9c\u6570\u636e\u751f\u621021\u4e07\u5f20\u6807\u6ce8\u7684\u8857\u666f\u56fe\u50cf\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528\u6539\u8fdb\u7684ConvNeXt V2\u6a21\u578b\u8bad\u7ec3\u68c0\u6d4b\u5668\u3002", "result": "\u751f\u6210\u6570\u636e\u96c6\u7cbe\u5ea694.0%\uff0c\u53ec\u56de\u738792.5%\uff1b\u68c0\u6d4b\u6a21\u578bAP\u8fbe0.9236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8d21\u732e\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8def\u7f18\u5761\u9053\u68c0\u6d4b\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2508.09811", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09811", "abs": "https://arxiv.org/abs/2508.09811", "authors": ["Jinxi Li", "Ziyang Song", "Bo Yang"], "title": "TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos", "comment": "ICCV 2025. Code and data are available at:\n  https://github.com/vLAR-group/TRACE", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and physical\ninformation just from dynamic multi-view videos in the absence of any human\nlabels. By leveraging physics-informed losses as soft constraints or\nintegrating simple physics models into neural nets, existing works often fail\nto learn complex motion physics, or doing so requires additional labels such as\nobject types or masks. We propose a new framework named TRACE to model the\nmotion physics of complex dynamic 3D scenes. The key novelty of our method is\nthat, by formulating each 3D point as a rigid particle with size and\norientation in space, we directly learn a translation rotation dynamics system\nfor each particle, explicitly estimating a complete set of physical parameters\nto govern the particle's motion over time. Extensive experiments on three\nexisting dynamic datasets and one newly created challenging synthetic datasets\ndemonstrate the extraordinary performance of our method over baselines in the\ntask of future frame extrapolation. A nice property of our framework is that\nmultiple objects or parts can be easily segmented just by clustering the\nlearned physical parameters.", "AI": {"tldr": "TRACE\u6846\u67b6\u901a\u8fc7\u5c063D\u70b9\u5efa\u6a21\u4e3a\u521a\u6027\u7c92\u5b50\uff0c\u76f4\u63a5\u5b66\u4e60\u6bcf\u4e2a\u7c92\u5b50\u7684\u5e73\u79fb\u65cb\u8f6c\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u7b7e\u5373\u53ef\u5efa\u6a21\u590d\u6742\u52a8\u60013D\u573a\u666f\u7684\u8fd0\u52a8\u7269\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u4e4f\u4eba\u5de5\u6807\u7b7e\u65f6\u96be\u4ee5\u5b66\u4e60\u590d\u6742\u8fd0\u52a8\u7269\u7406\uff0c\u6216\u9700\u8981\u989d\u5916\u6807\u7b7e\uff08\u5982\u7269\u4f53\u7c7b\u578b\u6216\u63a9\u7801\uff09\u3002", "method": "\u5c06\u6bcf\u4e2a3D\u70b9\u5efa\u6a21\u4e3a\u5177\u6709\u5927\u5c0f\u548c\u65b9\u5411\u7684\u521a\u6027\u7c92\u5b50\uff0c\u76f4\u63a5\u5b66\u4e60\u5176\u5e73\u79fb\u65cb\u8f6c\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u663e\u5f0f\u4f30\u8ba1\u7269\u7406\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u73b0\u6709\u52a8\u6001\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u65b0\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0cTRACE\u5728\u5e27\u5916\u63a8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TRACE\u4e0d\u4ec5\u80fd\u5efa\u6a21\u590d\u6742\u8fd0\u52a8\u7269\u7406\uff0c\u8fd8\u80fd\u901a\u8fc7\u805a\u7c7b\u7269\u7406\u53c2\u6570\u8f7b\u677e\u5206\u5272\u591a\u4e2a\u7269\u4f53\u6216\u90e8\u5206\u3002"}}
{"id": "2508.09428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09428", "abs": "https://arxiv.org/abs/2508.09428", "authors": ["Yuxiao Wang", "Yu Lei", "Wolin Liang", "Weiying Xue", "Zhenao Wei", "Nan Zhuang", "Qi Liu"], "title": "What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset", "comment": null, "summary": "People control their bodies to establish contact with the environment. To\ncomprehensively understand actions across diverse visual contexts, it is\nessential to simultaneously consider \\textbf{what} action is occurring and\n\\textbf{where} it is happening. Current methodologies, however, often\ninadequately capture this duality, typically failing to jointly model both\naction semantics and their spatial contextualization within scenes. To bridge\nthis gap, we introduce a novel vision task that simultaneously predicts\nhigh-level action semantics and fine-grained body-part contact regions. Our\nproposed framework, PaIR-Net, comprises three key components: the Contact Prior\nAware Module (CPAM) for identifying contact-relevant body parts, the\nPrior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and\nthe Interaction Inference Module (IIM) responsible for integrating global\ninteraction relationships. To facilitate this task, we present PaIR (Part-aware\nInteraction Representation), a comprehensive dataset containing 13,979 images\nthat encompass 654 actions, 80 object categories, and 17 body parts.\nExperimental evaluation demonstrates that PaIR-Net significantly outperforms\nbaseline approaches, while ablation studies confirm the efficacy of each\narchitectural component. The code and dataset will be released upon\npublication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff0c\u540c\u65f6\u9884\u6d4b\u9ad8\u7ea7\u52a8\u4f5c\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u8eab\u4f53\u90e8\u4f4d\u63a5\u89e6\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u4e86PaIR-Net\u6846\u67b6\u548cPaIR\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u5efa\u6a21\u52a8\u4f5c\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PaIR-Net\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aCPAM\u3001PGCS\u548cIIM\uff0c\u5206\u522b\u7528\u4e8e\u8bc6\u522b\u63a5\u89e6\u76f8\u5173\u8eab\u4f53\u90e8\u4f4d\u3001\u50cf\u7d20\u7ea7\u63a5\u89e6\u5206\u5272\u548c\u6574\u5408\u5168\u5c40\u4ea4\u4e92\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePaIR-Net\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "PaIR-Net\u548cPaIR\u6570\u636e\u96c6\u4e3a\u540c\u65f6\u7406\u89e3\u52a8\u4f5c\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09446", "categories": ["cs.CV", "I.2.8"], "pdf": "https://arxiv.org/pdf/2508.09446", "abs": "https://arxiv.org/abs/2508.09446", "authors": ["Jiateng Liu", "Hengcan Shi", "Feng Chen", "Zhiwen Shao", "Yaonan Wang", "Jianfei Cai", "Wenming Zheng"], "title": "MPT: Motion Prompt Tuning for Micro-Expression Recognition", "comment": null, "summary": "Micro-expression recognition (MER) is crucial in the affective computing\nfield due to its wide application in medical diagnosis, lie detection, and\ncriminal investigation. Despite its significance, obtaining micro-expression\n(ME) annotations is challenging due to the expertise required from\npsychological professionals. Consequently, ME datasets often suffer from a\nscarcity of training samples, severely constraining the learning of MER models.\nWhile current large pre-training models (LMs) offer general and discriminative\nrepresentations, their direct application to MER is hindered by an inability to\ncapture transitory and subtle facial movements-essential elements for effective\nMER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to\nadapting LMs for MER, representing a pioneering method for subtle motion prompt\ntuning. Particularly, we introduce motion prompt generation, including motion\nmagnification and Gaussian tokenization, to extract subtle motions as prompts\nfor LMs. Additionally, a group adapter is carefully designed and inserted into\nthe LM to enhance it in the target MER domain, facilitating a more nuanced\ndistinction of ME representation. Furthermore, extensive experiments conducted\non three widely used MER datasets demonstrate that our proposed MPT\nconsistently surpasses state-of-the-art approaches and verifies its\neffectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMotion Prompt Tuning (MPT)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08LMs\uff09\u9002\u914d\u5230\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u8fd0\u52a8\u63d0\u793a\u751f\u6210\u548c\u7ec4\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86MER\u6027\u80fd\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\u5728\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u6807\u6ce8\u56f0\u96be\uff0c\u6570\u636e\u96c6\u6837\u672c\u7a00\u7f3a\uff0c\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5fae\u8868\u60c5\u7684\u77ed\u6682\u548c\u7ec6\u5fae\u7279\u5f81\u3002", "method": "\u63d0\u51faMPT\u65b9\u6cd5\uff0c\u5305\u62ec\u8fd0\u52a8\u63d0\u793a\u751f\u6210\uff08\u8fd0\u52a8\u653e\u5927\u548c\u9ad8\u65af\u6807\u8bb0\u5316\uff09\u548c\u7ec4\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u4ee5\u589e\u5f3aLMs\u5728MER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684MER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPT\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MPT\u65b9\u6cd5\u901a\u8fc7\u8fd0\u52a8\u63d0\u793a\u548c\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86MER\u4efb\u52a1\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5fae\u8868\u60c5\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09449", "abs": "https://arxiv.org/abs/2508.09449", "authors": ["Jiaqi Yan", "Shuning Xu", "Xiangyu Chen", "Dell Zhang", "Jie Tang", "Gangshan Wu", "Jie Liu"], "title": "RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration", "comment": null, "summary": "Reference-based Super Resolution (RefSR) improves upon Single Image Super\nResolution (SISR) by leveraging high-quality reference images to enhance\ntexture fidelity and visual realism. However, a critical limitation of existing\nRefSR approaches is their reliance on manually curated target-reference image\npairs, which severely constrains their practicality in real-world scenarios. To\novercome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new\nand practical RefSR paradigm that automatically retrieves semantically relevant\nhigh-resolution images from a reference database given only a low-quality\ninput. This enables scalable and flexible RefSR in realistic use cases, such as\nenhancing mobile photos taken in environments like zoos or museums, where\ncategory-specific reference data (e.g., animals, artworks) can be readily\ncollected or pre-curated. To facilitate research in this direction, we\nconstruct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike\nprior datasets with fixed target-reference pairs, RASR-Flickr30 provides\nper-category reference databases to support open-world retrieval. We further\npropose RASRNet, a strong baseline that combines a semantic reference retriever\nwith a diffusion-based RefSR generator. It retrieves relevant references based\non semantic similarity and employs a diffusion-based generator enhanced with\nsemantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet\nconsistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131\nLPIPS, while generating more realistic textures. These findings highlight\nretrieval augmentation as a promising direction to bridge the gap between\nacademic RefSR research and real-world applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08RASR\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u6765\u63d0\u5347\u4f4e\u8d28\u91cf\u8f93\u5165\u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRefSR\u4f9d\u8d56\u624b\u52a8\u914d\u5bf9\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709RefSR\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u914d\u5bf9\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002RASR\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u53c2\u8003\u56fe\u50cf\uff0c\u63d0\u5347\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faRASR\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u53c2\u8003\u68c0\u7d22\u5668\u548c\u57fa\u4e8e\u6269\u6563\u7684RefSR\u751f\u6210\u5668\u3002\u6784\u5efaRASR-Flickr30\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u5f00\u653e\u68c0\u7d22\u3002", "result": "RASRNet\u5728PSNR\u548cLPIPS\u6307\u6807\u4e0a\u4f18\u4e8eSISR\u57fa\u7ebf\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u662f\u8fde\u63a5\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u6709\u6548\u65b9\u5411\u3002"}}
{"id": "2508.09453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09453", "abs": "https://arxiv.org/abs/2508.09453", "authors": ["Abdul Matin", "Tanjim Bin Faruk", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss", "comment": null, "summary": "The proliferation of foundation models, pretrained on large-scale unlabeled\ndatasets, has emerged as an effective approach in creating adaptable and\nreusable architectures that can be leveraged for various downstream tasks using\nsatellite observations. However, their direct application to hyperspectral\nremote sensing remains challenging due to inherent spectral disparities and the\nscarcity of available observations. In this work, we present HyperKD, a novel\nknowledge distillation framework that enables transferring learned\nrepresentations from a teacher model into a student model for effective\ndevelopment of a foundation model on hyperspectral images. Unlike typical\nknowledge distillation frameworks, which use a complex teacher to guide a\nsimpler student, HyperKD enables an inverse form of knowledge transfer across\ndifferent types of spectral data, guided by a simpler teacher model. Building\nupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi\nfoundational model into a student tailored for EnMAP hyperspectral imagery.\nHyperKD addresses the inverse domain adaptation problem with spectral gaps by\nintroducing a feature-based strategy that includes spectral range-based channel\nalignment, spatial feature-guided masking, and an enhanced loss function\ntailored for hyperspectral images. HyperKD bridges the substantial spectral\ndomain gap, enabling the effective use of pretrained foundation models for\ngeospatial applications. Extensive experiments show that HyperKD significantly\nimproves representation learning in MAEs, leading to enhanced reconstruction\nfidelity and more robust performance on downstream tasks such as land cover\nclassification, crop type identification, and soil organic carbon prediction,\nunderpinning the potential of knowledge distillation frameworks in remote\nsensing analytics with hyperspectral imagery.", "AI": {"tldr": "HyperKD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u5411\u77e5\u8bc6\u8f6c\u79fb\u89e3\u51b3\u9ad8\u5149\u8c31\u9065\u611f\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\u6311\u6218\u3002", "motivation": "\u9ad8\u5149\u8c31\u9065\u611f\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u5149\u8c31\u5dee\u5f02\u548c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "HyperKD\u901a\u8fc7\u7279\u5f81\u7b56\u7565\uff08\u5982\u5149\u8c31\u8303\u56f4\u5bf9\u9f50\u548c\u7a7a\u95f4\u7279\u5f81\u63a9\u7801\uff09\u4ece\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u8f6c\u79fb\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHyperKD\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "HyperKD\u5c55\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u5728\u9ad8\u5149\u8c31\u9065\u611f\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09454", "abs": "https://arxiv.org/abs/2508.09454", "authors": ["Shuai Tan", "Biao Gong", "Zhuoxin Liu", "Yan Wang", "Xi Chen", "Yifan Feng", "Hengshuang Zhao"], "title": "Animate-X++: Universal Character Image Animation with Dynamic Backgrounds", "comment": "Project page: https://lucaria-academy.github.io/Animate-X++/", "summary": "Character image animation, which generates high-quality videos from a\nreference image and target pose sequence, has seen significant progress in\nrecent years. However, most existing methods only apply to human figures, which\nusually do not generalize well on anthropomorphic characters commonly used in\nindustries like gaming and entertainment. Furthermore, previous methods could\nonly generate videos with static backgrounds, which limits the realism of the\nvideos. For the first challenge, our in-depth analysis suggests to attribute\nthis limitation to their insufficient modeling of motion, which is unable to\ncomprehend the movement pattern of the driving video, thus imposing a pose\nsequence rigidly onto the target character. To this end, this paper proposes\nAnimate-X++, a universal animation framework based on DiT for various character\ntypes, including anthropomorphic characters. To enhance motion representation,\nwe introduce the Pose Indicator, which captures comprehensive motion pattern\nfrom the driving video through both implicit and explicit manner. The former\nleverages CLIP visual features of a driving video to extract its gist of\nmotion, like the overall movement pattern and temporal relations among motions,\nwhile the latter strengthens the generalization of DiT by simulating possible\ninputs in advance that may arise during inference. For the second challenge, we\nintroduce a multi-task training strategy that jointly trains the animation and\nTI2V tasks. Combined with the proposed partial parameter training, this\napproach achieves not only character animation but also text-driven background\ndynamics, making the videos more realistic. Moreover, we introduce a new\nAnimated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of\nAnimate-X++ on universal and widely applicable animation images. Extensive\nexperiments demonstrate the superiority and effectiveness of Animate-X++.", "AI": {"tldr": "Animate-X++ \u662f\u4e00\u4e2a\u57fa\u4e8e DiT \u7684\u901a\u7528\u52a8\u753b\u6846\u67b6\uff0c\u9488\u5bf9\u5305\u62ec\u62df\u4eba\u5316\u89d2\u8272\u5728\u5185\u7684\u591a\u79cd\u89d2\u8272\u7c7b\u578b\uff0c\u901a\u8fc7 Pose Indicator \u589e\u5f3a\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u80cc\u666f\u52a8\u6001\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4eba\u7c7b\u89d2\u8272\uff0c\u5bf9\u62df\u4eba\u5316\u89d2\u8272\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4ec5\u80fd\u751f\u6210\u9759\u6001\u80cc\u666f\u89c6\u9891\uff0c\u9650\u5236\u4e86\u89c6\u9891\u7684\u771f\u5b9e\u611f\u3002", "method": "\u63d0\u51fa Pose Indicator \u901a\u8fc7\u9690\u5f0f\u548c\u663e\u5f0f\u65b9\u5f0f\u6355\u6349\u9a71\u52a8\u89c6\u9891\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\u8054\u5408\u8bad\u7ec3\u52a8\u753b\u548c TI2V \u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e Animate-X++ \u5728\u901a\u7528\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "Animate-X++ \u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u52a8\u753b\u7684\u771f\u5b9e\u611f\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09456", "categories": ["cs.CV", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09456", "abs": "https://arxiv.org/abs/2508.09456", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f93\u5165\u611f\u77e5\u540e\u95e8\u653b\u51fb\u65b9\u6cd5IAG\uff0c\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u751f\u6210\u5668\u5d4c\u5165\u653b\u51fb\u76ee\u6807\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u9690\u853d\u653b\u51fb\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u540e\u95e8\u653b\u51fb\uff0c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faIAG\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6U-Net\u751f\u6210\u81ea\u9002\u5e94\u89e6\u53d1\u5668\uff0c\u5d4c\u5165\u653b\u51fb\u76ee\u6807\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u635f\u5931\u786e\u4fdd\u653b\u51fb\u7684\u9690\u853d\u6027\u3002", "result": "IAG\u5728InternVL-2.5-8B\u4e0aASR@0.5\u8d85\u8fc765%\uff0c\u4e14\u5bf9Ferret-7B\u548cLlaVA-1.5-7B\u7684\u653b\u51fb\u6548\u679c\u663e\u8457\u3002", "conclusion": "IAG\u5c55\u793a\u4e86\u540e\u95e8\u653b\u51fb\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2508.09459", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09459", "abs": "https://arxiv.org/abs/2508.09459", "authors": ["Wen Huang", "Jiarui Yang", "Tao Dai", "Jiawei Li", "Shaoxiong Zhan", "Bin Wang", "Shu-Tao Xia"], "title": "RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization", "comment": null, "summary": "Visual manipulation localization (VML) -- across both images and videos -- is\na crucial task in digital forensics that involves identifying tampered regions\nin visual content. However, existing methods often lack cross-modal\ngeneralization and struggle to handle high-resolution or long-duration inputs\nefficiently.\n  We propose RelayFormer, a unified and modular architecture for visual\nmanipulation localization across images and videos. By leveraging flexible\nlocal units and a Global-Local Relay Attention (GLoRA) mechanism, it enables\nscalable, resolution-agnostic processing with strong generalization. Our\nframework integrates seamlessly with existing Transformer-based backbones, such\nas ViT and SegFormer, via lightweight adaptation modules that require only\nminimal architectural changes, ensuring compatibility without disrupting\npretrained representations.\n  Furthermore, we design a lightweight, query-based mask decoder that supports\none-shot inference across video sequences with linear complexity. Extensive\nexperiments across multiple benchmarks demonstrate that our approach achieves\nstate-of-the-art localization performance, setting a new baseline for scalable\nand modality-agnostic VML. Code is available at:\nhttps://github.com/WenOOI/RelayFormer.", "AI": {"tldr": "RelayFormer\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u7be1\u6539\u5b9a\u4f4d\uff08VML\uff09\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u5c40\u90e8\u5355\u5143\u548c\u5168\u5c40-\u5c40\u90e8\u4e2d\u7ee7\u6ce8\u610f\u529b\u673a\u5236\uff08GLoRA\uff09\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u6cdb\u5316\u548c\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u6216\u957f\u65f6\u95f4\u8f93\u5165\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7075\u6d3b\u7684\u5c40\u90e8\u5355\u5143\u548cGLoRA\u673a\u5236\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\u4e0e\u73b0\u6709Transformer\u9aa8\u5e72\u7f51\u7edc\uff08\u5982ViT\u548cSegFormer\uff09\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u6a21\u6001\u65e0\u5173\u7684VML\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "RelayFormer\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u9ad8\u6548\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9\u7be1\u6539\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09461", "abs": "https://arxiv.org/abs/2508.09461", "authors": ["Hao Yu", "Rupayan Mallick", "Margrit Betke", "Sarah Adel Bargal"], "title": "Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy", "comment": null, "summary": "Different forms of customized 2D avatars are widely used in gaming\napplications, virtual communication, education, and content creation. However,\nexisting approaches often fail to capture fine-grained facial expressions and\nstruggle to preserve identity across different expressions. We propose\nGEN-AFFECT, a novel framework for personalized avatar generation that generates\nexpressive and identity-consistent avatars with a diverse set of facial\nexpressions. Our framework proposes conditioning a multimodal diffusion\ntransformer on an extracted identity-expression representation. This enables\nidentity preservation and representation of a wide range of facial expressions.\nGEN-AFFECT additionally employs consistent attention at inference for\ninformation sharing across the set of generated expressions, enabling the\ngeneration process to maintain identity consistency over the array of generated\nfine-grained expressions. GEN-AFFECT demonstrates superior performance compared\nto previous state-of-the-art methods on the basis of the accuracy of the\ngenerated expressions, the preservation of the identity and the consistency of\nthe target identity across an array of fine-grained facial expressions.", "AI": {"tldr": "GEN-AFFECT\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e2a\u6027\u5316\u4e14\u8868\u60c5\u4e30\u5bcc\u76842D\u865a\u62df\u5f62\u8c61\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8868\u60c5\u6355\u6349\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u62102D\u865a\u62df\u5f62\u8c61\u65f6\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u8868\u60c5\u5e76\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0cGEN-AFFECT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u7ed3\u5408\u8eab\u4efd-\u8868\u60c5\u8868\u793a\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u4e00\u81f4\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8868\u60c5\u591a\u6837\u4e14\u8eab\u4efd\u4e00\u81f4\u3002", "result": "GEN-AFFECT\u5728\u751f\u6210\u8868\u60c5\u7684\u51c6\u786e\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u53ca\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GEN-AFFECT\u4e3a\u4e2a\u6027\u5316\u865a\u62df\u5f62\u8c61\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09466", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.09466", "abs": "https://arxiv.org/abs/2508.09466", "authors": ["Tam Ngoc-Bang Nguyen", "Anh-Dzung Doan", "Zhipeng Cai", "Tat-Jun Chin"], "title": "Event-driven Robust Fitting on Neuromorphic Hardware", "comment": "11 pages, accepted in ICCV 2025 Workshop on Neuromorphic Vision\n  (NeVI)", "summary": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7684\u80fd\u91cf\u9ad8\u6548\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\uff0c\u4f7f\u7528Intel Loihi 2\u786c\u4ef6\u5b9e\u73b0\uff0c\u80fd\u8017\u4ec5\u4e3a\u4f20\u7edfCPU\u65b9\u6cd5\u768415%\u3002", "motivation": "\u968f\u7740AI\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u73b0\u6709\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\u5728\u80fd\u91cf\u6548\u7387\u65b9\u9762\u7f3a\u4e4f\u5173\u6ce8\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u7684\u6a21\u578b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u914dIntel Loihi 2\u786c\u4ef6\u67b6\u6784\uff0c\u5e76\u4f18\u5316\u7b97\u6cd5\u4ee5\u5e94\u5bf9\u786c\u4ef6\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u7cbe\u5ea6\u4e0b\u80fd\u8017\u4ec5\u4e3a\u4f20\u7edfCPU\u65b9\u6cd5\u768415%\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e3a\u9c81\u68d2\u62df\u5408\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u8282\u80fd\u6f5c\u529b\u3002"}}
{"id": "2508.09470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09470", "abs": "https://arxiv.org/abs/2508.09470", "authors": ["Jialei Xu", "Zizhuang Wei", "Weikang You", "Linyun Li", "Weijian Sun"], "title": "CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios", "comment": null, "summary": "Semantic segmentation of city-scale point clouds is a critical technology for\nUnmanned Aerial Vehicle (UAV) perception systems, enabling the classification\nof 3D points without relying on any visual information to achieve comprehensive\n3D understanding. However, existing models are frequently constrained by the\nlimited scale of 3D data and the domain gap between datasets, which lead to\nreduced generalization capability. To address these challenges, we propose\nCitySeg, a foundation model for city-scale point cloud semantic segmentation\nthat incorporates text modality to achieve open vocabulary segmentation and\nzero-shot inference. Specifically, in order to mitigate the issue of\nnon-uniform data distribution across multiple domains, we customize the data\npreprocessing rules, and propose a local-global cross-attention network to\nenhance the perception capabilities of point networks in UAV scenarios. To\nresolve semantic label discrepancies across datasets, we introduce a\nhierarchical classification strategy. A hierarchical graph established\naccording to the data annotation rules consolidates the data labels, and the\ngraph encoder is used to model the hierarchical relationships between\ncategories. In addition, we propose a two-stage training strategy and employ\nhinge loss to increase the feature separability of subcategories. Experimental\nresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)\nperformance on nine closed-set benchmarks, significantly outperforming existing\napproaches. Moreover, for the first time, CitySeg enables zero-shot\ngeneralization in city-scale point cloud scenarios without relying on visual\ninformation.", "AI": {"tldr": "CitySeg\u662f\u4e00\u79cd\u7528\u4e8e\u57ce\u5e02\u5c3a\u5ea6\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u6a21\u6001\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u548c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u89c4\u6a21\u5c0f\u548c\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u56e03D\u6570\u636e\u89c4\u6a21\u6709\u9650\u548c\u6570\u636e\u96c6\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faCitySeg\u6a21\u578b\uff0c\u5305\u62ec\u5b9a\u5236\u6570\u636e\u9884\u5904\u7406\u89c4\u5219\u3001\u5c40\u90e8-\u5168\u5c40\u4ea4\u53c9\u6ce8\u610f\u529b\u7f51\u7edc\u3001\u5206\u5c42\u5206\u7c7b\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4e5d\u4e2a\u95ed\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e0d\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "CitySeg\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u5c3a\u5ea6\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09475", "abs": "https://arxiv.org/abs/2508.09475", "authors": ["Shibo Yao", "Renshuai Tao", "Xiaolong Zheng", "Chao Liang", "Chunjie Zhang"], "title": "Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection", "comment": null, "summary": "Recent deepfake detection studies often treat unseen sample detection as a\n``zero-shot\" task, training on images generated by known models but\ngeneralizing to unknown ones. A key real-world challenge arises when a model\nperforms poorly on unknown samples, yet these samples remain available for\nanalysis. This highlights that it should be approached as a ``few-shot\" task,\nwhere effectively utilizing a small number of samples can lead to significant\nimprovement. Unlike typical few-shot tasks focused on semantic understanding,\ndeepfake detection prioritizes image realism, which closely mirrors real-world\ndistributions. In this work, we propose the Few-shot Training-free Network\n(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet\ndiffers from traditional methods that rely on large-scale known data for\ntraining. Instead, FTNet uses only one fake samplefrom an evaluation set,\nmimicking the scenario where new samples emerge in the real world and can be\ngathered for use, without any training or parameter updates. During evaluation,\neach test sample is compared to the known fake and real samples, and it is\nclassified based on the category of the nearest sample. We conduct a\ncomprehensive analysis of AI-generated images from 29 different generative\nmodels and achieve a new SoTA performance, with an average improvement of 8.7\\%\ncompared to existing methods. This work introduces a fresh perspective on\nreal-world deepfake detection: when the model struggles to generalize on a\nfew-shot sample, leveraging the failed samples leads to better performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFew-shot Training-free Network (FTNet)\uff0c\u7528\u4e8e\u5c0f\u6837\u672c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u672a\u77e5\u6837\u672c\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u73b0\u5b9e\u4e2d\u8fd9\u4e9b\u6837\u672c\u4ecd\u53ef\u7528\u4e8e\u5206\u6790\uff0c\u56e0\u6b64\u9700\u5c06\u5176\u89c6\u4e3a\u5c0f\u6837\u672c\u4efb\u52a1\u3002", "method": "FTNet\u4ec5\u9700\u4e00\u4e2a\u4f2a\u9020\u6837\u672c\u4f5c\u4e3a\u53c2\u8003\uff0c\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u6837\u672c\u4e0e\u5df2\u77e5\u6837\u672c\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u5206\u7c7b\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u572829\u79cd\u751f\u6210\u6a21\u578b\u7684\u6d4b\u8bd5\u4e2d\uff0cFTNet\u5e73\u5747\u6027\u80fd\u63d0\u53478.7%\uff0c\u8fbe\u5230\u65b0SoTA\u3002", "conclusion": "\u5229\u7528\u5931\u8d25\u6837\u672c\u53ef\u63d0\u5347\u5c0f\u6837\u672c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09476", "abs": "https://arxiv.org/abs/2508.09476", "authors": ["Yuji Wang", "Moran Li", "Xiaobin Hu", "Ran Yi", "Jiangning Zhang", "Chengming Xu", "Weijian Cao", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts", "comment": null, "summary": "Current video generation models struggle with identity preservation under\nlarge facial angles, primarily facing two challenges: the difficulty in\nexploring an effective mechanism to integrate identity features into DiT\nstructure, and the lack of targeted coverage of large facial angles in existing\nopen-source video datasets. To address these, we present two key innovations.\nFirst, we introduce a Mixture of Facial Experts (MoFE) that dynamically\ncombines complementary cues from three specialized experts, each designed to\ncapture distinct but mutually reinforcing aspects of facial attributes. The\nidentity expert captures cross-pose identity-sensitive features, the semantic\nexpert extracts high-level visual semantxics, and the detail expert preserves\npixel-level features (e.g., skin texture, color gradients). Furthermore, to\nmitigate dataset limitations, we have tailored a data processing pipeline\ncentered on two key aspects: Face Constraints and Identity Consistency. Face\nConstraints ensure facial angle diversity and a high proportion of facial\nregions, while Identity Consistency preserves coherent person-specific features\nacross temporal sequences, collectively addressing the scarcity of large facial\nangles and identity-stable training data in existing datasets. Leveraging this\npipeline, we have curated and refined a Large Face Angles (LFA) Dataset from\nexisting open-source human video datasets, comprising 460K video clips with\nannotated facial angles. Experimental results on the LFA benchmark demonstrate\nthat our method, empowered by the LFA dataset, significantly outperforms prior\nSOTA methods in face similarity, face FID, and CLIP semantic alignment. The\ncode and dataset will be made publicly available at\nhttps://github.com/rain152/LFA-Video-Generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u9762\u90e8\u4e13\u5bb6\uff08MoFE\uff09\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e13\u95e8\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5927\u89d2\u5ea6\u9762\u90e8\u8eab\u4efd\u4fdd\u6301\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5927\u89d2\u5ea6\u9762\u90e8\u65f6\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u6709\u6548\u7684\u8eab\u4efd\u7279\u5f81\u6574\u5408\u673a\u5236\u548c\u9488\u5bf9\u6027\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165MoFE\u52a8\u6001\u7ed3\u5408\u4e09\u79cd\u4e13\u5bb6\uff08\u8eab\u4efd\u3001\u8bed\u4e49\u3001\u7ec6\u8282\uff09\uff0c\u5e76\u8bbe\u8ba1\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff08\u9762\u90e8\u7ea6\u675f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff09\u6784\u5efaLFA\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LFA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u9762\u90e8\u76f8\u4f3c\u5ea6\u3001FID\u548cCLIP\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "MoFE\u548cLFA\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89d2\u5ea6\u9762\u90e8\u8eab\u4efd\u4fdd\u6301\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.09477", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09477", "abs": "https://arxiv.org/abs/2508.09477", "authors": ["Zhipeng Yuan", "Kai Wang", "Weize Quan", "Dong-Ming Yan", "Tieru Wu"], "title": "CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection", "comment": null, "summary": "With the rapid advancement of AI generative models, the visual quality of\nAI-generated images (AIIs) has become increasingly close to natural images,\nwhich inevitably raises security concerns. Most AII detectors often employ the\nconventional image classification pipeline with natural images and AIIs\n(generated by a generative model), which can result in limited detection\nperformance for AIIs from unseen generative models. To solve this, we proposed\na universal AI-generated image detector from the perspective of anomaly\ndetection. Our discriminator does not need to access any AIIs and learn a\ngeneralizable representation with unsupervised learning. Specifically, we use\nthe pre-trained CLIP encoder as the feature extractor and design a normalizing\nflow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by\napplying a spectral modification operation on natural images, are used for\ntraining. Our models are trained by minimizing the likelihood of proxy images,\noptionally combined with maximizing the likelihood of natural images. Extensive\nexperiments demonstrate the effectiveness of our method on AIIs produced by\nvarious image generators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u901a\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\uff0c\u65e0\u9700\u63a5\u89e6AI\u751f\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u63a5\u8fd1\u81ea\u7136\u56fe\u50cf\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u6a21\u578b\u6027\u80fd\u6709\u9650\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3CLIP\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u8bbe\u8ba1\u7c7b\u4f3c\u5f52\u4e00\u5316\u6d41\u7684\u65e0\u76d1\u7763\u6a21\u578b\uff0c\u5229\u7528\u4ee3\u7406\u56fe\u50cf\uff08\u5982\u9891\u8c31\u4fee\u6539\u7684\u81ea\u7136\u56fe\u50cf\uff09\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u56fe\u50cf\u751f\u6210\u5668\u751f\u6210\u7684AI\u56fe\u50cf\u6709\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u901a\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.09478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09478", "abs": "https://arxiv.org/abs/2508.09478", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Shubham Jain", "Prateek Prasanna"], "title": "GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs", "comment": null, "summary": "In this work, we present GazeLT, a human visual attention\nintegration-disintegration approach for long-tailed disease classification. A\nradiologist's eye gaze has distinct patterns that capture both fine-grained and\ncoarser level disease related information. While interpreting an image, a\nradiologist's attention varies throughout the duration; it is critical to\nincorporate this into a deep learning framework to improve automated image\ninterpretation. Another important aspect of visual attention is that apart from\nlooking at major/obvious disease patterns, experts also look at\nminor/incidental findings (few of these constituting long-tailed classes)\nduring the course of image interpretation. GazeLT harnesses the temporal aspect\nof the visual search process, via an integration and disintegration mechanism,\nto improve long-tailed disease classification. We show the efficacy of GazeLT\non two publicly available datasets for long-tailed disease classification,\nnamely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.\nGazeLT outperforms the best long-tailed loss by 4.1% and the visual\nattention-based baseline by 21.7% in average accuracy metrics for these\ndatasets. Our code is available at https://github.com/lordmoinak1/gazelt.", "AI": {"tldr": "GazeLT\u662f\u4e00\u79cd\u7ed3\u5408\u653e\u5c04\u79d1\u533b\u751f\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e0e\u5206\u89e3\u673a\u5236\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u653e\u5c04\u79d1\u533b\u751f\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\u5305\u542b\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u75be\u75c5\u4fe1\u606f\uff0c\u5c06\u5176\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u53ef\u63d0\u5347\u81ea\u52a8\u5316\u56fe\u50cf\u89e3\u8bfb\u80fd\u529b\u3002", "method": "\u5229\u7528\u89c6\u89c9\u641c\u7d22\u8fc7\u7a0b\u7684\u65f6\u95f4\u7279\u6027\uff0c\u901a\u8fc7\u6574\u5408\u4e0e\u5206\u89e3\u673a\u5236\u6539\u8fdb\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u3002", "result": "\u5728NIH-CXR-LT\u548cMIMIC-CXR-LT\u6570\u636e\u96c6\u4e0a\uff0cGazeLT\u6bd4\u6700\u4f73\u957f\u5c3e\u635f\u5931\u65b9\u6cd5\u63d0\u53474.1%\uff0c\u6bd4\u89c6\u89c9\u6ce8\u610f\u529b\u57fa\u7ebf\u63d0\u534721.7%\u3002", "conclusion": "GazeLT\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.09479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09479", "abs": "https://arxiv.org/abs/2508.09479", "authors": ["Xuejun Huang", "Xinyi Liu", "Yi Wan", "Zhi Zheng", "Bin Zhang", "Mingtao Xiong", "Yingying Pei", "Yongjun Zhang"], "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images", "comment": null, "summary": "Three-dimensional scene reconstruction from sparse-view satellite images is a\nlong-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its\nvariants have recently attracted attention for its high efficiency, existing\nmethods remain unsuitable for satellite images due to incompatibility with\nrational polynomial coefficient (RPC) models and limited generalization\ncapability. Recent advances in generalizable 3DGS approaches show potential,\nbut they perform poorly on multi-temporal sparse satellite images due to\nlimited geometric constraints, transient objects, and radiometric\ninconsistencies. To address these limitations, we propose SkySplat, a novel\nself-supervised framework that integrates the RPC model into the generalizable\n3DGS pipeline, enabling more effective use of sparse geometric cues for\nimproved reconstruction. SkySplat relies only on RGB images and\nradiometric-robust relative height supervision, thereby eliminating the need\nfor ground-truth height maps. Key components include a Cross-Self Consistency\nModule (CSCM), which mitigates transient object interference via\nconsistency-based masking, and a multi-view consistency aggregation strategy\nthat refines reconstruction results. Compared to per-scene optimization\nmethods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.\nIt also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to\n1.80 m on the DFC19 dataset significantly, and demonstrates strong\ncross-dataset generalization on the MVS3D benchmark.", "AI": {"tldr": "SkySplat\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5c06RPC\u6a21\u578b\u96c6\u6210\u5230\u901a\u75283DGS\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u4e09\u7ef4\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u4e09\u7ef4\u91cd\u5efa\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u4e0eRPC\u6a21\u578b\u4e0d\u517c\u5bb9\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "SkySplat\u7ed3\u5408RPC\u6a21\u578b\u548c\u901a\u75283DGS\uff0c\u5229\u7528RGB\u56fe\u50cf\u548c\u8f90\u5c04\u9c81\u68d2\u7684\u76f8\u5bf9\u9ad8\u5ea6\u76d1\u7763\uff0c\u901a\u8fc7CSCM\u6a21\u5757\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7b56\u7565\u4f18\u5316\u91cd\u5efa\u3002", "result": "SkySplat\u6bd4EOGS\u5feb86\u500d\u4e14\u66f4\u51c6\u786e\uff0c\u5728DFC19\u6570\u636e\u96c6\u4e0aMAE\u4ece13.18\u7c73\u964d\u81f31.80\u7c73\uff0c\u5e76\u5728MVS3D\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SkySplat\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u4e09\u7ef4\u91cd\u5efa\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2508.09486", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.09486", "abs": "https://arxiv.org/abs/2508.09486", "authors": ["Yun Wang", "Long Zhang", "Jingren Liu", "Jiaqi Yan", "Zhanjie Zhang", "Jiahao Zheng", "Xun Yang", "Dapeng Wu", "Xiangyu Chen", "Xuelong Li"], "title": "Episodic Memory Representation for Long-form Video Understanding", "comment": "10 pages, 5 figures", "summary": "Video Large Language Models (Video-LLMs) excel at general video understanding\nbut struggle with long-form videos due to context window limits. Consequently,\nrecent approaches focus on keyframe retrieval, condensing lengthy videos into a\nsmall set of informative frames. Despite their practicality, these methods\nsimplify the problem to static text image matching, overlooking spatio temporal\nrelationships crucial for capturing scene transitions and contextual\ncontinuity, and may yield redundant keyframes with limited information,\ndiluting salient cues essential for accurate video question answering. To\naddress these limitations, we introduce Video-EM, a training free framework\ninspired by the principles of human episodic memory, designed to facilitate\nrobust and contextually grounded reasoning. Rather than treating keyframes as\nisolated visual entities, Video-EM explicitly models them as temporally ordered\nepisodic events, capturing both spatial relationships and temporal dynamics\nnecessary for accurately reconstructing the underlying narrative. Furthermore,\nthe framework leverages chain of thought (CoT) thinking with LLMs to\niteratively identify a minimal yet highly informative subset of episodic\nmemories, enabling efficient and accurate question answering by Video-LLMs.\nExtensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench\nbenchmarks confirm the superiority of Video-EM, which achieves highly\ncompetitive results with performance gains of 4-9 percent over respective\nbaselines while utilizing fewer frames.", "AI": {"tldr": "Video-EM\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86Video-LLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u5e27\u5197\u4f59\u548c\u65f6\u7a7a\u5173\u7cfb\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u957f\u89c6\u9891\u7b80\u5316\u4e3a\u9759\u6001\u5173\u952e\u5e27\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u5173\u952e\u5e27\u5197\u4f59\u548c\u4fe1\u606f\u7a00\u91ca\u3002", "method": "Video-EM\u5c06\u5173\u952e\u5e27\u5efa\u6a21\u4e3a\u65f6\u5e8f\u60c5\u666f\u4e8b\u4ef6\uff0c\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u5e8f\u52a8\u6001\uff0c\u5e76\u5229\u7528LLMs\u7684\u94fe\u5f0f\u601d\u7ef4\u8fed\u4ee3\u7b5b\u9009\u6700\u5c0f\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u8bb0\u5fc6\u5b50\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-EM\u6027\u80fd\u63d0\u53474-9%\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u5e27\u6570\u3002", "conclusion": "Video-EM\u901a\u8fc7\u60c5\u666f\u8bb0\u5fc6\u5efa\u6a21\u548c\u94fe\u5f0f\u601d\u7ef4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09487", "abs": "https://arxiv.org/abs/2508.09487", "authors": ["Ju Yeon Kang", "Jaehong Park", "Semin Kim", "Ji Won Yoon", "Nam Soo Kim"], "title": "SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection", "comment": "Work in progress", "summary": "Recently, diffusion-generated image detection has gained increasing\nattention, as the rapid advancement of diffusion models has raised serious\nconcerns about their potential misuse. While existing detection methods have\nachieved promising results, their performance often degrades significantly when\nfacing fake images from unseen, out-of-distribution (OOD) generative models,\nsince they primarily rely on model-specific artifacts. To address this\nlimitation, we explore a fundamental property commonly observed in fake images.\nMotivated by the observation that fake images tend to exhibit higher similarity\nto their captions than real images, we propose a novel representation, namely\nSemantic-Aware Reconstruction Error (SARE), that measures the semantic\ndifference between an image and its caption-guided reconstruction. The\nhypothesis behind SARE is that real images, whose captions often fail to fully\ncapture their complex visual content, may undergo noticeable semantic shifts\nduring the caption-guided reconstruction process. In contrast, fake images,\nwhich closely align with their captions, show minimal semantic changes. By\nquantifying these semantic shifts, SARE can be utilized as a discriminative\nfeature for robust detection across diverse generative models. We empirically\ndemonstrate that the proposed method exhibits strong generalization,\noutperforming existing baselines on benchmarks including GenImage and\nCommunityForensics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u611f\u77e5\u91cd\u5efa\u8bef\u5dee\uff08SARE\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5047\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u89c1\u751f\u6210\u6a21\u578b\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u6f5c\u5728\u6ee5\u7528\u7684\u62c5\u5fe7\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u89c1\u751f\u6210\u6a21\u578b\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u56fe\u50cf\u4e0e\u5176\u6807\u9898\u5f15\u5bfc\u91cd\u5efa\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u5f02\uff08SARE\uff09\uff0c\u5229\u7528\u5047\u56fe\u50cf\u4e0e\u6807\u9898\u9ad8\u5ea6\u4e00\u81f4\u6027\u7684\u7279\u6027\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSARE\u5728GenImage\u548cCommunityForensics\u7b49\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SARE\u662f\u4e00\u79cd\u6709\u6548\u7684\u8de8\u6a21\u578b\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5e94\u5bf9\u591a\u6837\u5316\u7684\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2508.09522", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09522", "abs": "https://arxiv.org/abs/2508.09522", "authors": ["Ajeet Kumar Yadav", "Nishant Kumar", "Rathna G N"], "title": "Generation of Indian Sign Language Letters, Numbers, and Words", "comment": "6 pages, 5 figures, 2024 International Conference on Intelligent\n  Algorithms for Computational Intelligence Systems (IACIS)", "summary": "Sign language, which contains hand movements, facial expressions and bodily\ngestures, is a significant medium for communicating with hard-of-hearing\npeople. A well-trained sign language community communicates easily, but those\nwho don't know sign language face significant challenges. Recognition and\ngeneration are basic communication methods between hearing and hard-of-hearing\nindividuals. Despite progress in recognition, sign language generation still\nneeds to be explored. The Progressive Growing of Generative Adversarial Network\n(ProGAN) excels at producing high-quality images, while the Self-Attention\nGenerative Adversarial Network (SAGAN) generates feature-rich images at medium\nresolutions. Balancing resolution and detail is crucial for sign language image\ngeneration. We are developing a Generative Adversarial Network (GAN) variant\nthat combines both models to generate feature-rich, high-resolution, and\nclass-conditional sign language images. Our modified Attention-based model\ngenerates high-quality images of Indian Sign Language letters, numbers, and\nwords, outperforming the traditional ProGAN in Inception Score (IS) and\nFr\\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,\nrespectively. Additionally, we are publishing a large dataset incorporating\nhigh-quality images of Indian Sign Language alphabets, numbers, and 129 words.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ProGAN\u548cSAGAN\u7684GAN\u53d8\u4f53\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\uff0c\u5e76\u5728IS\u548cFID\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edfProGAN\u3002", "motivation": "\u624b\u8bed\u751f\u6210\u9886\u57df\u4ecd\u9700\u63a2\u7d22\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5206\u8fa8\u7387\u548c\u7ec6\u8282\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u4e8c\u8005\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Progressive Growing GAN\u548cSelf-Attention GAN\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5370\u5ea6\u624b\u8bed\u7684\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u5355\u8bcd\u56fe\u50cf\u3002", "result": "\u6539\u8fdb\u6a21\u578b\u5728IS\u548cFID\u4e0a\u5206\u522b\u63d0\u5347\u4e863.2\u548c30.12\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u624b\u8bed\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u542c\u969c\u4eba\u58eb\u7684\u6c9f\u901a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09524", "abs": "https://arxiv.org/abs/2508.09524", "authors": ["Yipei Wang", "Shiyu Hu", "Shukun Jia", "Panxi Xu", "Hongfei Ma", "Yiping Ma", "Jing Zhang", "Xiaobo Lu", "Xin Zhao"], "title": "SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking", "comment": null, "summary": "In this paper, we present the first systematic investigation and\nquantification of Similar Object Interference (SOI), a long-overlooked yet\ncritical bottleneck in Single Object Tracking (SOT). Through controlled Online\nInterference Masking (OIM) experiments, we quantitatively demonstrate that\neliminating interference sources leads to substantial performance improvements\n(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a\nprimary constraint for robust tracking and highlighting the feasibility of\nexternal cognitive guidance. Building upon these insights, we adopt natural\nlanguage as a practical form of external guidance, and construct SOIBench-the\nfirst semantic cognitive guidance benchmark specifically targeting SOI\nchallenges. It automatically mines SOI frames through multi-tracker collective\njudgment and introduces a multi-level annotation protocol to generate precise\nsemantic guidance texts. Systematic evaluation on SOIBench reveals a striking\nfinding: existing vision-language tracking (VLT) methods fail to effectively\nexploit semantic cognitive guidance, achieving only marginal improvements or\neven performance degradation (AUC changes of -0.26 to +0.71). In contrast, we\npropose a novel paradigm employing large-scale vision-language models (VLM) as\nexternal cognitive engines that can be seamlessly integrated into arbitrary RGB\ntrackers. This approach demonstrates substantial improvements under semantic\ncognitive guidance (AUC gains up to 0.93), representing a significant\nadvancement over existing VLT methods. We hope SOIBench will serve as a\nstandardized evaluation platform to advance semantic cognitive tracking\nresearch and contribute new insights to the tracking research community.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u5e76\u91cf\u5316\u4e86\u5355\u76ee\u6807\u8ddf\u8e2a\uff08SOT\uff09\u4e2d\u7684\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\uff08SOI\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u8ba4\u77e5\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9SOI\u6311\u6218\u7684\u57fa\u51c6\u6d4b\u8bd5SOIBench\u3002", "motivation": "\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\uff08SOI\uff09\u662f\u5355\u76ee\u6807\u8ddf\u8e2a\uff08SOT\uff09\u4e2d\u88ab\u957f\u671f\u5ffd\u89c6\u7684\u5173\u952e\u74f6\u9888\uff0c\u672c\u6587\u65e8\u5728\u91cf\u5316\u5176\u5f71\u54cd\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5e72\u6270\u63a9\u853d\uff08OIM\uff09\u5b9e\u9a8c\u91cf\u5316SOI\u5f71\u54cd\uff0c\u6784\u5efaSOIBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u6d88\u9664\u5e72\u6270\u6e90\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\uff08AUC\u589e\u76ca\u8fbe4.35\uff09\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\uff08VLT\uff09\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff08AUC\u53d8\u5316-0.26\u81f3+0.71\uff09\uff0c\u800c\u65b0\u65b9\u6cd5\u5728\u8bed\u4e49\u5f15\u5bfc\u4e0b\u8868\u73b0\u4f18\u5f02\uff08AUC\u589e\u76ca\u8fbe0.93\uff09\u3002", "conclusion": "SOIBench\u53ef\u4f5c\u4e3a\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u63a8\u52a8\u8bed\u4e49\u8ba4\u77e5\u8ddf\u8e2a\u7814\u7a76\uff0c\u5e76\u4e3a\u8ddf\u8e2a\u9886\u57df\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002"}}
{"id": "2508.09525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09525", "abs": "https://arxiv.org/abs/2508.09525", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Learning Spatial Decay for Vision Transformers", "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision, yet their\nself-attention mechanism lacks explicit spatial inductive biases, leading to\nsuboptimal performance on spatially-structured tasks. Existing approaches\nintroduce data-independent spatial decay based on fixed distance metrics,\napplying uniform attention weighting regardless of image content and limiting\nadaptability to diverse visual scenarios. Inspired by recent advances in large\nlanguage models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)\nsignificantly outperform static alternatives, we present the first successful\nadaptation of data-dependent spatial decay to 2D vision transformers. We\nintroduce \\textbf{Spatial Decay Transformer (SDT)}, featuring a novel\nContext-Aware Gating (CAG) mechanism that generates dynamic, data-dependent\ndecay for patch interactions. Our approach learns to modulate spatial attention\nbased on both content relevance and spatial proximity. We address the\nfundamental challenge of 1D-to-2D adaptation through a unified spatial-content\nfusion framework that integrates manhattan distance-based spatial priors with\nlearned content representations. Extensive experiments on ImageNet-1K\nclassification and generation tasks demonstrate consistent improvements over\nstrong baselines. Our work establishes data-dependent spatial decay as a new\nparadigm for enhancing spatial attention in vision transformers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDT\u7684\u65b0\u578b\u89c6\u89c9Transformer\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff08CAG\uff09\u52a8\u6001\u8c03\u6574\u7a7a\u95f4\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfViT\u5728\u7a7a\u95f4\u7ed3\u6784\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfViT\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f3a\u4e4f\u663e\u5f0f\u7684\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4\u5728\u7a7a\u95f4\u7ed3\u6784\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u8ddd\u79bb\u5ea6\u91cf\u7684\u7a7a\u95f4\u8870\u51cf\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u89c6\u89c9\u573a\u666f\u3002", "method": "\u63d0\u51faSDT\uff0c\u5f15\u5165CAG\u673a\u5236\uff0c\u52a8\u6001\u751f\u6210\u6570\u636e\u4f9d\u8d56\u7684\u7a7a\u95f4\u8870\u51cf\uff0c\u7ed3\u5408\u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u7a7a\u95f4\u5148\u9a8c\u548c\u5b66\u4e60\u7684\u5185\u5bb9\u8868\u793a\u3002", "result": "\u5728ImageNet-1K\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSDT\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6570\u636e\u4f9d\u8d56\u7684\u7a7a\u95f4\u8870\u51cf\u4e3a\u89c6\u89c9Transformer\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.09528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09528", "abs": "https://arxiv.org/abs/2508.09528", "authors": ["Gang Qu", "Ping Wang", "Siming Zheng", "Xin Yuan"], "title": "Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing", "comment": "9 pages, 4 figures", "summary": "Deep networks have achieved remarkable success in image compressed sensing\n(CS) task, namely reconstructing a high-fidelity image from its compressed\nmeasurement. However, existing works are deficient inincoherent compressed\nmeasurement at sensing phase and implicit measurement representations at\nreconstruction phase, limiting the overall performance. In this work, we answer\ntwo questions: 1) how to improve the measurement incoherence for decreasing the\nill-posedness; 2) how to learn informative representations from measurements.\nTo this end, we propose a novel asymmetric Kronecker CS (AKCS) model and\ntheoretically present its better incoherence than previous Kronecker CS with\nminimal complexity increase. Moreover, we reveal that the unfolding networks'\nsuperiority over non-unfolding ones result from sufficient gradient descents,\ncalled explicit measurement representations. We propose a measurement-aware\ncross attention (MACA) mechanism to learn implicit measurement representations.\nWe integrate AKCS and MACA into widely-used unfolding architecture to get a\nmeasurement-enhanced unfolding network (MEUNet). Extensive experiences\ndemonstrate that our MEUNet achieves state-of-the-art performance in\nreconstruction accuracy and inference speed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u975e\u5bf9\u79f0Kronecker\u538b\u7f29\u611f\u77e5\uff08AKCS\uff09\u6a21\u578b\u548c\u6d4b\u91cf\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\uff08MACA\uff09\u673a\u5236\uff0c\u7ed3\u5408\u4e3aMEUNet\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d4b\u91cf\u9636\u6bb5\u7f3a\u4e4f\u975e\u76f8\u5e72\u6027\uff0c\u91cd\u5efa\u9636\u6bb5\u7f3a\u4e4f\u663e\u5f0f\u6d4b\u91cf\u8868\u793a\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51faAKCS\u6a21\u578b\u63d0\u9ad8\u6d4b\u91cf\u975e\u76f8\u5e72\u6027\uff0cMACA\u673a\u5236\u5b66\u4e60\u9690\u5f0f\u6d4b\u91cf\u8868\u793a\uff0c\u5e76\u6574\u5408\u4e3aMEUNet\u3002", "result": "MEUNet\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "AKCS\u548cMACA\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u6d4b\u91cf\u975e\u76f8\u5e72\u6027\u548c\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u538b\u7f29\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2508.09533", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09533", "abs": "https://arxiv.org/abs/2508.09533", "authors": ["Peiran Peng", "Tingfa Xu", "Liqiang Song", "Mengqi Zhu", "Yuqiang Fang", "Jianan Li"], "title": "COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection", "comment": null, "summary": "Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is\na critical challenge in computer vision, particularly in surveillance, search\nand rescue, and autonomous navigation. Drone-based scenarios exacerbate these\nchallenges due to spatial misalignment, low-light conditions, occlusion, and\ncluttered backgrounds. Current methods struggle to leverage the complementary\ninformation between visible and thermal modalities effectively. We propose\nCOXNet, a novel framework for RGBT tiny object detection, addressing these\nissues through three core innovations: i) the Cross-Layer Fusion Module, fusing\nhigh-level visible and low-level thermal features for enhanced semantic and\nspatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,\ncorrecting cross-modal spatial misalignments and preserving multi-scale\nfeatures; and iii) an optimized label assignment strategy using the GeoShape\nSimilarity Measure for better localization. COXNet achieves a 3.32\\% mAP$_{50}$\nimprovement on the RGBTDronePerson dataset over state-of-the-art methods,\ndemonstrating its effectiveness for robust detection in complex environments.", "AI": {"tldr": "COXNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RGBT\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u878d\u5408\u3001\u52a8\u6001\u5bf9\u9f50\u548c\u6807\u7b7e\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u65e0\u4eba\u673a\u573a\u666f\u4e2d\uff0cRGBT\u56fe\u50cf\u4e2d\u7684\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u7a7a\u95f4\u9519\u4f4d\u3001\u4f4e\u5149\u7167\u548c\u590d\u6742\u80cc\u666f\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u4e92\u8865\u4fe1\u606f\u3002", "method": "COXNet\u91c7\u7528\u8de8\u5c42\u878d\u5408\u6a21\u5757\u3001\u52a8\u6001\u5bf9\u9f50\u4e0e\u5c3a\u5ea6\u7ec6\u5316\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u51e0\u4f55\u5f62\u72b6\u76f8\u4f3c\u5ea6\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728RGBTDronePerson\u6570\u636e\u96c6\u4e0a\uff0cCOXNet\u7684mAP50\u63d0\u5347\u4e863.32%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "COXNet\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3RGBT\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.09543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09543", "abs": "https://arxiv.org/abs/2508.09543", "authors": ["Yuanting Gao", "Linghao Shen"], "title": "Iterative Volume Fusion for Asymmetric Stereo Matching", "comment": null, "summary": "Stereo matching is vital in 3D computer vision, with most algorithms assuming\nsymmetric visual properties between binocular visions. However, the rise of\nasymmetric multi-camera systems (e.g., tele-wide cameras) challenges this\nassumption and complicates stereo matching. Visual asymmetry disrupts stereo\nmatching by affecting the crucial cost volume computation. To address this, we\nexplore the matching cost distribution of two established cost volume\nconstruction methods in asymmetric stereo. We find that each cost volume\nexperiences distinct information distortion, indicating that both should be\ncomprehensively utilized to solve the issue. Based on this, we propose the\ntwo-phase Iterative Volume Fusion network for Asymmetric Stereo matching\n(IVF-AStereo). Initially, the aggregated concatenation volume refines the\ncorrelation volume. Subsequently, both volumes are fused to enhance fine\ndetails. Our method excels in asymmetric scenarios and shows robust performance\nagainst significant visual asymmetry. Extensive comparative experiments on\nbenchmark datasets, along with ablation studies, confirm the effectiveness of\nour approach in asymmetric stereo with resolution and color degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIVF-AStereo\u7684\u53cc\u9636\u6bb5\u8fed\u4ee3\u4f53\u79ef\u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u5bf9\u79f0\u7acb\u4f53\u89c6\u89c9\u4e2d\u7684\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u5047\u8bbe\u53cc\u76ee\u89c6\u89c9\u5bf9\u79f0\uff0c\u4f46\u975e\u5bf9\u79f0\u591a\u76f8\u673a\u7cfb\u7edf\uff08\u5982\u5e7f\u89d2-\u957f\u7126\u76f8\u673a\uff09\u7684\u51fa\u73b0\u6253\u7834\u4e86\u8fd9\u4e00\u5047\u8bbe\uff0c\u5bfc\u81f4\u5339\u914d\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e24\u79cd\u6210\u672c\u4f53\u79ef\u6784\u5efa\u65b9\u6cd5\u7684\u5339\u914d\u6210\u672c\u5206\u5e03\uff0c\u53d1\u73b0\u4fe1\u606f\u5931\u771f\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u5408\u4e24\u79cd\u4f53\u79ef\u7684\u53cc\u9636\u6bb5\u878d\u5408\u7f51\u7edc\u3002", "result": "\u5728\u975e\u5bf9\u79f0\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u5206\u8fa8\u7387\u548c\u989c\u8272\u9000\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u975e\u5bf9\u79f0\u7acb\u4f53\u5339\u914d\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.09547", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09547", "abs": "https://arxiv.org/abs/2508.09547", "authors": ["Fengyi Wu", "Yifei Dong", "Zhi-Qi Cheng", "Yilong Dai", "Guangyu Chen", "Hang Wang", "Qi Dai", "Alexander G. Hauptmann"], "title": "GoViG: Goal-Conditioned Visual Navigation Instruction Generation", "comment": "Under review. Code: https://github.com/F1y1113/GoViG", "summary": "We introduce Goal-Conditioned Visual Navigation Instruction Generation\n(GoViG), a new task that aims to autonomously generate precise and contextually\ncoherent navigation instructions solely from egocentric visual observations of\ninitial and goal states. Unlike conventional approaches that rely on structured\ninputs such as semantic annotations or environmental maps, GoViG exclusively\nleverages raw egocentric visual data, substantially improving its adaptability\nto unseen and unstructured environments. Our method addresses this task by\ndecomposing it into two interconnected subtasks: (1) visual forecasting, which\npredicts intermediate visual states bridging the initial and goal views; and\n(2) instruction generation, which synthesizes linguistically coherent\ninstructions grounded in both observed and anticipated visuals. These subtasks\nare integrated within an autoregressive multimodal large language model trained\nwith tailored objectives to ensure spatial accuracy and linguistic clarity.\nFurthermore, we introduce two complementary multimodal reasoning strategies,\none-pass and interleaved reasoning, to mimic incremental human cognitive\nprocesses during navigation. To evaluate our method, we propose the R2R-Goal\ndataset, combining diverse synthetic and real-world trajectories. Empirical\nresults demonstrate significant improvements over state-of-the-art methods,\nachieving superior BLEU-4 and CIDEr scores along with robust cross-domain\ngeneralization.", "AI": {"tldr": "GoViG\u4efb\u52a1\u5229\u7528\u539f\u59cb\u89c6\u89c9\u6570\u636e\u751f\u6210\u5bfc\u822a\u6307\u4ee4\uff0c\u901a\u8fc7\u89c6\u89c9\u9884\u6d4b\u548c\u6307\u4ee4\u751f\u6210\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u6784\u5316\u8f93\u5165\uff08\u5982\u8bed\u4e49\u6807\u6ce8\u6216\u73af\u5883\u5730\u56fe\uff09\uff0c\u9650\u5236\u4e86\u5728\u672a\u77e5\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002GoViG\u65e8\u5728\u4ec5\u901a\u8fc7\u89c6\u89c9\u89c2\u5bdf\u751f\u6210\u7cbe\u786e\u5bfc\u822a\u6307\u4ee4\u3002", "method": "\u4efb\u52a1\u5206\u89e3\u4e3a\u89c6\u89c9\u9884\u6d4b\uff08\u9884\u6d4b\u4e2d\u95f4\u72b6\u6001\uff09\u548c\u6307\u4ee4\u751f\u6210\uff08\u5408\u6210\u8bed\u8a00\u6307\u4ee4\uff09\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u79cd\u63a8\u7406\u7b56\u7565\uff08\u5355\u6b21\u548c\u4ea4\u66ff\u63a8\u7406\uff09\u3002", "result": "\u5728R2R-Goal\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cBLEU-4\u548cCIDEr\u5206\u6570\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GoViG\u901a\u8fc7\u7eaf\u89c6\u89c9\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u5bfc\u822a\u6307\u4ee4\uff0c\u4e3a\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09550", "abs": "https://arxiv.org/abs/2508.09550", "authors": ["Haowen Wang", "Guowei Zhang", "Xiang Zhang", "Zeyuan Chen", "Haiyang Xu", "Dou Hoon Kwark", "Zhuowen Tu"], "title": "Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification", "comment": null, "summary": "In this paper, we address a key scientific problem in machine learning: Given\na training set for an image classification task, can we train a generative\nmodel on this dataset to enhance the classification performance? (i.e.,\nclosed-set generative data augmentation). We start by exploring the\ndistinctions and similarities between real images and closed-set synthetic\nimages generated by advanced generative models. Through extensive experiments,\nwe offer systematic insights into the effective use of closed-set synthetic\ndata for augmentation. Notably, we empirically determine the equivalent scale\nof synthetic images needed for augmentation. In addition, we also show\nquantitative equivalence between the real data augmentation and open-set\ngenerative augmentation (generative models trained using data beyond the given\ntraining set). While it aligns with the common intuition that real images are\ngenerally preferred, our empirical formulation also offers a guideline to\nquantify the increased scale of synthetic data augmentation required to achieve\ncomparable image classification performance. Our results on natural and medical\nimage datasets further illustrate how this effect varies with the baseline\ntraining set size and the amount of synthetic data incorporated.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u6210\u6a21\u578b\u589e\u5f3a\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u6548\u679c\u5dee\u5f02\uff0c\u5e76\u91cf\u5316\u4e86\u5408\u6210\u6570\u636e\u7684\u9700\u6c42\u89c4\u6a21\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u5982\u4f55\u5229\u7528\u751f\u6210\u6a21\u578b\u589e\u5f3a\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u6548\u679c\uff0c\u91cf\u5316\u5408\u6210\u6570\u636e\u7684\u7b49\u6548\u89c4\u6a21\uff0c\u5e76\u4e0e\u5f00\u653e\u96c6\u751f\u6210\u589e\u5f3a\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u589e\u5f3a\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u9700\u8981\u66f4\u5927\u89c4\u6a21\u624d\u80fd\u4e0e\u771f\u5b9e\u6570\u636e\u589e\u5f3a\u6548\u679c\u76f8\u5f53\uff0c\u4e14\u6548\u679c\u53d7\u57fa\u7ebf\u8bad\u7ec3\u96c6\u5927\u5c0f\u548c\u5408\u6210\u6570\u636e\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u91cf\u5316\u5408\u6210\u6570\u636e\u589e\u5f3a\u9700\u6c42\u7684\u6307\u5357\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u81ea\u7136\u548c\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2508.09555", "categories": ["cs.CV", "55N31, 55U10, 68U10, 68T07", "I.4.6; I.5.4; G.2.3"], "pdf": "https://arxiv.org/pdf/2508.09555", "abs": "https://arxiv.org/abs/2508.09555", "authors": ["Ahmet \u00d6ztel", "\u0130smet Karaca"], "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning", "comment": "10 pages, 5 figures, includes visual abstract, focuses on topological\n  invariants for iris recognition", "summary": "Objective - This study presents a biometric identification method based on\ntopological invariants from 2D iris images, representing iris texture via\nformally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids\n(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their\nratio using a recent algorithm for homology groups in 2D digital images. The\nresulting invariants form a feature matrix used with logistic regression, KNN,\nand SVM (with PCA and 100 randomized repetitions). A convolutional neural\nnetwork (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,\noutperforming CNN (96.44 +/- 1.32%) and other feature-based models. The\ntopological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal\ndigital homology for iris recognition. The method offers a compact,\ninterpretable, and accurate alternative to deep learning, useful when\nexplainability or limited data is important. Beyond iris recognition, it can\napply to other biometrics, medical imaging, materials science, remote sensing,\nand interpretable AI. It runs efficiently on CPU-only systems and produces\nrobust, explainable features valuable for security-critical domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u4e0d\u53d8\u91cf\u7684\u8679\u819c\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b57\u540c\u8c03\u7406\u8bba\u63d0\u53d6\u7279\u5f81\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u8679\u819c\u8bc6\u522b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u6216\u9700\u8981\u89e3\u91ca\u6027\u7684\u573a\u666f\u3002", "method": "\u5c06\u8679\u819c\u56fe\u50cf\u5206\u5757\uff0c\u8ba1\u7b97\u6bcf\u5757\u7684Betti0\u3001Betti1\u53ca\u5176\u6bd4\u503c\uff0c\u5f62\u6210\u7279\u5f81\u77e9\u9635\uff0c\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u7b49\u5206\u7c7b\u5668\u8fdb\u884c\u8bc6\u522b\u3002", "result": "\u903b\u8f91\u56de\u5f52\u51c6\u786e\u7387\u8fbe97.78%\uff0c\u4f18\u4e8eCNN\uff0896.44%\uff09\u548c\u5176\u4ed6\u7279\u5f81\u6a21\u578b\uff0c\u7279\u5f81\u7a33\u5b9a\u6027\u9ad8\u3002", "conclusion": "\u9996\u6b21\u5c06\u6570\u5b57\u540c\u8c03\u7406\u8bba\u7528\u4e8e\u8679\u819c\u8bc6\u522b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2508.09565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09565", "abs": "https://arxiv.org/abs/2508.09565", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description", "comment": null, "summary": "Multi-exposure correction technology is essential for restoring images\naffected by insufficient or excessive lighting, enhancing the visual experience\nby improving brightness, contrast, and detail richness. However, current\nmulti-exposure correction methods often encounter challenges in addressing\nintra-class variability caused by diverse lighting conditions, shooting\nenvironments, and weather factors, particularly when processing images captured\nat a single exposure level. To enhance the adaptability of these models under\ncomplex imaging conditions, this paper proposes a Wavelet-based Exposure\nCorrection method with Degradation Guidance (WEC-DG). Specifically, we\nintroduce a degradation descriptor within the Exposure Consistency Alignment\nModule (ECAM) at both ends of the processing pipeline to ensure exposure\nconsistency and achieve final alignment. This mechanism effectively addresses\nmiscorrected exposure anomalies caused by existing methods' failure to\nrecognize 'blurred' exposure degradation. Additionally, we investigate the\nlight-detail decoupling properties of the wavelet transform to design the\nExposure Restoration and Detail Reconstruction Module (EDRM), which processes\nlow-frequency information related to exposure enhancement before utilizing\nhigh-frequency information as a prior guide for reconstructing spatial domain\ndetails. This serial processing strategy guarantees precise light correction\nand enhances detail recovery. Extensive experiments conducted on multiple\npublic datasets demonstrate that the proposed method outperforms existing\nalgorithms, achieving significant performance improvements and validating its\neffectiveness and practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u591a\u66dd\u5149\u6821\u6b63\u65b9\u6cd5\uff08WEC-DG\uff09\uff0c\u901a\u8fc7\u9000\u5316\u63cf\u8ff0\u7b26\u548c\u5c0f\u6ce2\u7279\u6027\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u66dd\u5149\u6821\u6b63\u65b9\u6cd5\u5728\u5904\u7406\u5355\u66dd\u5149\u56fe\u50cf\u65f6\u96be\u4ee5\u5e94\u5bf9\u5149\u7167\u3001\u73af\u5883\u548c\u5929\u6c14\u7b49\u56e0\u7d20\u5f15\u8d77\u7684\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u66dd\u5149\u5f02\u5e38\u6821\u6b63\u4e0d\u51c6\u786e\u3002", "method": "\u8bbe\u8ba1\u4e86\u66dd\u5149\u4e00\u81f4\u6027\u5bf9\u9f50\u6a21\u5757\uff08ECAM\uff09\u548c\u66dd\u5149\u6062\u590d\u4e0e\u7ec6\u8282\u91cd\u5efa\u6a21\u5757\uff08EDRM\uff09\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u7684\u5149-\u7ec6\u8282\u89e3\u8026\u7279\u6027\u8fdb\u884c\u5e8f\u5217\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002", "conclusion": "WEC-DG\u65b9\u6cd5\u5728\u590d\u6742\u6210\u50cf\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u66dd\u5149\u6821\u6b63\u4e2d\u7684\u7ec6\u8282\u6062\u590d\u548c\u4e00\u81f4\u6027\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2508.09566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09566", "abs": "https://arxiv.org/abs/2508.09566", "authors": ["Haibo Jin", "Haoxuan Che", "Sunan He", "Hao Chen"], "title": "A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation", "comment": "Accepted to IEEE TMI", "summary": "Despite the progress of radiology report generation (RRG), existing works\nface two challenges: 1) The performances in clinical efficacy are\nunsatisfactory, especially for lesion attributes description; 2) the generated\ntext lacks explainability, making it difficult for radiologists to trust the\nresults. To address the challenges, we focus on a trustworthy RRG model, which\nnot only generates accurate descriptions of abnormalities, but also provides\nbasis of its predictions. To this end, we propose a framework named chain of\ndiagnosis (CoD), which maintains a chain of diagnostic process for clinically\naccurate and explainable RRG. It first generates question-answer (QA) pairs via\ndiagnostic conversation to extract key findings, then prompts a large language\nmodel with QA diagnoses for accurate generation. To enhance explainability, a\ndiagnosis grounding module is designed to match QA diagnoses and generated\nsentences, where the diagnoses act as a reference. Moreover, a lesion grounding\nmodule is designed to locate abnormalities in the image, further improving the\nworking efficiency of radiologists. To facilitate label-efficient training, we\npropose an omni-supervised learning strategy with clinical consistency to\nleverage various types of annotations from different datasets. Our efforts lead\nto 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a\nevaluation tool for assessing the accuracy of reports in describing lesion\nlocation and severity; 3) extensive experiments to demonstrate the\neffectiveness of CoD, where it outperforms both specialist and generalist\nmodels consistently on two RRG benchmarks and shows promising explainability by\naccurately grounding generated sentences to QA diagnoses and images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8bca\u65ad\u94fe\uff08CoD\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u4e2d\u7684\u4e34\u5e8a\u6548\u679c\u4e0d\u4f73\u548c\u751f\u6210\u6587\u672c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002\u901a\u8fc7\u751f\u6210\u95ee\u7b54\u5bf9\u548c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0cCoD\u63d0\u9ad8\u4e86\u62a5\u544a\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u75c5\u7076\u5b9a\u4f4d\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u5728\u4e34\u5e8a\u6548\u679c\uff08\u5c24\u5176\u662f\u75c5\u7076\u5c5e\u6027\u63cf\u8ff0\uff09\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u653e\u5c04\u79d1\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u751f\u6210\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u8bca\u65ad\u94fe\uff08CoD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u8bca\u65ad\u5bf9\u8bdd\u7684\u95ee\u7b54\u5bf9\u63d0\u53d6\u5173\u952e\u53d1\u73b0\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u51c6\u786e\u62a5\u544a\u3002\u8bbe\u8ba1\u4e86\u8bca\u65ad\u548c\u75c5\u7076\u5b9a\u4f4d\u6a21\u5757\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u5168\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u4ee5\u9ad8\u6548\u5229\u7528\u6807\u6ce8\u6570\u636e\u3002", "result": "CoD\u5728\u4e24\u4e2aRRG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e13\u5bb6\u548c\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u51c6\u786e\u5c06\u751f\u6210\u7684\u53e5\u5b50\u4e0e\u95ee\u7b54\u8bca\u65ad\u548c\u56fe\u50cf\u5173\u8054\u3002", "conclusion": "CoD\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2508.09575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09575", "abs": "https://arxiv.org/abs/2508.09575", "authors": ["Jiwon Kim", "Pureum Kim", "SeonHwa Kim", "Soobin Park", "Eunju Cha", "Kyong Hwan Jin"], "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion", "comment": null, "summary": "Recent advancements in controllable text-to-image (T2I) diffusion models,\nsuch as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance\ncontrol without requiring auxiliary module training. However, these models\noften struggle to accurately preserve spatial structures and fail to capture\nfine-grained conditions related to object poses and scene layouts. To address\nthese challenges, we propose a training-free Dual Recursive Feedback (DRF)\nsystem that properly reflects control conditions in controllable T2I models.\nThe proposed DRF consists of appearance feedback and generation feedback that\nrecursively refines the intermediate latents to better reflect the given\nappearance information and the user's intent. This dual-update mechanism guides\nlatent representations toward reliable manifolds, effectively integrating\nstructural and appearance attributes. Our approach enables fine-grained\ngeneration even between class-invariant structure-appearance fusion, such as\ntransferring human motion onto a tiger's form. Extensive experiments\ndemonstrate the efficacy of our method in producing high-quality, semantically\ncoherent, and structurally consistent image generations. Our source code is\navailable at https://github.com/jwonkm/DRF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u53cc\u9012\u5f52\u53cd\u9988\uff08DRF\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdb\u53ef\u63a7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u7684\u7a7a\u95f4\u548c\u5916\u89c2\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u53ef\u63a7T2I\u6a21\u578b\u5728\u4fdd\u7559\u7a7a\u95f4\u7ed3\u6784\u548c\u6355\u6349\u7ec6\u7c92\u5ea6\u6761\u4ef6\uff08\u5982\u7269\u4f53\u59ff\u6001\u548c\u573a\u666f\u5e03\u5c40\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5916\u89c2\u53cd\u9988\u548c\u751f\u6210\u53cd\u9988\u7684\u53cc\u9012\u5f52\u673a\u5236\uff0c\u4f18\u5316\u4e2d\u95f4\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5916\u89c2\u4fe1\u606f\u548c\u7528\u6237\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u7ed3\u6784\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u652f\u6301\u7c7b\u4e0d\u53d8\u7684\u7ed3\u6784-\u5916\u89c2\u878d\u5408\u3002", "conclusion": "DRF\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u53ef\u63a7T2I\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.09584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09584", "abs": "https://arxiv.org/abs/2508.09584", "authors": ["Bei Yan", "Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "title": "SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs", "comment": null, "summary": "Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer\nfrom hallucinations, i.e., generating content inconsistent with input or\nestablished world knowledge, which correspond to faithfulness and factuality\nhallucinations, respectively. Prior studies primarily evaluate faithfulness\nhallucination at a coarse level (e.g., object-level) and lack fine-grained\nanalysis. Additionally, existing benchmarks rely on costly manual curation or\nreused public datasets, raising concerns about scalability and data leakage. To\naddress these limitations, we propose an automated data construction pipeline\nthat produces scalable, controllable, and diverse evaluation data. We also\ndesign a hierarchical hallucination induction framework with input\nperturbations to simulate realistic noisy scenarios. Integrating these designs,\nwe construct SHALE, a Scalable HALlucination Evaluation benchmark designed to\nassess both faithfulness and factuality hallucinations via a fine-grained\nhallucination categorization scheme. SHALE comprises over 30K image-instruction\npairs spanning 12 representative visual perception aspects for faithfulness and\n6 knowledge domains for factuality, considering both clean and noisy scenarios.\nExtensive experiments on over 20 mainstream LVLMs reveal significant factuality\nhallucinations and high sensitivity to semantic perturbations.", "AI": {"tldr": "SHALE\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u5206\u6790\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6db5\u76d6\u5fe0\u5b9e\u6027\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u5e76\u63d0\u4f9b\u566a\u58f0\u573a\u666f\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LVLMs\u7684\u5e7b\u89c9\u8bc4\u4f30\u4e0d\u591f\u7ec6\u7c92\u5ea6\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u6216\u516c\u5171\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u53ef\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u548c\u5206\u5c42\u5e7b\u89c9\u8bf1\u5bfc\u6846\u67b6\uff0c\u751f\u6210\u591a\u6837\u5316\u8bc4\u4f30\u6570\u636e\u5e76\u6a21\u62df\u566a\u58f0\u573a\u666f\u3002", "result": "SHALE\u5305\u542b30K+\u56fe\u50cf-\u6307\u4ee4\u5bf9\uff0c\u8986\u76d612\u4e2a\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u548c6\u4e2a\u77e5\u8bc6\u9886\u57df\uff0c\u5b9e\u9a8c\u663e\u793a\u4e3b\u6d41LVLMs\u5b58\u5728\u663e\u8457\u4e8b\u5b9e\u6027\u5e7b\u89c9\u548c\u5bf9\u8bed\u4e49\u6270\u52a8\u7684\u9ad8\u654f\u611f\u6027\u3002", "conclusion": "SHALE\u4e3aLVLMs\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2508.09585", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09585", "abs": "https://arxiv.org/abs/2508.09585", "authors": ["Stefan Haag", "Bharanidhar Duraisamy", "Felix Govaers", "Wolfgang Koch", "Martin Fritzsche", "Juergen Dickmann"], "title": "Offline Auto Labeling: BAAS", "comment": null, "summary": "This paper introduces BAAS, a new Extended Object Tracking (EOT) and\nfusion-based label annotation framework for radar detections in autonomous\ndriving. Our framework utilizes Bayesian-based tracking, smoothing and\neventually fusion methods to provide veritable and precise object trajectories\nalong with shape estimation to provide annotation labels on the detection level\nunder various supervision levels. Simultaneously, the framework provides\nevaluation of tracking performance and label annotation. If manually labeled\ndata is available, each processing module can be analyzed independently or\ncombined with other modules to enable closed-loop continuous improvements. The\nframework performance is evaluated in a challenging urban real-world scenario\nin terms of tracking performance and the label annotation errors. We\ndemonstrate the functionality of the proposed approach for varying dynamic\nobjects and class types", "AI": {"tldr": "BAAS\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u8ddf\u8e2a\u548c\u878d\u5408\u7684\u96f7\u8fbe\u68c0\u6d4b\u6807\u6ce8\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6269\u5c55\u76ee\u6807\u8ddf\u8e2a\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u8f68\u8ff9\u548c\u5f62\u72b6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96f7\u8fbe\u68c0\u6d4b\u7684\u6807\u6ce8\u548c\u8ddf\u8e2a\u95ee\u9898\uff0c\u652f\u6301\u591a\u7ea7\u76d1\u7763\u548c\u95ed\u73af\u6539\u8fdb\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u8ddf\u8e2a\u3001\u5e73\u6ed1\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u624b\u52a8\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6a21\u5757\u5316\u5206\u6790\u3002", "result": "\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8ddf\u8e2a\u6027\u80fd\u548c\u6807\u6ce8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u52a8\u6001\u76ee\u6807\u548c\u7c7b\u522b\u3002", "conclusion": "BAAS\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u96f7\u8fbe\u68c0\u6d4b\u6807\u6ce8\u548c\u8ddf\u8e2a\uff0c\u652f\u6301\u6301\u7eed\u4f18\u5316\u3002"}}
{"id": "2508.09593", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09593", "abs": "https://arxiv.org/abs/2508.09593", "authors": ["Haotian Tang", "Jianwei Chen", "Xinrui Tang", "Yunjia Wu", "Zhengyang Miao", "Chao Li"], "title": "Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma", "comment": null, "summary": "Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for\nglioma prognosis. However, current prediction methods are limited by the low\navailability and noise of functional MRI. Structural and morphological\nconnectomes offer a non-invasive alternative, yet existing approaches often\nignore the brain's hierarchical organisation and multiscale interactions. To\naddress this, we propose Hi-SMGNN, a hierarchical framework that integrates\nstructural and morphological connectomes from regional to modular levels. It\nfeatures a multimodal interaction module with a Siamese network and cross-modal\nattention, a multiscale feature fusion mechanism for reducing redundancy, and a\npersonalised modular partitioning strategy to enhance individual specificity\nand interpretability. Experiments on the UCSF-PDGM dataset demonstrate that\nHi-SMGNN outperforms baseline and state-of-the-art models, showing improved\nrobustness and effectiveness in IDH mutation prediction.", "AI": {"tldr": "Hi-SMGNN\u662f\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u6574\u5408\u7ed3\u6784\u548c\u5f62\u6001\u8fde\u63a5\u7ec4\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u63d0\u5347IDH\u7a81\u53d8\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u529f\u80fdMRI\u7684\u4f4e\u53ef\u7528\u6027\u548c\u566a\u58f0\uff0c\u4e14\u5ffd\u7565\u5927\u8111\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u591a\u5c3a\u5ea6\u4ea4\u4e92\u3002", "method": "\u63d0\u51faHi-SMGNN\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u5f62\u6001\u8fde\u63a5\u7ec4\uff0c\u91c7\u7528Siamese\u7f51\u7edc\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u53ca\u4e2a\u6027\u5316\u6a21\u5757\u5212\u5206\u3002", "result": "\u5728UCSF-PDGM\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u9884\u6d4b\u6548\u679c\u548c\u9c81\u68d2\u6027\u63d0\u5347\u3002", "conclusion": "Hi-SMGNN\u4e3aIDH\u7a81\u53d8\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u975e\u4fb5\u5165\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u4e2a\u4f53\u7279\u5f02\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.09597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09597", "abs": "https://arxiv.org/abs/2508.09597", "authors": ["Heyi Sun", "Cong Wang", "Tian-Xing Xu", "Jingwei Huang", "Di Kang", "Chunchao Guo", "Song-Hai Zhang"], "title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing", "comment": null, "summary": "Creating high-fidelity and editable head avatars is a pivotal challenge in\ncomputer vision and graphics, boosting many AR/VR applications. While recent\nadvancements have achieved photorealistic renderings and plausible animation,\nhead editing, especially real-time appearance editing, remains challenging due\nto the implicit representation and entangled modeling of the geometry and\nglobal appearance. To address this, we propose Surface-Volumetric Gaussian Head\nAvatar (SVG-Head), a novel hybrid representation that explicitly models the\ngeometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled\ntexture images to capture the global appearance. Technically, it contains two\ntypes of Gaussians, in which surface Gaussians explicitly model the appearance\nof head avatars using learnable texture images, facilitating real-time texture\nediting, while volumetric Gaussians enhance the reconstruction quality of\nnon-Lambertian regions (e.g., lips and hair). To model the correspondence\nbetween 3D world and texture space, we provide a mesh-aware Gaussian UV mapping\nmethod, which leverages UV coordinates given by the FLAME mesh to obtain sharp\ntexture images and real-time rendering speed. A hierarchical optimization\nstrategy is further designed to pursue the optimal performance in both\nreconstruction quality and editing flexibility. Experiments on the NeRSemble\ndataset show that SVG-Head not only generates high-fidelity rendering results,\nbut also is the first method to obtain explicit texture images for Gaussian\nhead avatars and support real-time appearance editing.", "AI": {"tldr": "SVG-Head\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u8868\u9762\u548c\u4f53\u79ef\u9ad8\u65af\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u53ef\u5b9e\u65f6\u7f16\u8f91\u7684\u5934\u50cf\u5efa\u6a21\u3002", "motivation": "\u89e3\u51b3\u5934\u50cf\u5efa\u6a21\u4e2d\u51e0\u4f55\u4e0e\u5168\u5c40\u5916\u89c2\u7684\u9690\u5f0f\u8868\u793a\u548c\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u5347AR/VR\u5e94\u7528\u7684\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u4f7f\u7528FLAME\u7f51\u683c\u4e0a\u76843D\u9ad8\u65af\u548c\u5206\u79bb\u7684\u7eb9\u7406\u56fe\u50cf\uff0c\u7ed3\u5408\u8868\u9762\u548c\u4f53\u79ef\u9ad8\u65af\uff0c\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u3002", "result": "\u5728NeRSemble\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u5e76\u9996\u6b21\u652f\u6301\u9ad8\u65af\u5934\u50cf\u7684\u663e\u5f0f\u7eb9\u7406\u56fe\u50cf\u548c\u5b9e\u65f6\u7f16\u8f91\u3002", "conclusion": "SVG-Head\u4e3a\u5934\u50cf\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u548c\u5b9e\u65f6\u7f16\u8f91\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09598", "abs": "https://arxiv.org/abs/2508.09598", "authors": ["Jie Shao", "Ke Zhu", "Minghao Fu", "Guo-hua Wang", "Jianxin Wu"], "title": "Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality", "comment": null, "summary": "Diffusion models have achieved remarkable progress in class-to-image\ngeneration. However, we observe that despite impressive FID scores,\nstate-of-the-art models often generate distorted or low-quality images,\nespecially in certain classes. This gap arises because FID evaluates global\ndistribution alignment, while ignoring the perceptual quality of individual\nsamples. We further examine the role of CFG, a common technique used to enhance\ngeneration quality. While effective in improving metrics and suppressing\noutliers, CFG can introduce distribution shift and visual artifacts due to its\nmisalignment with both training objectives and user expectations. In this work,\nwe propose FaME, a training-free and inference-efficient method for improving\nperceptual quality. FaME uses an image quality assessment model to identify\nlow-quality generations and stores their sampling trajectories. These failure\nmodes are then used as negative guidance to steer future sampling away from\npoor-quality regions. Experiments on ImageNet demonstrate that FaME brings\nconsistent improvements in visual quality without compromising FID. FaME also\nshows the potential to be extended to improve text-to-image generation.", "AI": {"tldr": "FaME\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u4f4e\u8d28\u91cf\u751f\u6210\u6837\u672c\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u4f5c\u4e3a\u8d1f\u5411\u5f15\u5bfc\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u7c7b\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u67d0\u4e9b\u7c7b\u522b\u751f\u6210\u7684\u56fe\u50cf\u5b58\u5728\u626d\u66f2\u6216\u4f4e\u8d28\u91cf\u95ee\u9898\u3002FID\u6307\u6807\u4ec5\u8bc4\u4f30\u5168\u5c40\u5206\u5e03\u5bf9\u9f50\uff0c\u5ffd\u7565\u5355\u4e2a\u6837\u672c\u7684\u611f\u77e5\u8d28\u91cf\u3002CFG\u6280\u672f\u867d\u80fd\u63d0\u5347\u6307\u6807\uff0c\u4f46\u4f1a\u5f15\u5165\u5206\u5e03\u504f\u79fb\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u63d0\u51faFaME\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u4f4e\u8d28\u91cf\u751f\u6210\u6837\u672c\uff0c\u5b58\u50a8\u5176\u91c7\u6837\u8f68\u8ff9\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u4f5c\u4e3a\u8d1f\u5411\u5f15\u5bfc\uff0c\u907f\u514d\u672a\u6765\u91c7\u6837\u8fdb\u5165\u4f4e\u8d28\u91cf\u533a\u57df\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaME\u5728\u4e0d\u5f71\u54cdFID\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "FaME\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u6709\u671b\u6269\u5c55\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2508.09599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09599", "abs": "https://arxiv.org/abs/2508.09599", "authors": ["Beomjun Kim", "Suhan Woo", "Sejong Heo", "Euntai Kim"], "title": "BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation", "comment": "9 pages, 6 figures", "summary": "Bird's-Eye-View (BEV) map segmentation is one of the most important and\nchallenging tasks in autonomous driving. Camera-only approaches have drawn\nattention as cost-effective alternatives to LiDAR, but they still fall behind\nLiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been\nexplored to narrow this gap, but existing methods mainly enlarge the student\nmodel by mimicking the teacher's architecture, leading to higher inference\ncost. To address this issue, we introduce BridgeTA, a cost-effective\ndistillation framework to bridge the representation gap between LC fusion and\nCamera-only models through a Teacher Assistant (TA) network while keeping the\nstudent's architecture and inference cost unchanged. A lightweight TA network\ncombines the BEV representations of the teacher and student, creating a shared\nlatent space that serves as an intermediate representation. To ground the\nframework theoretically, we derive a distillation loss using Young's\nInequality, which decomposes the direct teacher-student distillation path into\nteacher-TA and TA-student dual paths, stabilizing optimization and\nstrengthening knowledge transfer. Extensive experiments on the challenging\nnuScenes dataset demonstrate the effectiveness of our method, achieving an\nimprovement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than\nthe improvement of other state-of-the-art KD methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBridgeTA\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6559\u5e08\u52a9\u7406\u7f51\u7edc\u7f29\u5c0fLiDAR-Camera\u878d\u5408\u4e0e\u7eaf\u76f8\u673a\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u8ddd\uff0c\u63d0\u5347\u6027\u80fd\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u7eaf\u76f8\u673a\u65b9\u6cd5\u5728BEV\u5206\u5272\u4efb\u52a1\u4e2d\u6027\u80fd\u843d\u540e\u4e8eLiDAR-Camera\u878d\u5408\u65b9\u6cd5\uff0c\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u56e0\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u67b6\u6784\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u589e\u52a0\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u6559\u5e08\u52a9\u7406\u7f51\u7edc\uff0c\u7ed3\u5408\u6559\u5e08\u548c\u5b66\u751f\u7684BEV\u8868\u793a\uff0c\u6784\u5efa\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7Young\u4e0d\u7b49\u5f0f\u63a8\u5bfc\u84b8\u998f\u635f\u5931\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u53474.2% mIoU\uff0c\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u84b8\u998f\u65b9\u6cd545%\u3002", "conclusion": "BridgeTA\u6846\u67b6\u6709\u6548\u63d0\u5347\u7eaf\u76f8\u673a\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2508.09616", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09616", "abs": "https://arxiv.org/abs/2508.09616", "authors": ["Daniel Barco", "Marc Stadelmann", "Martin Oswald", "Ivo Herzig", "Lukas Lichtensteiger", "Pascal Paysan", "Igor Peterlik", "Michal Walczak", "Bjoern Menze", "Frank-Peter Schilling"], "title": "MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography", "comment": null, "summary": "We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first\n3D conditional diffusion-based model for real-world sparse-view Cone Beam\nComputed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation\nexposure. A key contribution is extending the \"InDI\" concept from 2D to a full\n3D volumetric approach for medical images, implementing an iterative denoising\nprocess that refines the CBCT volume directly from sparse-view input. A further\ncontribution is the generation of a large pseudo-CBCT dataset (16,182) from\nchest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We\nperformed a comprehensive evaluation, including quantitative metrics,\nscalability analysis, generalisation tests, and a clinical assessment by 11\nclinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)\ndB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE\npseudo-CBCT (independent real-world) test set and enabling an 8x reduction in\nimaging radiation exposure. We demonstrate its scalability by showing that\nperformance improves with more training data. Importantly, MInDI-3D matches the\nperformance of a 3D U-Net on real-world scans from 16 cancer patients across\ndistortion and task-based metrics. It also generalises to new CBCT scanner\ngeometries. Clinicians rated our model as sufficient for patient positioning\nacross all anatomical sites and found it preserved lung tumour boundaries well.", "AI": {"tldr": "MInDI-3D\u662f\u9996\u4e2a\u57fa\u4e8e3D\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u7a00\u758f\u89c6\u56feCBCT\u4f2a\u5f71\u53bb\u9664\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u63d0\u5347\u7a00\u758f\u89c6\u56feCBCT\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u5c062D InDI\u6269\u5c55\u52303D\uff0c\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u76f4\u63a5\u4ece\u7a00\u758f\u8f93\u5165\u4f18\u5316CBCT\u4f53\u79ef\uff0c\u5e76\u5229\u7528\u4f2aCBCT\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728CT-RATE\u6d4b\u8bd5\u96c6\u4e0aPSNR\u63d0\u534712.96 dB\uff0c\u8f90\u5c04\u5242\u91cf\u964d\u4f4e8\u500d\uff0c\u4e14\u6027\u80fd\u4e0e3D U-Net\u76f8\u5f53\u3002", "conclusion": "MInDI-3D\u5728\u4e34\u5e8a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u5256\u90e8\u4f4d\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0CBCT\u626b\u63cf\u4eea\u3002"}}
{"id": "2508.09626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09626", "abs": "https://arxiv.org/abs/2508.09626", "authors": ["Xu Tang", "Junan Jia", "Yijing Wang", "Jingjing Ma", "Xiangrong Zhang"], "title": "Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation", "comment": "9 pages, 4 figures, AAAI 2026", "summary": "In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),\ntraditional methods struggle to address semantic ambiguity caused by scale\nvariations and structural occlusions in aerial images. This limits their\nsegmentation accuracy and consistency. To tackle these challenges, we propose a\nnovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian\npoint drop module, which integrates semantic confidence estimation with a\nlearnable sparsity mechanism based on the Hard Concrete distribution. This\nmodule effectively eliminates redundant and semantically ambiguous Gaussian\npoints, enhancing both segmentation performance and representation compactness.\nFurthermore, SAD-Splat incorporates a high-confidence pseudo-label generation\npipeline. It leverages 2D foundation models to enhance supervision when\nground-truth labels are limited, thereby further improving segmentation\naccuracy. To advance research in this domain, we introduce a challenging\nbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse\nreal-world aerial scenes with sparse annotations. Experimental results\ndemonstrate that SAD-Splat achieves an excellent balance between segmentation\naccuracy and representation compactness. It offers an efficient and scalable\nsolution for 3D aerial scene understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAD-Splat\u76843D-AVS-SS\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u70b9\u4e22\u5f03\u6a21\u5757\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u751f\u6210\u7ba1\u9053\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c63D-AS\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u57283D-AVS-SS\u4efb\u52a1\u4e2d\u96be\u4ee5\u5904\u7406\u5c3a\u5ea6\u53d8\u5316\u548c\u7ed3\u6784\u906e\u6321\u5bfc\u81f4\u7684\u8bed\u4e49\u6a21\u7cca\uff0c\u9650\u5236\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u9ad8\u65af\u70b9\u4e22\u5f03\u6a21\u5757\uff08\u57fa\u4e8eHard Concrete\u5206\u5e03\uff09\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u76d1\u7763\u3002", "result": "SAD-Splat\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8868\u793a\u7d27\u51d1\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u79c0\u5e73\u8861\uff0c\u5e76\u57283D-AS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SAD-Splat\u4e3a3D\u7a7a\u4e2d\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09629", "abs": "https://arxiv.org/abs/2508.09629", "authors": ["Giorgos Karvounas", "Nikolaos Kyriazis", "Iason Oikonomidis", "Georgios Pavlakos", "Antonis A. Argyros"], "title": "Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors", "comment": null, "summary": "We revisit the role of texture in monocular 3D hand reconstruction, not as an\nafterthought for photorealism, but as a dense, spatially grounded cue that can\nactively support pose and shape estimation. Our observation is simple: even in\nhigh-performing models, the overlay between predicted hand geometry and image\nappearance is often imperfect, suggesting that texture alignment may be an\nunderused supervisory signal. We propose a lightweight texture module that\nembeds per-pixel observations into UV texture space and enables a novel dense\nalignment loss between predicted and observed hand appearances. Our approach\nassumes access to a differentiable rendering pipeline and a model that maps\nimages to 3D hand meshes with known topology, allowing us to back-project a\ntextured hand onto the image and perform pixel-based alignment. The module is\nself-contained and easily pluggable into existing reconstruction pipelines. To\nisolate and highlight the value of texture-guided supervision, we augment\nHaMeR, a high-performing yet unadorned transformer architecture for 3D hand\npose estimation. The resulting system improves both accuracy and realism,\ndemonstrating the value of appearance-guided alignment in hand reconstruction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7eb9\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u7eb9\u7406\u5bf9\u9f50\u63d0\u5347\u5355\u76ee3D\u624b\u90e8\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u6a21\u578b\u5728\u624b\u90e8\u51e0\u4f55\u4e0e\u56fe\u50cf\u5916\u89c2\u7684\u5bf9\u9f50\u4e0a\u4ecd\u4e0d\u5b8c\u7f8e\uff0c\u7eb9\u7406\u5bf9\u9f50\u53ef\u80fd\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7eb9\u7406\u6a21\u5757\uff0c\u5c06\u50cf\u7d20\u89c2\u6d4b\u5d4c\u5165UV\u7eb9\u7406\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bc6\u96c6\u5bf9\u9f50\u635f\u5931\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u7eb9\u7406\u5f15\u5bfc\u7684\u76d1\u7763\uff0c\u7cfb\u7edf\u5728\u624b\u90e8\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u7eb9\u7406\u5bf9\u9f50\u57283D\u624b\u90e8\u91cd\u5efa\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e14\u8be5\u65b9\u6cd5\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2508.09632", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09632", "abs": "https://arxiv.org/abs/2508.09632", "authors": ["Jingwei Liu", "Ling Yang", "Hao Luo", "Fan Wang Hongyan Li", "Mengdi Wang"], "title": "Preacher: Paper-to-Video Agentic System", "comment": null, "summary": "The paper-to-video task converts a research paper into a structured video\nabstract, distilling key concepts, methods, and conclusions into an accessible,\nwell-organized format. While state-of-the-art video generation models\ndemonstrate potential, they are constrained by limited context windows, rigid\nvideo duration constraints, limited stylistic diversity, and an inability to\nrepresent domain-specific knowledge. To address these limitations, we introduce\nPreacher, the first paper-to-video agentic system. Preacher employs a top-down\napproach to decompose, summarize, and reformulate the paper, followed by\nbottom-up video generation, synthesizing diverse video segments into a coherent\nabstract. To align cross-modal representations, we define key scenes and\nintroduce a Progressive Chain of Thought (P-CoT) for granular, iterative\nplanning. Preacher successfully generates high-quality video abstracts across\nfive research fields, demonstrating expertise beyond current video generation\nmodels. Code will be released at: https://github.com/GenVerse/Paper2Video", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u89c6\u9891\u6458\u8981\u7684\u7cfb\u7edfPreacher\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5b58\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u3001\u89c6\u9891\u65f6\u957f\u56fa\u5b9a\u3001\u98ce\u683c\u5355\u4e00\u548c\u65e0\u6cd5\u8868\u8fbe\u9886\u57df\u77e5\u8bc6\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u7684\u5206\u89e3\u3001\u603b\u7ed3\u548c\u91cd\u6784\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u4e0b\u800c\u4e0a\u7684\u89c6\u9891\u751f\u6210\uff0c\u5f15\u5165\u5173\u952e\u573a\u666f\u548c\u6e10\u8fdb\u5f0f\u601d\u7ef4\u94fe\uff08P-CoT\uff09\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "Preacher\u5728\u4e94\u4e2a\u7814\u7a76\u9886\u57df\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6458\u8981\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Preacher\u662f\u9996\u4e2a\u8bba\u6587\u5230\u89c6\u9891\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5728\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09644", "abs": "https://arxiv.org/abs/2508.09644", "authors": ["Shengjun Zhu", "Siyu Liu", "Runqing Xiong", "Liping Zheng", "Duo Ma", "Rongshang Chen", "Jiaxin Cai"], "title": "Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification", "comment": null, "summary": "Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural\ndevelopment and detecting abnormalities, contributing to reduced perinatal\ncomplications and improved neonatal survival. Accurate identification of\nstandard fetal torso planes is essential for reliable assessment and\npersonalized prenatal care. However, limitations such as low contrast and\nunclear texture details in ultrasound imaging pose significant challenges for\nfine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast\nFusion Module (MCFM) to enhance the model's ability to extract detailed\ninformation from ultrasound images. MCFM operates exclusively on the lower\nlayers of the neural network, directly processing raw ultrasound data. By\nassigning attention weights to image representations under different contrast\nconditions, the module enhances feature modeling while explicitly maintaining\nminimal parameter overhead. Results: The proposed MCFM was evaluated on a\ncurated dataset of fetal torso plane ultrasound images. Experimental results\ndemonstrate that MCFM substantially improves recognition performance, with a\nminimal increase in model complexity. The integration of multi-contrast\nattention enables the model to better capture subtle anatomical structures,\ncontributing to higher classification accuracy and clinical reliability.\nConclusions: Our method provides an effective solution for improving fetal\ntorso plane recognition in ultrasound imaging. By enhancing feature\nrepresentation through multi-contrast fusion, the proposed approach supports\nclinicians in achieving more accurate and consistent diagnoses, demonstrating\nstrong potential for clinical adoption in prenatal screening. The codes are\navailable at https://github.com/sysll/MCFM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5bf9\u6bd4\u878d\u5408\u6a21\u5757\uff08MCFM\uff09\u4ee5\u63d0\u9ad8\u8d85\u58f0\u56fe\u50cf\u4e2d\u80ce\u513f\u8eaf\u5e72\u5e73\u9762\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u7684\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u7eb9\u7406\u7ec6\u8282\u9650\u5236\u4e86\u80ce\u513f\u8eaf\u5e72\u5e73\u9762\u7684\u7cbe\u7ec6\u8bc6\u522b\uff0c\u5f71\u54cd\u4ea7\u524d\u8bca\u65ad\u7684\u53ef\u9760\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86MCFM\u6a21\u5757\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u5bf9\u6bd4\u6761\u4ef6\u4e0b\u5206\u914d\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u5f00\u9500\u6700\u5c0f\u3002", "result": "\u5728\u80ce\u513f\u8eaf\u5e72\u5e73\u9762\u8d85\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cMCFM\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u589e\u52a0\u6781\u5c0f\u3002", "conclusion": "MCFM\u901a\u8fc7\u591a\u5bf9\u6bd4\u878d\u5408\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u4ea7\u524d\u7b5b\u67e5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u4e00\u81f4\u7684\u8bca\u65ad\u652f\u6301\uff0c\u5177\u6709\u4e34\u5e8a\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2508.09645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09645", "abs": "https://arxiv.org/abs/2508.09645", "authors": ["Zhongyuan Wu", "Chuan-Xian Ren", "Yu Wang", "Xiaohua Ban", "Jianning Xiao", "Xiaohui Duan"], "title": "Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model", "comment": null, "summary": "Parotid gland lesion segmentation is essential for the treatment of parotid\ngland diseases. However, due to the variable size and complex lesion\nboundaries, accurate parotid gland lesion segmentation remains challenging.\nRecently, the Segment Anything Model (SAM) fine-tuning has shown remarkable\nperformance in the field of medical image segmentation. Nevertheless, SAM's\ninteraction segmentation model relies heavily on precise lesion prompts\n(points, boxes, masks, etc.), which are very difficult to obtain in real-world\napplications. Besides, current medical image segmentation methods are\nautomatically generated, ignoring the domain knowledge of medical experts when\nperforming segmentation. To address these limitations, we propose the parotid\ngland segment anything model (PG-SAM), an expert diagnosis text-guided SAM\nincorporating expert domain knowledge for cross-sequence parotid gland lesion\nsegmentation. Specifically, we first propose an expert diagnosis report guided\nprompt generation module that can automatically generate prompt information\ncontaining the prior domain knowledge to guide the subsequent lesion\nsegmentation process. Then, we introduce a cross-sequence attention module,\nwhich integrates the complementary information of different modalities to\nenhance the segmentation effect. Finally, the multi-sequence image features and\ngenerated prompts are feed into the decoder to get segmentation result.\nExperimental results demonstrate that PG-SAM achieves state-of-the-art\nperformance in parotid gland lesion segmentation across three independent\nclinical centers, validating its clinical applicability and the effectiveness\nof diagnostic text for enhancing image segmentation in real-world clinical\nsettings.", "AI": {"tldr": "PG-SAM\u6a21\u578b\u901a\u8fc7\u4e13\u5bb6\u8bca\u65ad\u6587\u672c\u5f15\u5bfc\u7684\u63d0\u793a\u751f\u6210\u548c\u8de8\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u816e\u817a\u75c5\u53d8\u7684\u7cbe\u51c6\u5206\u5272\u3002", "motivation": "\u7531\u4e8e\u816e\u817a\u75c5\u53d8\u5927\u5c0f\u4e0d\u4e00\u4e14\u8fb9\u754c\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u63d0\u793a\u6216\u5ffd\u7565\u4e13\u5bb6\u77e5\u8bc6\uff0cPG-SAM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e13\u5bb6\u8bca\u65ad\u62a5\u544a\u751f\u6210\u63d0\u793a\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8de8\u5e8f\u5217\u6ce8\u610f\u529b\u6a21\u5757\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u6700\u7ec8\u8f93\u5165\u89e3\u7801\u5668\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728\u4e09\u4e2a\u72ec\u7acb\u4e34\u5e8a\u4e2d\u5fc3\u9a8c\u8bc1\u4e2d\uff0cPG-SAM\u8868\u73b0\u6700\u4f18\uff0c\u8bc1\u5b9e\u4e86\u5176\u4e34\u5e8a\u9002\u7528\u6027\u548c\u4e13\u5bb6\u6587\u672c\u7684\u6709\u6548\u6027\u3002", "conclusion": "PG-SAM\u901a\u8fc7\u878d\u5408\u4e13\u5bb6\u77e5\u8bc6\u548c\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u816e\u817a\u75c5\u53d8\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09649", "abs": "https://arxiv.org/abs/2508.09649", "authors": ["Reuben Dorent", "Laura Rigolo", "Colin P. Galvin", "Junyu Chen", "Mattias P. Heinrich", "Aaron Carass", "Olivier Colliot", "Demian Wassermann", "Alexandra Golby", "Tina Kapur", "William Wells"], "title": "The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge", "comment": null, "summary": "Accurate intraoperative image guidance is critical for achieving maximal safe\nresection in brain tumor surgery, yet neuronavigation systems based on\npreoperative MRI lose accuracy during the procedure due to brain shift.\nAligning post-resection intraoperative ultrasound (iUS) with preoperative MRI\ncan restore spatial accuracy by estimating brain shift deformations, but it\nremains a challenging problem given the large anatomical and topological\nchanges and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge\nprovides the largest public benchmark for this task, built upon the ReMIND\ndataset. It offers 99 training cases, 5 validation cases, and 10 private test\ncases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.\nData are provided without annotations for training, while validation and test\nperformance are evaluated on manually annotated anatomical landmarks. Metrics\ninclude target registration error (TRE), robustness to worst-case landmark\nmisalignment (TRE30), and runtime. By establishing a standardized evaluation\nframework for this clinically critical and technically complex problem,\nReMIND2Reg aims to accelerate the development of robust, generalizable, and\nclinically deployable multimodal registration algorithms for image-guided\nneurosurgery.", "AI": {"tldr": "ReMIND2Reg 2025\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u8111\u80bf\u7624\u624b\u672f\u4e2d\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u4ee5\u89e3\u51b3\u8111\u79fb\u4f4d\u5bfc\u81f4\u7684\u5bfc\u822a\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u8111\u80bf\u7624\u624b\u672f\u4e2d\uff0c\u672f\u524dMRI\u5bfc\u822a\u7cfb\u7edf\u56e0\u8111\u79fb\u4f4d\u5931\u53bb\u51c6\u786e\u6027\uff0c\u672f\u4e2d\u8d85\u58f0\u4e0e\u672f\u524dMRI\u914d\u51c6\u53ef\u6062\u590d\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u4f46\u9762\u4e34\u89e3\u5256\u7ed3\u6784\u53d8\u5316\u548c\u6a21\u6001\u5dee\u5f02\u7684\u6311\u6218\u3002", "method": "\u6311\u6218\u8d5b\u63d0\u4f9b99\u4e2a\u8bad\u7ec3\u6848\u4f8b\u30015\u4e2a\u9a8c\u8bc1\u6848\u4f8b\u548c10\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u5305\u542b\u914d\u5bf9\u76843D ceT1 MRI\u3001T2 MRI\u548c\u672f\u540e3D iUS\u6570\u636e\uff0c\u8bc4\u4f30\u57fa\u4e8e\u624b\u52a8\u6807\u6ce8\u7684\u89e3\u5256\u6807\u5fd7\u3002", "result": "\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u76ee\u6807\u914d\u51c6\u8bef\u5dee\uff08TRE\uff09\u3001\u6700\u574f\u60c5\u51b5\u6807\u5fd7\u9519\u4f4d\u9c81\u68d2\u6027\uff08TRE30\uff09\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "ReMIND2Reg\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u5f00\u53d1\u7a33\u5065\u3001\u901a\u7528\u4e14\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684\u591a\u6a21\u6001\u914d\u51c6\u7b97\u6cd5\u3002"}}
{"id": "2508.09650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09650", "abs": "https://arxiv.org/abs/2508.09650", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazely", "Sunil Aryal"], "title": "TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos", "comment": "8 pages, 6 figures,", "summary": "Robust ball tracking under occlusion remains a key challenge in sports video\nanalysis, affecting tasks like event detection and officiating. We present\nTOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,\nvisibility-weighted loss, and occlusion augmentation to improve performance\nunder partial and full occlusions. Developed in collaboration with Paralympics\nAustralia, TOTNet is designed for real-world sports analytics. We introduce\nTTA, a new occlusion-rich table tennis dataset collected from\nprofessional-level Paralympic matches, comprising 9,159 samples with 1,996\nocclusion cases. Evaluated on four datasets across tennis, badminton, and table\ntennis, TOTNet significantly outperforms prior state-of-the-art methods,\nreducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded\nframes from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for\noffline sports analytics in fast-paced scenarios. Code and data\naccess:\\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.", "AI": {"tldr": "TOTNet\u662f\u4e00\u79cd\u7528\u4e8e\u4f53\u80b2\u89c6\u9891\u5206\u6790\u7684\u65f6\u5e8f\u906e\u6321\u8ddf\u8e2a\u7f51\u7edc\uff0c\u901a\u8fc73D\u5377\u79ef\u3001\u53ef\u89c1\u6027\u52a0\u6743\u635f\u5931\u548c\u906e\u6321\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u7403\u4f53\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f53\u80b2\u89c6\u9891\u5206\u6790\u4e2d\uff0c\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u7403\u4f53\u8ddf\u8e2a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u4e8b\u4ef6\u68c0\u6d4b\u548c\u88c1\u5224\u7b49\u4efb\u52a1\u3002", "method": "TOTNet\u91c7\u75283D\u5377\u79ef\u3001\u53ef\u89c1\u6027\u52a0\u6743\u635f\u5931\u548c\u906e\u6321\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u57fa\u4e8e\u65b0\u7684\u906e\u6321\u4e30\u5bcc\u7684\u4e52\u4e53\u7403\u6570\u636e\u96c6TTA\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cTOTNet\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cRMSE\u4ece37.30\u964d\u81f37.19\uff0c\u5168\u906e\u6321\u5e27\u7684\u51c6\u786e\u7387\u4ece0.63\u63d0\u5347\u81f30.80\u3002", "conclusion": "TOTNet\u5728\u5feb\u901f\u573a\u666f\u4e0b\u7684\u79bb\u7ebf\u4f53\u80b2\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.09655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09655", "abs": "https://arxiv.org/abs/2508.09655", "authors": ["Lianfang Wang", "Kuilin Qin", "Xueying Liu", "Huibin Chang", "Yong Wang", "Yuping Duan"], "title": "Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging", "comment": null, "summary": "Computational imaging, especially non-line-of-sight (NLOS) imaging, the\nextraction of information from obscured or hidden scenes is achieved through\nthe utilization of indirect light signals resulting from multiple reflections\nor scattering. The inherently weak nature of these signals, coupled with their\nsusceptibility to noise, necessitates the integration of physical processes to\nensure accurate reconstruction. This paper presents a parameterized inverse\nproblem framework tailored for large-scale linear problems in 3D imaging\nreconstruction. Initially, a noise estimation module is employed to adaptively\nassess the noise levels present in transient data. Subsequently, a\nparameterized neural operator is developed to approximate the inverse mapping,\nfacilitating end-to-end rapid image reconstruction. Our 3D image reconstruction\nframework, grounded in operator learning, is constructed through deep algorithm\nunfolding, which not only provides commendable model interpretability but also\nenables dynamic adaptation to varying noise levels in the acquired data,\nthereby ensuring consistently robust and accurate reconstruction outcomes.\nFurthermore, we introduce a novel method for the fusion of global and local\nspatiotemporal data features. By integrating structural and detailed\ninformation, this method significantly enhances both accuracy and robustness.\nComprehensive numerical experiments conducted on both simulated and real\ndatasets substantiate the efficacy of the proposed method. It demonstrates\nremarkable performance with fast scanning data and sparse illumination point\ndata, offering a viable solution for NLOS imaging in complex scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u9006\u95ee\u9898\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a213D\u6210\u50cf\u91cd\u5efa\uff0c\u7ed3\u5408\u566a\u58f0\u4f30\u8ba1\u6a21\u5757\u548c\u53c2\u6570\u5316\u795e\u7ecf\u7b97\u5b50\uff0c\u5b9e\u73b0\u5feb\u901f\u7aef\u5230\u7aef\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u4e0e\u5c40\u90e8\u65f6\u7a7a\u6570\u636e\u7279\u5f81\u878d\u5408\u63d0\u5347\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u975e\u89c6\u8ddd\uff08NLOS\uff09\u6210\u50cf\u4e2d\uff0c\u95f4\u63a5\u5149\u4fe1\u53f7\u5fae\u5f31\u4e14\u6613\u53d7\u566a\u58f0\u5e72\u6270\uff0c\u9700\u7ed3\u5408\u7269\u7406\u8fc7\u7a0b\u5b9e\u73b0\u51c6\u786e\u91cd\u5efa\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u9006\u95ee\u9898\u6846\u67b6\uff0c\u5148\u901a\u8fc7\u566a\u58f0\u4f30\u8ba1\u6a21\u5757\u81ea\u9002\u5e94\u8bc4\u4f30\u77ac\u6001\u6570\u636e\u566a\u58f0\uff0c\u518d\u5f00\u53d1\u53c2\u6570\u5316\u795e\u7ecf\u7b97\u5b50\u8fd1\u4f3c\u9006\u6620\u5c04\uff0c\u7ed3\u5408\u6df1\u5ea6\u7b97\u6cd5\u5c55\u5f00\u548c\u5168\u5c40\u5c40\u90e8\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5feb\u901f\u626b\u63cf\u548c\u7a00\u758f\u7167\u660e\u70b9\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684NLOS\u6210\u50cf\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u52a8\u6001\u9002\u5e94\u6027\u3002"}}
{"id": "2508.09661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09661", "abs": "https://arxiv.org/abs/2508.09661", "authors": ["Eduarda Caldeira", "Naser Damer", "Fadi Boutros"], "title": "NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation", "comment": "Accepted at ICCV Workshops", "summary": "The use of synthetic data as an alternative to authentic datasets in face\nrecognition (FR) development has gained significant attention, addressing\nprivacy, ethical, and practical concerns associated with collecting and using\nauthentic data. Recent state-of-the-art approaches have proposed\nidentity-conditioned diffusion models to generate identity-consistent face\nimages, facilitating their use in training FR models. However, these methods\noften lack explicit sampling mechanisms to enforce inter-class separability,\nleading to identity overlap in the generated data and, consequently, suboptimal\nFR performance. In this work, we introduce NegFaceDiff, a novel sampling method\nthat incorporates negative conditions into the identity-conditioned diffusion\nprocess. NegFaceDiff enhances identity separation by leveraging negative\nconditions that explicitly guide the model away from unwanted features while\npreserving intra-class consistency. Extensive experiments demonstrate that\nNegFaceDiff significantly improves the identity consistency and separability of\ndata generated by identity-conditioned diffusion models. Specifically, identity\nseparability, measured by the Fisher Discriminant Ratio (FDR), increases from\n2.427 to 5.687. These improvements are reflected in FR systems trained on the\nNegFaceDiff dataset, which outperform models trained on data generated without\nnegative conditions across multiple benchmarks.", "AI": {"tldr": "NegFaceDiff\u662f\u4e00\u79cd\u65b0\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8eab\u4efd\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8d1f\u6761\u4ef6\uff0c\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u53ef\u5206\u79bb\u6027\uff0c\u4ece\u800c\u4f18\u5316\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8eab\u4efd\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u6570\u636e\u65f6\u7f3a\u4e4f\u660e\u786e\u7684\u7c7b\u95f4\u5206\u79bb\u673a\u5236\uff0c\u5bfc\u81f4\u8eab\u4efd\u91cd\u53e0\u548c\u6b21\u4f18\u8bc6\u522b\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faNegFaceDiff\u65b9\u6cd5\uff0c\u5229\u7528\u8d1f\u6761\u4ef6\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u4e0d\u9700\u8981\u7684\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u7c7b\u5185\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNegFaceDiff\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6570\u636e\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u53ef\u5206\u79bb\u6027\uff08FDR\u4ece2.427\u63d0\u5347\u81f35.687\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u65e0\u8d1f\u6761\u4ef6\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "NegFaceDiff\u901a\u8fc7\u8d1f\u6761\u4ef6\u91c7\u6837\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u91cd\u53e0\u95ee\u9898\uff0c\u4e3a\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d28\u7684\u5408\u6210\u6570\u636e\u3002"}}
{"id": "2508.09667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09667", "abs": "https://arxiv.org/abs/2508.09667", "authors": ["Xingyilang Yin", "Qi Zhang", "Jiahao Chang", "Ying Feng", "Qingnan Fan", "Xi Yang", "Chi-Man Pun", "Huaqi Zhang", "Xiaodong Cun"], "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors", "comment": null, "summary": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views\nis an ill-posed problem due to insufficient information, often resulting in\nnoticeable artifacts. While recent approaches have sought to leverage\ngenerative priors to complete information for under-constrained regions, they\nstruggle to generate content that remains consistent with input observations.\nTo address this challenge, we propose GSFixer, a novel framework designed to\nimprove the quality of 3DGS representations reconstructed from sparse inputs.\nThe core of our approach is the reference-guided video restoration model, built\nupon a DiT-based video diffusion model trained on paired artifact 3DGS renders\nand clean frames with additional reference-based conditions. Considering the\ninput sparse views as references, our model integrates both 2D semantic\nfeatures and 3D geometric features of reference views extracted from the visual\ngeometry foundation model, enhancing the semantic coherence and 3D consistency\nwhen fixing artifact novel views. Furthermore, considering the lack of suitable\nbenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which\ncontains artifact frames rendered using low-quality 3DGS. Extensive experiments\ndemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS\nartifact restoration and sparse-view 3D reconstruction. Project page:\nhttps://github.com/GVCLab/GSFixer.", "AI": {"tldr": "GSFixer\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u5f15\u5bfc\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\u63d0\u5347\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u91cd\u5efa3D\u573a\u666f\u65f6\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u660e\u663e\u4f2a\u5f71\uff0c\u73b0\u6709\u751f\u6210\u5148\u9a8c\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u4e0e\u8f93\u5165\u89c2\u5bdf\u7684\u4e00\u81f4\u6027\u3002", "method": "\u57fa\u4e8eDiT\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u54082D\u8bed\u4e49\u548c3D\u51e0\u4f55\u7279\u5f81\uff0c\u5229\u7528\u53c2\u8003\u89c6\u56fe\u4fee\u590d\u4f2a\u5f71\u3002", "result": "GSFixer\u57283DGS\u4f2a\u5f71\u4fee\u590d\u548c\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GSFixer\u901a\u8fc7\u591a\u7279\u5f81\u6574\u5408\u548c\u57fa\u51c6\u6570\u636e\u96c6DL3DV-Res\uff0c\u663e\u8457\u63d0\u5347\u4e863DGS\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2508.09691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09691", "abs": "https://arxiv.org/abs/2508.09691", "authors": ["Yin Xie", "Zhichao Chen", "Xiaoze Yu", "Yongle Zhao", "Xiang An", "Kaicheng Yang", "Zimin Ran", "Jia Guo", "Ziyong Feng", "Jiankang Deng"], "title": "PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training", "comment": null, "summary": "Facial representation pre-training is crucial for tasks like facial\nrecognition, expression analysis, and virtual reality. However, existing\nmethods face three key challenges: (1) failing to capture distinct facial\nfeatures and fine-grained semantics, (2) ignoring the spatial structure\ninherent to facial anatomy, and (3) inefficiently utilizing limited labeled\ndata. To overcome these, we introduce PaCo-FR, an unsupervised framework that\ncombines masked image modeling with patch-pixel alignment. Our approach\nintegrates three innovative components: (1) a structured masking strategy that\npreserves spatial coherence by aligning with semantically meaningful facial\nregions, (2) a novel patch-based codebook that enhances feature discrimination\nwith multiple candidate tokens, and (3) spatial consistency constraints that\npreserve geometric relationships between facial components. PaCo-FR achieves\nstate-of-the-art performance across several facial analysis tasks with just 2\nmillion unlabeled images for pre-training. Our method demonstrates significant\nimprovements, particularly in scenarios with varying poses, occlusions, and\nlighting conditions. We believe this work advances facial representation\nlearning and offers a scalable, efficient solution that reduces reliance on\nexpensive annotated datasets, driving more effective facial analysis systems.", "AI": {"tldr": "PaCo-FR\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u548c\u5757\u50cf\u7d20\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9762\u90e8\u8868\u793a\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u9762\u90e8\u5206\u6790\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u9762\u90e8\u7279\u5f81\u3001\u7a7a\u95f4\u7ed3\u6784\u548c\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PaCo-FR\u91c7\u7528\u7ed3\u6784\u5316\u63a9\u7801\u7b56\u7565\u3001\u57fa\u4e8e\u5757\u7684\u4ee3\u7801\u4e66\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u4e0e\u5757\u50cf\u7d20\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700200\u4e07\u672a\u6807\u6ce8\u56fe\u50cf\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u59ff\u6001\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u7b49\u573a\u666f\u4e0b\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PaCo-FR\u63a8\u52a8\u4e86\u9762\u90e8\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.09699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09699", "abs": "https://arxiv.org/abs/2508.09699", "authors": ["Javier Rodenas", "Eduardo Aguilar", "Petia Radeva"], "title": "Slot Attention-based Feature Filtering for Few-Shot Learning", "comment": "CVPR Workshop LatinX 2025", "summary": "Irrelevant features can significantly degrade few-shot learn ing performance.\nThis problem is used to match queries and support images based on meaningful\nsimilarities despite the limited data. However, in this process, non-relevant\nfea tures such as background elements can easily lead to confu sion and\nmisclassification. To address this issue, we pro pose Slot Attention-based\nFeature Filtering for Few-Shot Learning (SAFF) that leverages slot attention\nmechanisms to discriminate and filter weak features, thereby improving few-shot\nclassification performance. The key innovation of SAFF lies in its integration\nof slot attention with patch em beddings, unifying class-aware slots into a\nsingle attention mechanism to filter irrelevant features effectively. We intro\nduce a similarity matrix that computes across support and query images to\nquantify the relevance of filtered embed dings for classification. Through\nexperiments, we demon strate that Slot Attention performs better than other\natten tion mechanisms, capturing discriminative features while reducing\nirrelevant information. We validate our approach through extensive experiments\non few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma\ngeNet, outperforming several state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSlot Attention\u7684\u7279\u5f81\u8fc7\u6ee4\u65b9\u6cd5\uff08SAFF\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u5c0f\u6837\u672c\u5b66\u4e60\u6027\u80fd\uff0c\u901a\u8fc7\u8fc7\u6ee4\u65e0\u5173\u7279\u5f81\u51cf\u5c11\u8bef\u5206\u7c7b\u3002", "motivation": "\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u65e0\u5173\u7279\u5f81\uff08\u5982\u80cc\u666f\u5143\u7d20\uff09\u5bb9\u6613\u5bfc\u81f4\u6df7\u6dc6\u548c\u8bef\u5206\u7c7b\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u7ed3\u5408Slot Attention\u673a\u5236\u4e0epatch embeddings\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u77e9\u9635\u91cf\u5316\u8fc7\u6ee4\u540e\u5d4c\u5165\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAFF\u5728\u591a\u4e2a\u5c0f\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAFF\u901a\u8fc7\u6709\u6548\u8fc7\u6ee4\u65e0\u5173\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.09709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09709", "abs": "https://arxiv.org/abs/2508.09709", "authors": ["Qianru Qiu", "Jiafeng Mao", "Kento Masui", "Xueting Wang"], "title": "MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers", "comment": "Codes and benchmarks will be released soon", "summary": "Recent advances in diffusion models have significantly improved the\nperformance of reference-guided line art colorization. However, existing\nmethods still struggle with region-level color consistency, especially when the\nreference and target images differ in character pose or motion. Instead of\nrelying on external matching annotations between the reference and target, we\npropose to discover semantic correspondences implicitly through internal\nattention mechanisms. In this paper, we present MangaDiT, a powerful model for\nreference-guided line art colorization based on Diffusion Transformers (DiT).\nOur model takes both line art and reference images as conditional inputs and\nintroduces a hierarchical attention mechanism with a dynamic attention\nweighting strategy. This mechanism augments the vanilla attention with an\nadditional context-aware path that leverages pooled spatial features,\neffectively expanding the model's receptive field and enhancing region-level\ncolor alignment. Experiments on two benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches, achieving\nsuperior performance in both qualitative and quantitative evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMangaDiT\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u53c2\u8003\u5f15\u5bfc\u7ebf\u7a3f\u4e0a\u8272\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u533a\u57df\u7ea7\u989c\u8272\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53c2\u8003\u56fe\u50cf\u4e0e\u76ee\u6807\u56fe\u50cf\u59ff\u6001\u6216\u52a8\u4f5c\u5dee\u5f02\u5927\u65f6\uff0c\u533a\u57df\u7ea7\u989c\u8272\u4e00\u81f4\u6027\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u5916\u90e8\u5339\u914d\u6807\u6ce8\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u6ce8\u610f\u529b\u6743\u91cd\u7b56\u7565\uff0c\u589e\u5f3a\u6a21\u578b\u611f\u53d7\u91ce\u548c\u533a\u57df\u7ea7\u989c\u8272\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MangaDiT\u901a\u8fc7\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u53c2\u8003\u5f15\u5bfc\u7ebf\u7a3f\u4e0a\u8272\u7684\u533a\u57df\u7ea7\u989c\u8272\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.09715", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09715", "abs": "https://arxiv.org/abs/2508.09715", "authors": ["Devvrat Joshi", "Islem Rekik"], "title": "NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation", "comment": null, "summary": "The rapid growth of multimodal medical imaging data presents significant\nstorage and transmission challenges, particularly in resource-constrained\nclinical settings. We propose NEURAL, a novel framework that addresses this by\nusing semantics-guided data compression. Our approach repurposes\ncross-attention scores between the image and its radiological report from a\nfine-tuned generative vision-language model to structurally prune chest X-rays,\npreserving only diagnostically critical regions. This process transforms the\nimage into a highly compressed, graph representation. This unified graph-based\nrepresentation fuses the pruned visual graph with a knowledge graph derived\nfrom the clinical report, creating a universal data structure that simplifies\ndownstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for\npneumonia detection, NEURAL achieves a 93.4-97.7\\% reduction in image data size\nwhile maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming\nother baseline models that use uncompressed data. By creating a persistent,\ntask-agnostic data asset, NEURAL resolves the trade-off between data size and\nclinical utility, enabling efficient workflows and teleradiology without\nsacrificing performance. Our NEURAL code is available at\nhttps://github.com/basiralab/NEURAL.", "AI": {"tldr": "NEURAL\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u6570\u636e\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u4e34\u5e8a\u73af\u5883\u4e2d\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6311\u6218\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6ce8\u610f\u529b\u5206\u6570\u5bf9\u80f8\u90e8X\u5149\u8fdb\u884c\u7ed3\u6784\u4fee\u526a\uff0c\u751f\u6210\u9ad8\u5ea6\u538b\u7f29\u7684\u56fe\u8868\u793a\uff0c\u5e76\u4e0e\u4e34\u5e8a\u62a5\u544a\u7684\u77e5\u8bc6\u56fe\u878d\u5408\u3002", "result": "\u5728\u80ba\u708e\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cNEURAL\u5c06\u56fe\u50cf\u6570\u636e\u5927\u5c0f\u51cf\u5c1193.4-97.7%\uff0c\u540c\u65f6\u4fdd\u63010.88-0.95 AUC\u7684\u9ad8\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "NEURAL\u901a\u8fc7\u521b\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5927\u5c0f\u4e0e\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u652f\u6301\u9ad8\u6548\u5de5\u4f5c\u6d41\u7a0b\u548c\u8fdc\u7a0b\u653e\u5c04\u5b66\u3002"}}
{"id": "2508.09717", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09717", "abs": "https://arxiv.org/abs/2508.09717", "authors": ["Shekhnaz Idrissova", "Islem Rekik"], "title": "Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction", "comment": null, "summary": "Glioblastoma is a highly invasive brain tumor with rapid progression rates.\nRecent studies have shown that glioblastoma molecular subtype classification\nserves as a significant biomarker for effective targeted therapy selection.\nHowever, this classification currently requires invasive tissue extraction for\ncomprehensive histopathological analysis. Existing multimodal approaches\ncombining MRI and histopathology images are limited and lack robust mechanisms\nfor preserving shared structural information across modalities. In particular,\ngraph-based models often fail to retain discriminative features within\nheterogeneous graphs, and structural reconstruction mechanisms for handling\nmissing or incomplete modality data are largely underexplored. To address these\nlimitations, we propose a novel sheaf-based framework for structure-aware and\nconsistent fusion of MRI and histopathology data. Our model outperforms\nbaseline methods and demonstrates robustness in incomplete or missing data\nscenarios, contributing to the development of virtual biopsy tools for rapid\ndiagnostics. Our source code is available at\nhttps://github.com/basiralab/MMSN/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esheaf\u7684\u6846\u67b6\uff0c\u7528\u4e8eMRI\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u7684\u7ed3\u6784\u611f\u77e5\u548c\u4e00\u81f4\u6027\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u8de8\u6a21\u6001\u5171\u4eab\u7ed3\u6784\u4fe1\u606f\u548c\u5904\u7406\u7f3a\u5931\u6570\u636e\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u5206\u5b50\u4e9a\u578b\u5206\u7c7b\u9700\u8981\u4fb5\u5165\u6027\u7ec4\u7ec7\u63d0\u53d6\uff0c\u73b0\u6709\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u4fdd\u7559\u5171\u4eab\u7ed3\u6784\u4fe1\u606f\u548c\u5904\u7406\u7f3a\u5931\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esheaf\u7684\u6846\u67b6\uff0c\u7528\u4e8eMRI\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u7684\u7ed3\u6784\u611f\u77e5\u548c\u4e00\u81f4\u6027\u878d\u5408\u3002", "result": "\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u5b8c\u6574\u6216\u7f3a\u5931\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5feb\u901f\u8bca\u65ad\u7684\u865a\u62df\u6d3b\u68c0\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.09736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09736", "abs": "https://arxiv.org/abs/2508.09736", "authors": ["Lin Long", "Yichen He", "Wentao Ye", "Yiyuan Pan", "Yuan Lin", "Hang Li", "Junbo Zhao", "Wei Li"], "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory", "comment": null, "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent", "AI": {"tldr": "M3-Agent\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u4ee3\u7406\u6846\u67b6\uff0c\u5177\u5907\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u5b9e\u65f6\u89c6\u89c9\u548c\u542c\u89c9\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5177\u5907\u4eba\u7c7b\u7c7b\u4f3c\u957f\u671f\u8bb0\u5fc6\u7684\u591a\u6a21\u6001\u4ee3\u7406\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "M3-Agent\u91c7\u7528\u5b9e\u4f53\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u8bb0\u5fc6\u683c\u5f0f\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7M3-Bench\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "M3-Agent\u5728M3-Bench-robot\u3001M3-Bench-web\u548cVideoMME-long\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa6.7%\u30017.7%\u548c5.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "M3-Agent\u5728\u591a\u6a21\u6001\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2508.09746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09746", "abs": "https://arxiv.org/abs/2508.09746", "authors": ["Zhiqiu Zhang", "Dongqi Fan", "Mingjie Wang", "Qiang Tang", "Jian Yang", "Zili Yi"], "title": "Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection", "comment": null, "summary": "The goal of image harmonization is to adjust the foreground in a composite\nimage to achieve visual consistency with the background. Recently, latent\ndiffusion model (LDM) are applied for harmonization, achieving remarkable\nresults. However, LDM-based harmonization faces challenges in detail\npreservation and limited harmonization ability. Additionally, current synthetic\ndatasets rely on color transfer, which lacks local variations and fails to\ncapture complex real-world lighting conditions. To enhance harmonization\ncapabilities, we propose the Region-to-Region transformation. By injecting\ninformation from appropriate regions into the foreground, this approach\npreserves original details while achieving image harmonization or, conversely,\ngenerating new composite data. From this perspective, We propose a novel model\nR2R. Specifically, we design Clear-VAE to preserve high-frequency details in\nthe foreground using Adaptive Filter while eliminating disharmonious elements.\nTo further enhance harmonization, we introduce the Harmony Controller with\nMask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the\nforeground based on the channel importance of both foreground and background\nregions. To address the limitation of existing datasets, we propose Random\nPoisson Blending, which transfers color and lighting information from a\nsuitable region to the foreground, thereby generating more diverse and\nchallenging synthetic images. Using this method, we construct a new synthetic\ndataset, RPHarmony. Experiments demonstrate the superiority of our method over\nother methods in both quantitative metrics and visual harmony. Moreover, our\ndataset helps the model generate more realistic images in real examples. Our\ncode, dataset, and model weights have all been released for open access.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u5230\u533a\u57df\u53d8\u6362\uff08R2R\uff09\u7684\u56fe\u50cf\u534f\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7Clear-VAE\u548cHarmony Controller\u63d0\u5347\u7ec6\u8282\u4fdd\u7559\u4e0e\u534f\u8c03\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6RPHarmony\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLDM\u7684\u534f\u8c03\u65b9\u6cd5\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u534f\u8c03\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5408\u6210\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u5149\u7167\u53d8\u5316\u3002", "method": "\u63d0\u51faR2R\u6a21\u578b\uff0c\u7ed3\u5408Clear-VAE\u548cMACA\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u524d\u666f\uff1b\u4f7f\u7528Random Poisson Blending\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u534f\u8c03\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u65b0\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "R2R\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u534f\u8c03\u6548\u679c\uff0c\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002"}}
{"id": "2508.09779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09779", "abs": "https://arxiv.org/abs/2508.09779", "authors": ["Dianyi Wang", "Siyuan Wang", "Zejun Li", "Yikun Wang", "Yitong Li", "Duyu Tang", "Xiaoyu Shen", "Xuanjing Huang", "Zhongyu Wei"], "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08MoIIE\uff09\u7528\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\uff0c\u901a\u8fc7\u6a21\u6001\u5f15\u5bfc\u7684\u8def\u7531\u673a\u5236\u540c\u65f6\u5b66\u4e60\u6a21\u6001\u5185\u7279\u5f81\u548c\u8de8\u6a21\u6001\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6LVLM\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22\u7a00\u758fMoE\u67b6\u6784\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMoIIE\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u6001\u5f15\u5bfc\u7684\u8def\u7531\u673a\u5236\u5206\u914d\u4e13\u5bb6\uff0c\u5e76\u5f15\u5165\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u57285.5B\u548c11.3B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0cMoIIE\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90MoE\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "MoIIE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.09780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09780", "abs": "https://arxiv.org/abs/2508.09780", "authors": ["Nahyuk Lee", "Juhong Min", "Junhong Lee", "Chunghyun Park", "Minsu Cho"], "title": "Combinative Matching for Geometric Shape Assembly", "comment": "Accepted to ICCV 2025 (Highlight)", "summary": "This paper introduces a new shape-matching methodology, combinative matching,\nto combine interlocking parts for geometric shape assembly. Previous methods\nfor geometric assembly typically rely on aligning parts by finding identical\nsurfaces between the parts as in conventional shape matching and registration.\nIn contrast, we explicitly model two distinct properties of interlocking\nshapes: 'identical surface shape' and 'opposite volume occupancy.' Our method\nthus learns to establish correspondences across regions where their surface\nshapes appear identical but their volumes occupy the inverted space to each\nother. To facilitate this process, we also learn to align regions in rotation\nby estimating their shape orientations via equivariant neural networks. The\nproposed approach significantly reduces local ambiguities in matching and\nallows a robust combination of parts in assembly. Experimental results on\ngeometric assembly benchmarks demonstrate the efficacy of our method,\nconsistently outperforming the state of the art. Project page:\nhttps://nahyuklee.github.io/cmnet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u72b6\u5339\u914d\u65b9\u6cd5\u201c\u7ec4\u5408\u5339\u914d\u201d\uff0c\u7528\u4e8e\u51e0\u4f55\u5f62\u72b6\u7ec4\u88c5\uff0c\u901a\u8fc7\u5efa\u6a21\u201c\u76f8\u540c\u8868\u9762\u5f62\u72b6\u201d\u548c\u201c\u76f8\u53cd\u4f53\u79ef\u5360\u7528\u201d\u4e24\u4e2a\u7279\u6027\uff0c\u663e\u8457\u51cf\u5c11\u5339\u914d\u4e2d\u7684\u5c40\u90e8\u6a21\u7cca\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u9f50\u76f8\u540c\u8868\u9762\uff0c\u800c\u672c\u6587\u660e\u786e\u5efa\u6a21\u4e92\u9501\u5f62\u72b6\u7684\u4e24\u4e2a\u7279\u6027\uff0c\u4ee5\u66f4\u9c81\u68d2\u5730\u7ec4\u5408\u96f6\u4ef6\u3002", "method": "\u901a\u8fc7\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5bf9\u9f50\u65cb\u8f6c\u533a\u57df\uff0c\u5efa\u7acb\u8868\u9762\u5f62\u72b6\u76f8\u540c\u4f46\u4f53\u79ef\u5360\u7528\u76f8\u53cd\u533a\u57df\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u51e0\u4f55\u7ec4\u88c5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u7ec4\u5408\u5339\u914d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f62\u72b6\u7ec4\u88c5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09785", "abs": "https://arxiv.org/abs/2508.09785", "authors": ["Linpu He", "Yanan Li", "Bingze Li", "Elvis Han Cui", "Donghui Wang"], "title": "DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning", "comment": "Accepted to ACMMM 2025", "summary": "Learning from large-scale pre-trained models with strong generalization\nability has shown remarkable success in a wide range of downstream tasks\nrecently, but it is still underexplored in the challenging few-shot\nclass-incremental learning (FSCIL) task. It aims to continually learn new\nconcepts from limited training samples without forgetting the old ones at the\nsame time. In this paper, we introduce DSS-Prompt, a simple yet effective\napproach that transforms the pre-trained Vision Transformer with minimal\nmodifications in the way of prompts into a strong FSCIL classifier. Concretely,\nwe synergistically utilize two complementary types of prompts in each\nTransformer block: static prompts to bridge the domain gap between the\npre-training and downstream datasets, thus enabling better adaption; and\ndynamic prompts to capture instance-aware semantics, thus enabling easy\ntransfer from base to novel classes. Specially, to generate dynamic prompts, we\nleverage a pre-trained multi-modal model to extract input-related diverse\nsemantics, thereby generating complementary input-aware prompts, and then\nadaptively adjust their importance across different layers. In this way, on top\nof the prompted visual embeddings, a simple prototype classifier can beat\nstate-of-the-arts without further training on the incremental tasks. We conduct\nextensive experiments on four benchmarks to validate the effectiveness of our\nDSS-Prompt and show that it consistently achieves better performance than\nexisting approaches on all datasets and can alleviate the catastrophic\nforgetting issue as well.", "AI": {"tldr": "DSS-Prompt\u5229\u7528\u9759\u6001\u548c\u52a8\u6001\u63d0\u793a\u6539\u8fdb\u9884\u8bad\u7ec3Vision Transformer\uff0c\u5728\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u7ed3\u5408\u9759\u6001\u63d0\u793a\uff08\u9002\u5e94\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u5dee\u5f02\uff09\u548c\u52a8\u6001\u63d0\u793a\uff08\u6355\u83b7\u5b9e\u4f8b\u8bed\u4e49\uff09\uff0c\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u8f93\u5165\u611f\u77e5\u63d0\u793a\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "DSS-Prompt\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684FSCIL\u65b9\u6cd5\uff0c\u65e0\u9700\u589e\u91cf\u4efb\u52a1\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.09796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09796", "abs": "https://arxiv.org/abs/2508.09796", "authors": ["Yingjie Wang", "Zhixing Wang", "Le Zheng", "Tianxiao Liu", "Roujing Li", "Xueyao Hu"], "title": "MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking", "comment": null, "summary": "Multi-object tracking (MOT) in human-dominant scenarios, which involves\ncontinuously tracking multiple people within video sequences, remains a\nsignificant challenge in computer vision due to targets' complex motion and\nsevere occlusions. Conventional tracking-by-detection methods are fundamentally\nlimited by their reliance on Kalman filter (KF) and rigid Intersection over\nUnion (IoU)-based association. The motion model in KF often mismatches\nreal-world object dynamics, causing filtering errors, while rigid association\nstruggles under occlusions, leading to identity switches or target loss. To\naddress these issues, we propose MeMoSORT, a simple, online, and real-time MOT\ntracker with two key innovations. First, the Memory-assisted Kalman filter\n(MeKF) uses memory-augmented neural networks to compensate for mismatches\nbetween assumed and actual object motion. Second, the Motion-adaptive IoU\n(Mo-IoU) adaptively expands the matching space and incorporates height\nsimilarity to reduce the influence of detection errors and association\nfailures, while remaining lightweight. Experiments on DanceTrack and SportsMOT\nshow that MeMoSORT achieves state-of-the-art performance, with HOTA scores of\n67.9\\% and 82.1\\%, respectively.", "AI": {"tldr": "MeMoSORT\u662f\u4e00\u79cd\u5b9e\u65f6\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u81ea\u9002\u5e94IoU\u5339\u914d\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u8fd0\u52a8\u548c\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8ddf\u8e2a\u65b9\u6cd5\u56e0\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u521a\u6027IoU\u5339\u914d\u7684\u5c40\u9650\u6027\uff0c\u5728\u590d\u6742\u8fd0\u52a8\u548c\u906e\u6321\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMeKF\uff08\u8bb0\u5fc6\u8f85\u52a9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u548cMo-IoU\uff08\u8fd0\u52a8\u81ea\u9002\u5e94IoU\uff09\uff0c\u5206\u522b\u4f18\u5316\u8fd0\u52a8\u6a21\u578b\u548c\u5339\u914d\u7b56\u7565\u3002", "result": "\u5728DanceTrack\u548cSportsMOT\u6570\u636e\u96c6\u4e0a\uff0cHOTA\u5206\u6570\u5206\u522b\u8fbe\u523067.9%\u548c82.1%\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MeMoSORT\u901a\u8fc7\u8f7b\u91cf\u7ea7\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u5b9e\u65f6\u591a\u76ee\u6807\u8ddf\u8e2a\u3002"}}
{"id": "2508.09802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09802", "abs": "https://arxiv.org/abs/2508.09802", "authors": ["Xin Du", "Maoyuan Xu", "Zhi Ying"], "title": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention", "comment": null, "summary": "Physically Based Rendering (PBR) materials are typically characterized by\nmultiple 2D texture maps such as basecolor, normal, metallic, and roughness\nwhich encode spatially-varying bi-directional reflectance distribution function\n(SVBRDF) parameters to model surface reflectance properties and microfacet\ninteractions. Upscaling SVBRDF material is valuable for modern 3D graphics\napplications. However, existing Single Image Super-Resolution (SISR) methods\nstruggle with cross-map inconsistency, inadequate modeling of modality-specific\nfeatures, and limited generalization due to data distribution shifts. In this\nwork, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention\n(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based\nSISR models for PBR material super-resolution. MUJICA is seamlessly attached\nafter the pre-trained and frozen SISR backbone. It leverages cross-map\nattention to fuse features while preserving remarkable reconstruction ability\nof the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and\nHMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map\nconsistency. Experiments demonstrate that MUJICA enables efficient training\neven with limited resources and delivers state-of-the-art performance on PBR\nmaterial datasets.", "AI": {"tldr": "MUJICA\u662f\u4e00\u79cd\u57fa\u4e8e\u8de8\u56fe\u6ce8\u610f\u529b\u673a\u5236\u7684\u9002\u914d\u5668\uff0c\u7528\u4e8e\u63d0\u5347PBR\u6750\u8d28\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u8de8\u56fe\u4e00\u81f4\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728PBR\u6750\u8d28\u4e0a\u5b58\u5728\u8de8\u56fe\u4e0d\u4e00\u81f4\u3001\u6a21\u6001\u7279\u5f81\u5efa\u6a21\u4e0d\u8db3\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "MUJICA\u901a\u8fc7\u8de8\u56fe\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u9884\u8bad\u7ec3\u7684Swin-transformer SISR\u6a21\u578b\uff0c\u4fdd\u7559\u5176\u91cd\u5efa\u80fd\u529b\u5e76\u63d0\u5347\u6027\u80fd\u3002", "result": "MUJICA\u5728PSNR\u3001SSIM\u548cLPIPS\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u9ad8\u6548\u3002", "conclusion": "MUJICA\u5728PBR\u6750\u8d28\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5148\u8fdb\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.09805", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09805", "abs": "https://arxiv.org/abs/2508.09805", "authors": ["Jonathan Williams Ramirez", "Dina Zemlyanker", "Lucas Deden-Binder", "Rogeny Herisse", "Erendira Garcia Pallares", "Karthik Gopinath", "Harshvardhan Gazula", "Christopher Mount", "Liana N. Kozanno", "Michael S. Marshall", "Theresa R. Connors", "Matthew P. Frosch", "Mark Montine", "Derek H. Oakley", "Christine L. Mac Donald", "C. Dirk Keene", "Bradley T. Hyman", "Juan Eugenio Iglesias"], "title": "Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology", "comment": "19 pages, 10 figures", "summary": "Advances in image registration and machine learning have recently enabled\nvolumetric analysis of \\emph{postmortem} brain tissue from conventional\nphotographs of coronal slabs, which are routinely collected in brain banks and\nneuropathology laboratories worldwide. One caveat of this methodology is the\nrequirement of segmentation of the tissue from photographs, which currently\nrequires costly manual intervention. In this article, we present a deep\nlearning model to automate this process. The automatic segmentation tool relies\non a U-Net architecture that was trained with a combination of\n\\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,\nfrom specimens with varying diagnoses, photographed at two different sites; and\n\\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding\nmasks generated from MRI scans for improved generalizability to unseen\nphotographic setups. Automated model predictions on a subset of photographs not\nseen in training were analyzed to estimate performance compared to manual\nlabels -- including both inter- and intra-rater variability. Our model achieved\na median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\\%\nHausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.\nOur tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5206\u5272\u8111\u7ec4\u7ec7\u7167\u7247\u4e2d\u7684\u7ec4\u7ec7\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u5e72\u9884\u8fdb\u884c\u8111\u7ec4\u7ec7\u7167\u7247\u7684\u5206\u5272\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528U-Net\u67b6\u6784\uff0c\u7ed3\u54081,414\u5f20\u624b\u52a8\u6807\u6ce8\u7684\u56fe\u50cf\u548c2,000\u5f20\u5408\u6210\u7684MRI\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u7167\u7247\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u5206\u6570\u4e2d\u4f4d\u6570\u8d85\u8fc70.98\uff0c\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u7684\u53d8\u5f02\u6027\u6c34\u5e73\u3002", "conclusion": "\u8be5\u5de5\u5177\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u8111\u7ec4\u7ec7\u7167\u7247\u7684\u81ea\u52a8\u5316\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09812", "abs": "https://arxiv.org/abs/2508.09812", "authors": ["Aryan Pandhi", "Shrey Baid", "Sanjali Jha"], "title": "Poaching Hotspot Identification Using Satellite Imagery", "comment": null, "summary": "Elephant Poaching in African countries has been a decade-old problem. So much\nso that African Forest Elephants are now listed as an endangered species, and\nAfrican Savannah Elephants as critically endangered by the IUCN (International\nUnion for Conservation of Nature). [1] Elephants are hunted primarily for their\nivory tusks which caused many elephants to be born tuskless as a genetic\nmodification for survival. [2] Data gathered by recent studies shows that\nthough poaching methods remain the same, the poaching grounds are rather\ndynamic. Poachers have shifted to areas with less ranger patrols and several\nother factors like watering holes, seasons, altitude etc. cause constant shifts\nin poaching hotspot locations. [3] After a period of low poaching from\n2000-2014, poaching numbers in African countries are now on the rise again --\nWWF (World Wildlife Foundation) says there are 20,000 elephants poached\nannually [4]. In African countries, anti-poaching efforts are concentrated near\ntowns, while a majority of poaching occurs in the deserted regions. All of\nthese factors result in the need for a Computer Vision Model to identify\npoaching hotspots through locating the geographic indicators of favorable\npoaching regions. A CV model eliminates the need to manually track poachers and\naccount for the environmental factors to deploy resources and its combination\nwith satellite imagery allows us to survey large areas without disturbing local\nspecies or cross border aviation restrictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u901a\u8fc7\u5730\u7406\u6307\u6807\u8bc6\u522b\u975e\u6d32\u5927\u8c61\u5077\u730e\u70ed\u70b9\u533a\u57df\uff0c\u4ee5\u5e94\u5bf9\u5077\u730e\u6d3b\u52a8\u7684\u52a8\u6001\u53d8\u5316\u548c\u8d44\u6e90\u5206\u914d\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u975e\u6d32\u5927\u8c61\u5077\u730e\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u5077\u730e\u5730\u70b9\u52a8\u6001\u53d8\u5316\u4e14\u53cd\u5077\u730e\u8d44\u6e90\u5206\u914d\u4e0d\u5747\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5206\u6790\u536b\u661f\u56fe\u50cf\uff0c\u8bc6\u522b\u5077\u730e\u70ed\u70b9\u533a\u57df\u7684\u5730\u7406\u6307\u6807\uff0c\u5982\u5de1\u903b\u76f2\u533a\u3001\u6c34\u6e90\u7b49\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u76d1\u6d4b\u5927\u9762\u79ef\u533a\u57df\uff0c\u907f\u514d\u5e72\u6270\u5f53\u5730\u7269\u79cd\u6216\u8de8\u5883\u822a\u7a7a\u9650\u5236\u3002", "conclusion": "\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4e3a\u52a8\u6001\u5077\u730e\u70ed\u70b9\u8bc6\u522b\u548c\u8d44\u6e90\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09814", "abs": "https://arxiv.org/abs/2508.09814", "authors": ["Pablo Hern\u00e1ndez-C\u00e1mara", "Jose Manuel Ja\u00e9n-Lorites", "Jorge Vila-Tom\u00e1s", "Jesus Malo", "Valero Laparra"], "title": "Evolution of Low-Level and Texture Human-CLIP Alignment", "comment": null, "summary": "During the training of multi-modal models like CLIP, we observed an\nintriguing phenomenon: the correlation with low-level human image quality\nassessments peaks in the early epochs before gradually declining. This study\ninvestigates this observation and seeks to understand its causes through two\nkey factors: shape-texture bias alignment and classification accuracy drop\nunder noise. Our findings suggest that CLIP initially learn low-level visual\nfeatures, enhancing its alignment with low-level human perception but also\nincreasing its sensitivity to noise and its texture bias. As training\nprogresses, the model shifts toward more abstract shape-based representations,\nimproving noise robustness but reducing alignment with low-level human\nperception. These results suggest that these factors shared an underlying\nlearning mechanism and provide new insights into optimizing the trade-off\nbetween perceptual alignment and robustness in vision-language models.", "AI": {"tldr": "CLIP\u6a21\u578b\u8bad\u7ec3\u521d\u671f\u4e0e\u4eba\u7c7b\u4f4e\u5c42\u6b21\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u76f8\u5173\u6027\u9ad8\uff0c\u540e\u671f\u4e0b\u964d\uff0c\u7814\u7a76\u53d1\u73b0\u4e0e\u5f62\u72b6-\u7eb9\u7406\u504f\u5dee\u548c\u566a\u58f0\u4e0b\u5206\u7c7b\u51c6\u786e\u6027\u6709\u5173\u3002", "motivation": "\u63a2\u7a76CLIP\u6a21\u578b\u8bad\u7ec3\u4e2d\u4e0e\u4eba\u7c7b\u4f4e\u5c42\u6b21\u611f\u77e5\u76f8\u5173\u6027\u5148\u5347\u540e\u964d\u7684\u73b0\u8c61\u53ca\u5176\u539f\u56e0\u3002", "method": "\u5206\u6790\u5f62\u72b6-\u7eb9\u7406\u504f\u5dee\u548c\u566a\u58f0\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "CLIP\u521d\u671f\u5b66\u4e60\u4f4e\u5c42\u6b21\u7279\u5f81\uff0c\u540e\u671f\u8f6c\u5411\u62bd\u8c61\u5f62\u72b6\u8868\u793a\uff0c\u5f71\u54cd\u611f\u77e5\u5bf9\u9f50\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u4e60\u673a\u5236\uff0c\u4e3a\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u5bf9\u9f50\u4e0e\u9c81\u68d2\u6027\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09818", "abs": "https://arxiv.org/abs/2508.09818", "authors": ["Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Abir Ahmed", "Liew Tze Hui"], "title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video", "comment": "Accepted in ICCVDM '25", "summary": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.", "AI": {"tldr": "ViMoNet\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u548c\u8fd0\u52a8\u6570\u636e\uff0c\u63d0\u5347\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u7406\u89e3\u548c\u63a8\u65ad\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4ec5\u5173\u6ce8\u5355\u4e00\u6570\u636e\u7c7b\u578b\uff08\u8fd0\u52a8\u6216\u89c6\u9891\uff09\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51faViMoNet\u6846\u67b6\uff0c\u91c7\u7528\u8054\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u7cbe\u786e\u7684\u8fd0\u52a8-\u6587\u672c\u6570\u636e\u548c\u901a\u7528\u7684\u89c6\u9891-\u6587\u672c\u6570\u636e\u3002", "result": "ViMoNet\u5728\u5b57\u5e55\u751f\u6210\u3001\u8fd0\u52a8\u7406\u89e3\u548c\u884c\u4e3a\u89e3\u91ca\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ViMoNet\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2508.09822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09822", "abs": "https://arxiv.org/abs/2508.09822", "authors": ["Zijian Song", "Sihan Qin", "Tianshui Chen", "Liang Lin", "Guangrun Wang"], "title": "Physical Autoregressive Model for Robotic Manipulation without Action Pretraining", "comment": "16 pages, 6 figures", "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.", "AI": {"tldr": "PAR\u5229\u7528\u89c6\u9891\u9884\u8bad\u7ec3\u4e2d\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u901a\u8fc7\u7269\u7406\u6807\u8bb0\u7ed3\u5408\u5e27\u548c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u9ad8\u6548\u9884\u6d4b\u548c\u52a8\u4f5c\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\uff0c\u4fc3\u4f7f\u5229\u7528\u5176\u4ed6\u6a21\u6001\u7684\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u3002", "method": "\u63d0\u51faPAR\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528\u7269\u7406\u6807\u8bb0\u8868\u793a\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u8054\u5408\u6f14\u5316\uff0c\u91c7\u7528DiT-based\u53bb\u6807\u8bb0\u5668\u548c\u56e0\u679c\u63a9\u7801\u7b49\u6280\u672f\u3002", "result": "\u5728ManiSkill\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPushCube\u4efb\u52a1\u6210\u529f\u7387\u8fbe100%\uff0c\u5176\u4ed6\u4efb\u52a1\u4e0e\u52a8\u4f5c\u9884\u8bad\u7ec3\u57fa\u7ebf\u76f8\u5f53\uff0c\u89c6\u9891\u9884\u6d4b\u51c6\u786e\u3002", "conclusion": "PAR\u5c55\u793a\u4e86\u901a\u8fc7\u89c6\u9891\u9884\u8bad\u7ec3\u8fc1\u79fb\u4e16\u754c\u77e5\u8bc6\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09823", "abs": "https://arxiv.org/abs/2508.09823", "authors": ["Valentin Boussot", "Jean-Louis Dillenseger"], "title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging", "comment": "https://github.com/vboussot/KonfAI", "summary": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at\n\\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.", "AI": {"tldr": "KonfAI\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7YAML\u914d\u7f6e\u6587\u4ef6\u5b9e\u73b0\u5de5\u4f5c\u6d41\u5b9a\u4e49\uff0c\u63d0\u5347\u53ef\u91cd\u590d\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u8bbe\u8ba1KonfAI\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u7b80\u5316\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u914d\u7f6e\u51cf\u5c11\u4ee3\u7801\u4fee\u6539\u9700\u6c42\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u7b56\u7565\u548c\u590d\u6742\u8bad\u7ec3\u8bbe\u7f6e\u3002", "method": "KonfAI\u91c7\u7528\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u652f\u6301\u901a\u8fc7YAML\u6587\u4ef6\u5b9a\u4e49\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u9ad8\u7ea7\u529f\u80fd\u5982\u57fa\u4e8e\u8865\u4e01\u7684\u5b66\u4e60\u3001\u6d4b\u8bd5\u65f6\u589e\u5f3a\u548c\u6a21\u578b\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5206\u5272\u3001\u914d\u51c6\u548c\u56fe\u50cf\u5408\u6210\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u56fd\u9645\u533b\u5b66\u5f71\u50cf\u6311\u6218\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\u3002", "conclusion": "KonfAI\u901a\u8fc7\u5176\u7075\u6d3b\u6027\u548c\u6613\u7528\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u7684\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4e14\u5f00\u6e90\u53ef\u7528\u3002"}}
{"id": "2508.09824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09824", "abs": "https://arxiv.org/abs/2508.09824", "authors": ["Xuhong Huang", "Shiqi Liu", "Kai Zhang", "Ying Tai", "Jian Yang", "Hui Zeng", "Lei Zhang"], "title": "Reverse Convolution and Its Applications to Image Restoration", "comment": "ICCV 2025; https://github.com/cszn/ConverseNet", "summary": "Convolution and transposed convolution are fundamental operators widely used\nin neural networks. However, transposed convolution (a.k.a. deconvolution) does\nnot serve as a true inverse of convolution due to inherent differences in their\nmathematical formulations. To date, no reverse convolution operator has been\nestablished as a standard component in neural architectures. In this paper, we\npropose a novel depthwise reverse convolution operator as an initial attempt to\neffectively reverse depthwise convolution by formulating and solving a\nregularized least-squares optimization problem. We thoroughly investigate its\nkernel initialization, padding strategies, and other critical aspects to ensure\nits effective implementation. Building upon this operator, we further construct\na reverse convolution block by combining it with layer normalization,\n1$\\times$1 convolution, and GELU activation, forming a Transformer-like\nstructure. The proposed operator and block can directly replace conventional\nconvolution and transposed convolution layers in existing architectures,\nleading to the development of ConverseNet. Corresponding to typical image\nrestoration models such as DnCNN, SRResNet and USRNet, we train three variants\nof ConverseNet for Gaussian denoising, super-resolution and deblurring,\nrespectively. Extensive experiments demonstrate the effectiveness of the\nproposed reverse convolution operator as a basic building module. We hope this\nwork could pave the way for developing new operators in deep model design and\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u7528\u4e8e\u6709\u6548\u9006\u8f6c\u6df1\u5ea6\u5377\u79ef\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u57fa\u7840\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u8f6c\u7f6e\u5377\u79ef\u5e76\u975e\u5377\u79ef\u7684\u771f\u6b63\u9006\u8fd0\u7b97\uff0c\u76ee\u524d\u795e\u7ecf\u7f51\u7edc\u4e2d\u7f3a\u4e4f\u6807\u51c6\u7684\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u53cd\u5411\u5377\u79ef\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u6df1\u5ea6\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u5e76\u7ed3\u5408\u5c42\u5f52\u4e00\u5316\u30011\u00d71\u5377\u79ef\u548cGELU\u6fc0\u6d3b\u6784\u5efa\u53cd\u5411\u5377\u79ef\u5757\uff0c\u5f62\u6210\u7c7b\u4f3cTransformer\u7684\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u53cd\u5411\u5377\u79ef\u7b97\u5b50\u80fd\u6709\u6548\u66ff\u4ee3\u4f20\u7edf\u5377\u79ef\u548c\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff08\u5982\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u6a21\u578b\u8bbe\u8ba1\u4e2d\u65b0\u7b97\u5b50\u7684\u5f00\u53d1\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09843", "abs": "https://arxiv.org/abs/2508.09843", "authors": ["Hao Yang", "Xu Zhang", "Jiaqi Ma", "Linwei Zhu", "Yun Zhang", "Huan Zhang"], "title": "Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment", "comment": null, "summary": "Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to\nevaluate locally non-uniform distortions due to inadequate modeling of spatial\nvariations in quality and ineffective feature representation capturing both\nlocal details and global context. To address this, we propose a graph neural\nnetwork-based OIQA framework that explicitly models structural relationships\nbetween viewports to enhance perception of spatial distortion non-uniformity.\nOur approach employs Fibonacci sphere sampling to generate viewports with\nwell-structured topology, representing each as a graph node. Multi-stage\nfeature extraction networks then derive high-dimensional node representation.\nTo holistically capture spatial dependencies, we integrate a Graph Attention\nNetwork (GAT) modeling fine-grained local distortion variations among adjacent\nviewports, and a graph transformer capturing long-range quality interactions\nacross distant regions. Extensive experiments on two large-scale OIQA databases\nwith complex spatial distortions demonstrate that our method significantly\noutperforms existing approaches, confirming its effectiveness and strong\ngeneralization capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684OIQA\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u89c6\u53e3\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u6765\u63d0\u5347\u5bf9\u7a7a\u95f4\u5931\u771f\u975e\u5747\u5300\u6027\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709OIQA\u65b9\u6cd5\u96be\u4ee5\u8bc4\u4f30\u5c40\u90e8\u975e\u5747\u5300\u5931\u771f\uff0c\u56e0\u7f3a\u4e4f\u5bf9\u8d28\u91cf\u7a7a\u95f4\u53d8\u5316\u7684\u5efa\u6a21\u53ca\u65e0\u6548\u7684\u7279\u5f81\u8868\u793a\u3002", "method": "\u91c7\u7528\u6590\u6ce2\u90a3\u5951\u7403\u91c7\u6837\u751f\u6210\u89c6\u53e3\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u56fe\u8282\u70b9\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u53ca\u56fe\u53d8\u6362\u5668\u5efa\u6a21\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21OIQA\u6570\u636e\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09847", "abs": "https://arxiv.org/abs/2508.09847", "authors": ["Dhruvraj Singh Rawat", "Enggen Sherpa", "Rishikesan Kirupanantha", "Tin Hoang"], "title": "Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance", "comment": "10 pages, preprint", "summary": "We present a benchmark of diffusion models for human face generation on a\nsmall-scale CelebAMask-HQ dataset, evaluating both unconditional and\nconditional pipelines. Our study compares UNet and DiT architectures for\nunconditional generation and explores LoRA-based fine-tuning of pretrained\nStable Diffusion models as a separate experiment. Building on the\nmulti-conditioning approach of Giambi and Lisanti, which uses both attribute\nvectors and segmentation masks, our main contribution is the integration of an\nInfoNCE loss for attribute embedding and the adoption of a SegFormer-based\nsegmentation encoder. These enhancements improve the semantic alignment and\ncontrollability of attribute-guided synthesis. Our results highlight the\neffectiveness of contrastive embedding learning and advanced segmentation\nencoding for controlled face generation in limited data settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5c0f\u89c4\u6a21CelebAMask-HQ\u6570\u636e\u96c6\u7684\u6269\u6563\u6a21\u578b\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u65e0\u6761\u4ef6\u4e0e\u6709\u6761\u4ef6\u751f\u6210\u6d41\u7a0b\uff0c\u6bd4\u8f83\u4e86UNet\u548cDiT\u67b6\u6784\uff0c\u5e76\u63a2\u7d22\u4e86LoRA\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u4eba\u8138\u751f\u6210\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u63a7\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u4e86InfoNCE\u635f\u5931\u7528\u4e8e\u5c5e\u6027\u5d4c\u5165\u548cSegFormer\u5206\u5272\u7f16\u7801\u5668\uff0c\u6539\u8fdb\u4e86\u591a\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\u5bf9\u6bd4\u5d4c\u5165\u5b66\u4e60\u548c\u9ad8\u7ea7\u5206\u5272\u7f16\u7801\u5728\u6709\u9650\u6570\u636e\u4e0b\u5bf9\u53ef\u63a7\u4eba\u8138\u751f\u6210\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0b\u7684\u53ef\u63a7\u4eba\u8138\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.09849", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09849", "abs": "https://arxiv.org/abs/2508.09849", "authors": ["Jan Phillipp Albrecht", "Jose R. A. Godinho", "Christina H\u00fcbers", "Deborah Schmidt"], "title": "ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images", "comment": "2 figures and 6 pages main article, 17 pages total, 8 figures total,\n  to be published in SoftwareX", "summary": "X-ray computed tomography (CT) is the main 3D technique for imaging the\ninternal microstructures of materials. Quantitative analysis of the\nmicrostructures is usually achieved by applying a sequence of steps that are\nimplemented to the entire 3D image. This is challenged by various imaging\nartifacts inherent from the technique, e.g., beam hardening and partial volume.\nConsequently, the analysis requires users to make a number of decisions to\nsegment and classify the microstructures based on the voxel gray-values. In\nthis context, a software tool, here called ARI3D, is proposed to interactively\nanalyze regions in three-dimensional X-ray CT images, assisting users through\nthe various steps of a protocol designed to classify and quantify objects\nwithin regions of a three-dimensional image. ARI3D aims to 1) Improve phase\nidentification; 2) Account for partial volume effect; 3) Increase the detection\nlimit and accuracy of object quantification; and 4) Harmonize quantitative 3D\nanalysis that can be implemented in different fields of science.", "AI": {"tldr": "ARI3D\u662f\u4e00\u6b3e\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790X\u5c04\u7ebfCT\u56fe\u50cf\u4e2d\u533a\u57df\u7684\u8f6f\u4ef6\u5de5\u5177\uff0c\u65e8\u5728\u6539\u5584\u76f8\u4f4d\u8bc6\u522b\u3001\u8003\u8651\u90e8\u5206\u4f53\u79ef\u6548\u5e94\u3001\u63d0\u9ad8\u68c0\u6d4b\u9650\u548c\u51c6\u786e\u6027\uff0c\u5e76\u7edf\u4e003D\u5b9a\u91cf\u5206\u6790\u3002", "motivation": "X\u5c04\u7ebfCT\u6210\u50cf\u6280\u672f\u5b58\u5728\u591a\u79cd\u56fa\u6709\u6210\u50cf\u4f2a\u5f71\uff08\u5982\u675f\u786c\u5316\u548c\u90e8\u5206\u4f53\u79ef\u6548\u5e94\uff09\uff0c\u5bfc\u81f4\u7528\u6237\u9700\u57fa\u4e8e\u4f53\u7d20\u7070\u5ea6\u503c\u505a\u51fa\u5927\u91cf\u51b3\u7b56\u4ee5\u5206\u5272\u548c\u5206\u7c7b\u5fae\u7ed3\u6784\u3002", "method": "\u63d0\u51faARI3D\u8f6f\u4ef6\u5de5\u5177\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5206\u6790\u534f\u8bae\u8f85\u52a9\u7528\u6237\u5b8c\u62103D\u56fe\u50cf\u4e2d\u533a\u57df\u7684\u5206\u7c7b\u548c\u91cf\u5316\u3002", "result": "ARI3D\u80fd\u591f\u6539\u5584\u76f8\u4f4d\u8bc6\u522b\u3001\u5904\u7406\u90e8\u5206\u4f53\u79ef\u6548\u5e94\u3001\u63d0\u9ad8\u68c0\u6d4b\u9650\u548c\u91cf\u5316\u51c6\u786e\u6027\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u5b66\u79d1\u9886\u57df\u3002", "conclusion": "ARI3D\u4e3a3D X\u5c04\u7ebfCT\u56fe\u50cf\u7684\u5b9a\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09850", "abs": "https://arxiv.org/abs/2508.09850", "authors": ["Pablo Hern\u00e1ndez-C\u00e1mara", "Jose Manuel Ja\u00e9n-Lorites", "Jorge Vila-Tom\u00e1s", "Valero Laparra", "Jesus Malo"], "title": "Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment", "comment": null, "summary": "Vision Transformers (ViTs) achieve remarkable performance in image\nrecognition tasks, yet their alignment with human perception remains largely\nunexplored. This study systematically analyzes how model size, dataset size,\ndata augmentation and regularization impact ViT perceptual alignment with human\njudgments on the TID2013 dataset. Our findings confirm that larger models\nexhibit lower perceptual alignment, consistent with previous works. Increasing\ndataset diversity has a minimal impact, but exposing models to the same images\nmore times reduces alignment. Stronger data augmentation and regularization\nfurther decrease alignment, especially in models exposed to repeated training\ncycles. These results highlight a trade-off between model complexity, training\nstrategies, and alignment with human perception, raising important\nconsiderations for applications requiring human-like visual understanding.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5927\u7684ViT\u6a21\u578b\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u5ea6\u66f4\u4f4e\uff0c\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u964d\u4f4e\u5bf9\u9f50\u5ea6\uff0c\u5c24\u5176\u5728\u91cd\u590d\u8bad\u7ec3\u5468\u671f\u4e2d\u3002", "motivation": "\u63a2\u7d22Vision Transformers\uff08ViTs\uff09\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u5927\u5c0f\u3001\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u5bf9ViT\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u7684\u5f71\u54cd\uff0c\u4f7f\u7528TID2013\u6570\u636e\u96c6\u3002", "result": "\u5927\u6a21\u578b\u5bf9\u9f50\u5ea6\u66f4\u4f4e\uff1b\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u5f71\u54cd\u5c0f\uff0c\u4f46\u91cd\u590d\u8bad\u7ec3\u964d\u4f4e\u5bf9\u9f50\u5ea6\uff1b\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u51cf\u5c11\u5bf9\u9f50\u5ea6\u3002", "conclusion": "\u6a21\u578b\u590d\u6742\u6027\u3001\u8bad\u7ec3\u7b56\u7565\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u5728\u5e94\u7528\u4e2d\u6743\u8861\u8fd9\u4e9b\u56e0\u7d20\u3002"}}
{"id": "2508.09857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09857", "abs": "https://arxiv.org/abs/2508.09857", "authors": ["Yupeng Zhou", "Zhen Li", "Ziheng Ouyang", "Yuming Chen", "Ruoyi Du", "Daquan Zhou", "Bin Fu", "Yihao Liu", "Peng Gao", "Ming-Ming Cheng", "Qibin Hou"], "title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better", "comment": null, "summary": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fde\u7eed\u548c\u79bb\u6563\u89c6\u9891VAE\u7684\u65b9\u6cd5OneVAE\uff0c\u901a\u8fc7FSQ\u4fdd\u7559\u8fde\u7eedVAE\u7684\u5148\u9a8c\uff0c\u52a0\u901f\u8bad\u7ec3\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5f15\u5165\u591a\u4ee4\u724c\u91cf\u5316\u673a\u5236\u548c\u589e\u5f3a\u9996\u5e27\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u5728\u5355\u4e00\u7f51\u7edc\u4e2d\u540c\u65f6\u4f18\u5316\u4e24\u79cd\u8868\u793a\u3002", "motivation": "\u79bb\u6563\u89c6\u9891VAE\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u65f6\u95f4\u957f\u548c\u91cd\u5efa\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u800c\u8fde\u7eedVAE\u8bad\u7ec3\u66f4\u7b80\u5355\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6765\u6539\u8fdb\u79bb\u6563\u89c6\u9891VAE\u3002", "method": "\u5229\u7528FSQ\u4fdd\u7559\u8fde\u7eedVAE\u5148\u9a8c\uff0c\u63d0\u51fa\u591a\u4ee4\u724c\u91cf\u5316\u673a\u5236\u548c\u589e\u5f3a\u9996\u5e27\u91cd\u5efa\uff0c\u5e76\u8bbe\u8ba1\u8054\u5408\u79bb\u6563-\u8fde\u7eed\u4f18\u5316\u65b9\u6848\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u6570\u500d\uff0c\u6536\u655b\u65f6\u6027\u80fd\u66f4\u4f18\uff0cPSNR\u63d0\u5347\u8fd11 dB\uff0c\u5e76\u5728\u9ad8\u538b\u7f29\u89c6\u9891VAE\u4e2d\u663e\u8457\u6539\u5584\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "OneVAE\u6210\u529f\u7edf\u4e00\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u8868\u793a\uff0c\u9996\u6b21\u5728\u5355\u4e00\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u4e24\u8005\u7684\u7ade\u4e89\u6027\u80fd\u3002"}}
{"id": "2508.09858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09858", "abs": "https://arxiv.org/abs/2508.09858", "authors": ["Weiqi Li", "Zehao Zhang", "Liang Lin", "Guangrun Wang"], "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics", "comment": null, "summary": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of\nhuman subjects performing expressive, intention-driven motions. However,\ncurrent approaches face two core challenges: (1) \\emph{geometric inconsistency}\nand \\emph{coarse reconstruction}, due to limited 3D modeling and detail\npreservation; and (2) \\emph{motion generalization limitations} and \\emph{scene\ninharmonization}, stemming from weak generative capabilities. To address these,\nwe present \\textbf{HumanGenesis}, a framework that integrates geometric and\ngenerative modeling through four collaborative agents: (1)\n\\textbf{Reconstructor} builds 3D-consistent human-scene representations from\nmonocular video using 3D Gaussian Splatting and deformation decomposition. (2)\n\\textbf{Critique Agent} enhances reconstruction fidelity by identifying and\nrefining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose\nGuider} enables motion generalization by generating expressive pose sequences\nusing time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes\nphotorealistic, coherent video via a hybrid rendering pipeline with diffusion,\nrefining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis\nachieves state-of-the-art performance on tasks including text-guided synthesis,\nvideo reenactment, and novel-pose generalization, significantly improving\nexpressiveness, geometric fidelity, and scene integration.", "AI": {"tldr": "HumanGenesis\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u534f\u4f5c\u4ee3\u7406\u89e3\u51b3\u5408\u6210\u4eba\u7c7b\u52a8\u6001\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u8fd0\u52a8\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8868\u73b0\u529b\u3001\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u573a\u666f\u6574\u5408\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u91cd\u5efa\u7c97\u7cd9\u548c\u573a\u666f\u4e0d\u534f\u8c03\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u4e0e\u751f\u6210\u5efa\u6a21\uff0c\u901a\u8fc7Reconstructor\u3001Critique Agent\u3001Pose Guider\u548cVideo Harmonizer\u56db\u4e2a\u4ee3\u7406\u534f\u4f5c\u5b8c\u6210\u3002", "result": "\u5728\u6587\u672c\u5f15\u5bfc\u5408\u6210\u3001\u89c6\u9891\u91cd\u6f14\u548c\u65b0\u59ff\u52bf\u6cdb\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "HumanGenesis\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5408\u6210\u4eba\u7c7b\u52a8\u6001\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.09886", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09886", "abs": "https://arxiv.org/abs/2508.09886", "authors": ["Lingyu Chen", "Yawen Zeng", "Yue Wang", "Peng Wan", "Guo-chen Ning", "Hongen Liao", "Daoqiang Zhang", "Fang Chen"], "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "comment": "ICCV 2025", "summary": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOME\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u591a\u5f02\u8d28\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u901a\u8fc7\u53cc\u7ed3\u6784-\u8bed\u4e49\u5171\u4eab\u4e13\u5bb6\u548c\u6e90\u7279\u5b9a\u4e13\u5bb6\u7684\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u5728\u65b0\u6570\u636e\u5206\u5e03\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u7531\u4e8e\u6570\u636e\u6709\u9650\u3001\u58f0\u5b66\u9634\u5f71\u548c\u6591\u70b9\u566a\u58f0\u7b49\u95ee\u9898\uff0c\u6784\u5efa\u4e00\u4e2a\u901a\u7528\u7684\u591a\u5f02\u8d28\u6570\u636e\u96c6\u6846\u67b6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "COME\u901a\u8fc7\u5efa\u7acb\u53cc\u7ed3\u6784-\u8bed\u4e49\u5171\u4eab\u4e13\u5bb6\uff0c\u521b\u5efa\u901a\u7528\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u4e0e\u6e90\u7279\u5b9a\u4e13\u5bb6\u534f\u4f5c\uff0c\u63d0\u53d6\u5224\u522b\u6027\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8de8\u6570\u636e\u96c6\u7ecf\u9a8c\u5206\u5e03\uff0c\u4e3a\u5c0f\u6279\u91cf\u6216\u672a\u89c1\u6570\u636e\u63d0\u4f9b\u901a\u7528\u8d85\u58f0\u5148\u9a8c\u3002", "result": "\u5728\u4e09\u79cd\u8bc4\u4f30\u6a21\u5f0f\uff08\u5355\u6570\u636e\u96c6\u3001\u5668\u5b98\u5185\u548c\u5668\u5b98\u95f4\u96c6\u6210\u6570\u636e\u96c6\uff09\u4e0b\uff0cCOME\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747AP\u663e\u8457\u63d0\u5347\u3002", "conclusion": "COME\u901a\u8fc7\u534f\u4f5c\u5f02\u6784\u4e13\u5bb6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u7684\u9c81\u68d2\u6cdb\u5316\uff0c\u4e3a\u591a\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09912", "abs": "https://arxiv.org/abs/2508.09912", "authors": ["Chaoran Feng", "Zhenyu Tang", "Wangbo Yu", "Yatian Pang", "Yian Zhao", "Jianbin Zhao", "Li Yuan", "Yonghong Tian"], "title": "E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras", "comment": "16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025", "summary": "Novel view synthesis and 4D reconstruction techniques predominantly rely on\nRGB cameras, thereby inheriting inherent limitations such as the dependence on\nadequate lighting, susceptibility to motion blur, and a limited dynamic range.\nEvent cameras, offering advantages of low power, high temporal resolution and\nhigh dynamic range, have brought a new perspective to addressing the scene\nreconstruction challenges in high-speed motion and low-light scenes. To this\nend, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting\napproach, for novel view synthesis from multi-view event streams with\nfast-moving cameras. Specifically, we introduce an event-based initialization\nscheme to ensure stable training and propose event-adaptive slicing splatting\nfor time-aware reconstruction. Additionally, we employ intensity importance\npruning to eliminate floating artifacts and enhance 3D consistency, while\nincorporating an adaptive contrast threshold for more precise optimization. We\ndesign a synthetic multi-view camera setup with six moving event cameras\nsurrounding the object in a 360-degree configuration and provide a benchmark\nmulti-view event stream dataset that captures challenging motion scenarios. Our\napproach outperforms both event-only and event-RGB fusion baselines and paves\nthe way for the exploration of multi-view event-based reconstruction as a novel\napproach for rapid scene capture.", "AI": {"tldr": "E-4DGS\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5149\u7167\u573a\u666f\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u5728\u5149\u7167\u4e0d\u8db3\u3001\u52a8\u6001\u8303\u56f4\u6709\u9650\u548c\u8fd0\u52a8\u6a21\u7cca\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u4f4e\u529f\u8017\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7684\u4f18\u52bf\uff0c\u9002\u5408\u9ad8\u901f\u8fd0\u52a8\u548c\u4f4e\u5149\u573a\u666f\u7684\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u521d\u59cb\u5316\u65b9\u6848\u548c\u4e8b\u4ef6\u81ea\u9002\u5e94\u5207\u7247\u6cfc\u6e85\u6280\u672f\uff0c\u7ed3\u5408\u5f3a\u5ea6\u91cd\u8981\u6027\u526a\u679d\u548c\u81ea\u9002\u5e94\u5bf9\u6bd4\u5ea6\u9608\u503c\u4f18\u5316\uff0c\u5b9e\u73b0\u65f6\u95f4\u611f\u77e5\u91cd\u5efa\u3002", "result": "E-4DGS\u5728\u5408\u6210\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u4e8b\u4ef6\u548c\u4e8b\u4ef6-RGB\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "E-4DGS\u4e3a\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u5feb\u901f\u573a\u666f\u6355\u6349\u7684\u63a2\u7d22\u3002"}}
{"id": "2508.09913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09913", "abs": "https://arxiv.org/abs/2508.09913", "authors": ["Yachao Liang", "Min Yu", "Gang Li", "Jianguo Jiang", "Boquan Li", "Feng Yu", "Ning Zhang", "Xiang Meng", "Weiqing Huang"], "title": "SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection", "comment": "Accepted by NeurIPS 2024", "summary": "Detection of face forgery videos remains a formidable challenge in the field\nof digital forensics, especially the generalization to unseen datasets and\ncommon perturbations. In this paper, we tackle this issue by leveraging the\nsynergy between audio and visual speech elements, embarking on a novel approach\nthrough audio-visual speech representation learning. Our work is motivated by\nthe finding that audio signals, enriched with speech content, can provide\nprecise information effectively reflecting facial movements. To this end, we\nfirst learn precise audio-visual speech representations on real videos via a\nself-supervised masked prediction task, which encodes both local and global\nsemantic information simultaneously. Then, the derived model is directly\ntransferred to the forgery detection task. Extensive experiments demonstrate\nthat our method outperforms the state-of-the-art methods in terms of\ncross-dataset generalization and robustness, without the participation of any\nfake video in model training. Code is available at\nhttps://github.com/Eleven4AI/SpeechForensics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u68c0\u6d4b\u4f2a\u9020\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u534f\u540c\u4f5c\u7528\uff0c\u65e0\u9700\u4f2a\u9020\u89c6\u9891\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u5728\u6570\u5b57\u53d6\u8bc1\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u548c\u5e38\u89c1\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u97f3\u9891\u4fe1\u53f7\u80fd\u7cbe\u786e\u53cd\u6620\u9762\u90e8\u8fd0\u52a8\uff0c\u56e0\u6b64\u7ed3\u5408\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u53ef\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u63a9\u7801\u9884\u6d4b\u4efb\u52a1\u5b66\u4e60\u771f\u5b9e\u89c6\u9891\u7684\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8868\u793a\uff0c\u540c\u65f6\u7f16\u7801\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5c06\u6a21\u578b\u76f4\u63a5\u8fc1\u79fb\u81f3\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u4f7f\u7528\u4f2a\u9020\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002", "conclusion": "\u97f3\u9891-\u89c6\u89c9\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u662f\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u6cdb\u5316\u80fd\u529b\u5f3a\u548c\u9c81\u68d2\u6027\u9ad8\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.09926", "categories": ["cs.CV", "q-bio.QM", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.09926", "abs": "https://arxiv.org/abs/2508.09926", "authors": ["Benjamin Adjadj", "Pierre-Antoine Bannier", "Guillaume Horent", "Sebastien Mandela", "Aurore Lyon", "Kathryn Schutte", "Ulysse Marteau", "Valentin Gaury", "Laura Dumont", "Thomas Mathieu", "Reda Belbahri", "Beno\u00eet Schmauch", "Eric Durand", "Katharina Von Loga", "Lucie Gillet"], "title": "Towards Comprehensive Cellular Characterisation of H&E slides", "comment": "33 pages, 4 figures", "summary": "Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.", "AI": {"tldr": "HistoPLUS\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u7ec6\u80de\u5206\u6790\u6a21\u578b\uff0c\u5728\u68c0\u6d4b\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u5bf9\u7f55\u89c1\u7ec6\u80de\u7c7b\u578b\u7684\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7f55\u89c1\u7ec6\u80de\u7c7b\u578b\u548c\u8de8\u57df\u6cdb\u5316\u4e0a\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u65b0\u6784\u5efa\u7684\u6cdb\u764c\u6570\u636e\u96c6\uff08108,722\u4e2a\u6838\uff0c13\u79cd\u7ec6\u80de\u7c7b\u578b\uff09\u8bad\u7ec3HistoPLUS\u6a21\u578b\u3002", "result": "\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u68c0\u6d4b\u8d28\u91cf\u63d0\u53475.2%\uff0c\u5206\u7c7bF1\u5206\u6570\u63d0\u534723.7%\uff0c\u53c2\u6570\u51cf\u5c115\u500d\u3002\u652f\u63017\u79cd\u7f55\u89c1\u7ec6\u80de\u7c7b\u578b\u7814\u7a76\u3002", "conclusion": "HistoPLUS\u5728\u7ec6\u80de\u5206\u6790\u548c\u8de8\u57df\u6cdb\u5316\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u80bf\u7624\u5fae\u73af\u5883\u7814\u7a76\u3002"}}
{"id": "2508.09936", "categories": ["cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.09936", "abs": "https://arxiv.org/abs/2508.09936", "authors": ["Vittorio Pippi", "Konstantina Nikolaidou", "Silvia Cascianelli", "George Retsinas", "Giorgos Sfikas", "Rita Cucchiara", "Marcus Liwicki"], "title": "Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?", "comment": "Accepted at ICCV Workshop VisionDocs", "summary": "The digitization of historical manuscripts presents significant challenges\nfor Handwritten Text Recognition (HTR) systems, particularly when dealing with\nsmall, author-specific collections that diverge from the training data\ndistributions. Handwritten Text Generation (HTG) techniques, which generate\nsynthetic data tailored to specific handwriting styles, offer a promising\nsolution to address these challenges. However, the effectiveness of various HTG\nmodels in enhancing HTR performance, especially in low-resource transcription\nsettings, has not been thoroughly evaluated. In this work, we systematically\ncompare three state-of-the-art styled HTG models (representing the generative\nadversarial, diffusion, and autoregressive paradigms for HTG) to assess their\nimpact on HTR fine-tuning. We analyze how visual and linguistic characteristics\nof synthetic data influence fine-tuning outcomes and provide quantitative\nguidelines for selecting the most effective HTG model. The results of our\nanalysis provide insights into the current capabilities of HTG methods and\nhighlight key areas for further improvement in their application to\nlow-resource HTR.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u5148\u8fdb\u7684HTG\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u5bf9HTR\u5fae\u8c03\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u6709\u6548\u6a21\u578b\u7684\u5b9a\u91cf\u6307\u5357\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u624b\u7a3f\u6570\u5b57\u5316\u4e2dHTR\u7cfb\u7edf\u56e0\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8f6c\u5f55\u573a\u666f\u3002", "method": "\u6bd4\u8f83\u4e09\u79cdHTG\u6a21\u578b\uff08\u751f\u6210\u5bf9\u6297\u3001\u6269\u6563\u548c\u81ea\u56de\u5f52\u8303\u5f0f\uff09\uff0c\u5206\u6790\u5408\u6210\u6570\u636e\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\u5bf9\u5fae\u8c03\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u4f9b\u4e86HTG\u6a21\u578b\u5bf9HTR\u5fae\u8c03\u6548\u679c\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "HTG\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90HTR\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2508.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09943", "abs": "https://arxiv.org/abs/2508.09943", "authors": ["Tom\u00e1s de la Sotta", "Jos\u00e9 M. Saavedra", "H\u00e9ctor Henr\u00edquez", "Violeta Chang", "Aline Xavier"], "title": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models", "comment": null, "summary": "Low-dose CT (LDCT) protocols reduce radiation exposure but increase image\nnoise, compromising diagnostic confidence. Diffusion-based generative models\nhave shown promise for LDCT denoising by learning image priors and performing\niterative refinement. In this work, we introduce AST-n, an accelerated\ninference framework that initiates reverse diffusion from intermediate noise\nlevels, and integrate high-order ODE solvers within conditioned models to\nfurther reduce sampling steps. We evaluate two acceleration paradigms--AST-n\nsampling and standard scheduling with high-order solvers -- on the Low Dose CT\nGrand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %\nof standard dose. Conditioned models using only 25 steps (AST-25) achieve peak\nsignal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)\nabove 0.95, closely matching standard baselines while cutting inference time\nfrom ~16 seg to under 1 seg per slice. Unconditional sampling suffers\nsubstantial quality loss, underscoring the necessity of conditioning. We also\nassess DDIM inversion, which yields marginal PSNR gains at the cost of doubling\ninference time, limiting its clinical practicality. Our results demonstrate\nthat AST-n with high-order samplers enables rapid LDCT reconstruction without\nsignificant loss of image fidelity, advancing the feasibility of\ndiffusion-based methods in clinical workflows.", "AI": {"tldr": "AST-n\u6846\u67b6\u901a\u8fc7\u4ece\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u542f\u52a8\u53cd\u5411\u6269\u6563\u5e76\u7ed3\u5408\u9ad8\u9636ODE\u6c42\u89e3\u5668\uff0c\u663e\u8457\u52a0\u901f\u4f4e\u5242\u91cfCT\u53bb\u566a\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u51cf\u5c11\u8f90\u5c04\u4f46\u589e\u52a0\u56fe\u50cf\u566a\u58f0\uff0c\u5f71\u54cd\u8bca\u65ad\u4fe1\u5fc3\uff0c\u9700\u9ad8\u6548\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAST-n\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u9636ODE\u6c42\u89e3\u5668\u548c\u6761\u4ef6\u6a21\u578b\uff0c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "AST-25\u572825\u6b65\u5185\u5b9e\u73b0PSNR>38 dB\u548cSSIM>0.95\uff0c\u63a8\u7406\u65f6\u95f4\u4ece16\u79d2\u964d\u81f31\u79d2\u3002", "conclusion": "AST-n\u7ed3\u5408\u9ad8\u9636\u6c42\u89e3\u5668\u53ef\u5feb\u901f\u91cd\u5efaLDCT\u56fe\u50cf\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u4e34\u5e8a\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09949", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09949", "abs": "https://arxiv.org/abs/2508.09949", "authors": ["Trevine Oorloff", "Vishwanath Sindagi", "Wele Gedara Chaminda Bandara", "Ali Shafahi", "Amin Ghiasi", "Charan Prakash", "Reza Ardekani"], "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning", "comment": "Accepted to ICCV 2025", "summary": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance.", "AI": {"tldr": "\u5229\u7528\u73b0\u6210\u7684Stable Diffusion\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08V-ICL\uff09\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u7b80\u5316\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\uff0c\u907f\u514d\u590d\u6742\u7684\u8bad\u7ec3\u548c\u989d\u5916\u6570\u636e\u9700\u6c42\uff0c\u63d0\u9ad8\u901a\u7528\u6027\u3002", "method": "\u5728Stable Diffusion\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u91cd\u65b0\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u663e\u5f0f\u7ed3\u5408\u67e5\u8be2\u548c\u793a\u4f8b\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u516d\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u524d\u666f\u5206\u5272\u4efb\u52a1\u5728Pascal-5i\u6570\u636e\u96c6\u4e0amIoU\u63d0\u53478.9%\u3002", "conclusion": "Stable Diffusion\u6a21\u578b\u53ef\u7528\u4e8e\u9ad8\u6548\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e14\u901a\u8fc7\u96c6\u6210\u591a\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.09959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09959", "abs": "https://arxiv.org/abs/2508.09959", "authors": ["Yaohui Wang", "Di Yang", "Xinyuan Chen", "Francois Bremond", "Yu Qiao", "Antitza Dantcheva"], "title": "LIA-X: Interpretable Latent Portrait Animator", "comment": "Project Page: https://wyhsirius.github.io/LIA-X-project/", "summary": "We introduce LIA-X, a novel interpretable portrait animator designed to\ntransfer facial dynamics from a driving video to a source portrait with\nfine-grained control. LIA-X is an autoencoder that models motion transfer as a\nlinear navigation of motion codes in latent space. Crucially, it incorporates a\nnovel Sparse Motion Dictionary that enables the model to disentangle facial\ndynamics into interpretable factors. Deviating from previous 'warp-render'\napproaches, the interpretability of the Sparse Motion Dictionary allows LIA-X\nto support a highly controllable 'edit-warp-render' strategy, enabling precise\nmanipulation of fine-grained facial semantics in the source portrait. This\nhelps to narrow initial differences with the driving video in terms of pose and\nexpression. Moreover, we demonstrate the scalability of LIA-X by successfully\ntraining a large-scale model with approximately 1 billion parameters on\nextensive datasets. Experimental results show that our proposed method\noutperforms previous approaches in both self-reenactment and cross-reenactment\ntasks across several benchmarks. Additionally, the interpretable and\ncontrollable nature of LIA-X supports practical applications such as\nfine-grained, user-guided image and video editing, as well as 3D-aware portrait\nvideo manipulation.", "AI": {"tldr": "LIA-X\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u89e3\u91ca\u8096\u50cf\u52a8\u753b\u751f\u6210\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u8fd0\u52a8\u5b57\u5178\u5b9e\u73b0\u9762\u90e8\u52a8\u6001\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9762\u90e8\u52a8\u6001\u8fc1\u79fb\u4e2d\u7f3a\u4e4f\u7cbe\u7ec6\u63a7\u5236\u548c\u53ef\u89e3\u91ca\u6027\uff0cLIA-X\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u8fd0\u52a8\u5b57\u5178\u5c06\u9762\u90e8\u52a8\u6001\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u56e0\u7d20\uff0c\u652f\u6301\u201c\u7f16\u8f91-\u53d8\u5f62-\u6e32\u67d3\u201d\u7b56\u7565\u3002", "result": "\u5728\u81ea\u91cd\u73b0\u548c\u8de8\u91cd\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u7528\u6237\u5f15\u5bfc\u7684\u7cbe\u7ec6\u7f16\u8f91\u548c3D\u611f\u77e5\u64cd\u4f5c\u3002", "conclusion": "LIA-X\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u6027\uff0c\u4e3a\u8096\u50cf\u52a8\u753b\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09966", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09966", "abs": "https://arxiv.org/abs/2508.09966", "authors": ["Amir Hosseinian", "Ashkan Dehghani Zahedani", "Umer Mansoor", "Noosheen Hashemi", "Mark Woodward"], "title": "January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis", "comment": null, "summary": "Progress in AI for automated nutritional analysis is critically hampered by\nthe lack of standardized evaluation methodologies and high-quality, real-world\nbenchmark datasets. To address this, we introduce three primary contributions.\nFirst, we present the January Food Benchmark (JFB), a publicly available\ncollection of 1,000 food images with human-validated annotations. Second, we\ndetail a comprehensive benchmarking framework, including robust metrics and a\nnovel, application-oriented overall score designed to assess model performance\nholistically. Third, we provide baseline results from both general-purpose\nVision-Language Models (VLMs) and our own specialized model,\njanuary/food-vision-v1. Our evaluation demonstrates that the specialized model\nachieves an Overall Score of 86.2, a 12.1-point improvement over the\nbest-performing general-purpose configuration. This work offers the research\ncommunity a valuable new evaluation dataset and a rigorous framework to guide\nand benchmark future developments in automated nutritional analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86JFB\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u8425\u517b\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u8425\u517b\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165JFB\u6570\u636e\u96c6\u3001\u8bbe\u8ba1\u8bc4\u4f30\u6846\u67b6\uff08\u5305\u62ec\u65b0\u8bc4\u5206\u6807\u51c6\uff09\uff0c\u5e76\u6d4b\u8bd5\u901a\u7528\u4e0e\u4e13\u7528\u6a21\u578b\u3002", "result": "\u4e13\u7528\u6a21\u578b\uff08january/food-vision-v1\uff09\u5728\u7efc\u5408\u8bc4\u5206\u4e0a\u6bd4\u901a\u7528\u6a21\u578b\u63d0\u534712.1\u5206\u3002", "conclusion": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u81ea\u52a8\u8425\u517b\u5206\u6790\u53d1\u5c55\u3002"}}
{"id": "2508.09967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09967", "abs": "https://arxiv.org/abs/2508.09967", "authors": ["Tianqi Xiang", "Yi Li", "Qixiang Zhang", "Xiaomeng Li"], "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification", "comment": "Accepted in MICCAI 2025", "summary": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u4f18\u5316\u5206\u7c7b\u5668\uff08MOC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u5019\u9009\u5206\u7c7b\u5668\u548c\u5143\u5b66\u4e60\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u4e0b\u7684WSI\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLFM\uff09\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "MOC\u5305\u542b\u4e00\u4e2a\u5143\u5b66\u4e60\u5668\u548c\u4e00\u4e2a\u5206\u7c7b\u5668\u5e93\uff0c\u524d\u8005\u81ea\u52a8\u4f18\u5316\u5206\u7c7b\u5668\u914d\u7f6e\uff0c\u540e\u8005\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5019\u9009\u5206\u7c7b\u5668\u3002", "result": "\u5728TCGA-NSCLC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOC\u7684AUC\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8610.4%\uff0c\u57281-shot\u6761\u4ef6\u4e0b\u63d0\u5347\u9ad8\u8fbe26.25%\u3002", "conclusion": "MOC\u4e3a\u4e34\u5e8a\u90e8\u7f72\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09973", "abs": "https://arxiv.org/abs/2508.09973", "authors": ["Geonhee Sim", "Gyeongsik Moon"], "title": "PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image", "comment": "Accepted to ICCV 2025. https://mks0601.github.io/PERSONA/", "summary": "Two major approaches exist for creating animatable human avatars. The first,\na 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a\nsingle person, achieving personalization through a disentangled identity\nrepresentation. However, modeling pose-driven deformations, such as non-rigid\ncloth deformations, requires numerous pose-rich videos, which are costly and\nimpractical to capture in daily life. The second, a diffusion-based approach,\nlearns pose-driven deformations from large-scale in-the-wild videos but\nstruggles with identity preservation and pose-dependent identity entanglement.\nWe present PERSONA, a framework that combines the strengths of both approaches\nto obtain a personalized 3D human avatar with pose-driven deformations from a\nsingle image. PERSONA leverages a diffusion-based approach to generate\npose-rich videos from the input image and optimizes a 3D avatar based on them.\nTo ensure high authenticity and sharp renderings across diverse poses, we\nintroduce balanced sampling and geometry-weighted optimization. Balanced\nsampling oversamples the input image to mitigate identity shifts in\ndiffusion-generated training videos. Geometry-weighted optimization prioritizes\ngeometry constraints over image loss, preserving rendering quality in diverse\nposes.", "AI": {"tldr": "PERSONA\u6846\u67b6\u7ed3\u54083D\u548c\u6269\u6563\u65b9\u6cd5\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u4e2a\u6027\u53163D\u4eba\u4f53\u5316\u8eab\uff0c\u89e3\u51b3\u8eab\u4efd\u4fdd\u6301\u548c\u59ff\u6001\u9a71\u52a8\u53d8\u5f62\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u65b9\u6cd5\u9700\u591a\u59ff\u6001\u89c6\u9891\uff0c\u6269\u6563\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\uff0cPERSONA\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5229\u7528\u6269\u6563\u65b9\u6cd5\u751f\u6210\u591a\u59ff\u6001\u89c6\u9891\uff0c\u901a\u8fc7\u5e73\u8861\u91c7\u6837\u548c\u51e0\u4f55\u52a0\u6743\u4f18\u5316\u4f18\u53163D\u5316\u8eab\u3002", "result": "\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8eab\u4efd\u4fdd\u6301\u76843D\u5316\u8eab\u3002", "conclusion": "PERSONA\u4e3a\u5355\u56fe\u50cf\u751f\u62103D\u5316\u8eab\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09977", "abs": "https://arxiv.org/abs/2508.09977", "authors": ["Shuting He", "Peilin Ji", "Yitong Yang", "Changshuo Wang", "Jiayi Ji", "Yinglin Wang", "Henghui Ding"], "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation", "comment": "GitHub Repo:\n  https://github.com/heshuting555/Awesome-3DGS-Applications", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative\nto Neural Radiance Fields (NeRF) for 3D scene representation, offering\nhigh-fidelity photorealistic rendering with real-time performance. Beyond novel\nview synthesis, the explicit and compact nature of 3DGS enables a wide range of\ndownstream applications that require geometric and semantic understanding. This\nsurvey provides a comprehensive overview of recent progress in 3DGS\napplications. It first introduces 2D foundation models that support semantic\nunderstanding and control in 3DGS applications, followed by a review of\nNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS\napplications into segmentation, editing, generation, and other functional\ntasks. For each, we summarize representative methods, supervision strategies,\nand learning paradigms, highlighting shared design principles and emerging\ntrends. Commonly used datasets and evaluation protocols are also summarized,\nalong with comparative analyses of recent methods across public benchmarks. To\nsupport ongoing research and development, a continually updated repository of\npapers, code, and resources is maintained at\nhttps://github.com/heshuting555/Awesome-3DGS-Applications.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4f5c\u4e3aNeRF\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6e32\u67d3\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002\u672c\u6587\u7efc\u8ff0\u4e863DGS\u5728\u5206\u5272\u3001\u7f16\u8f91\u3001\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u7684\u8fdb\u5c55\u3002", "motivation": "\u63a2\u7d223DGS\u5728\u51e0\u4f55\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u603b\u7ed3\u5176\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u5206\u7c7b3DGS\u5e94\u7528\uff0c\u603b\u7ed3\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u76d1\u7763\u7b56\u7565\u548c\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u63d0\u4f9b\u4e863DGS\u5e94\u7528\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u516c\u5f00\u57fa\u51c6\u7684\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "3DGS\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672a\u6765\u7814\u7a76\u53ef\u901a\u8fc7\u6301\u7eed\u66f4\u65b0\u7684\u8d44\u6e90\u5e93\u8fdb\u4e00\u6b65\u63a8\u52a8\u53d1\u5c55\u3002"}}
{"id": "2508.09981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09981", "abs": "https://arxiv.org/abs/2508.09981", "authors": ["Chengtao Lv", "Bilang Zhang", "Yang Yong", "Ruihao Gong", "Yushi Huang", "Shiqiao Gu", "Jiajun Wu", "Yumeng Shi", "Jinyang Guo", "Wenya Wang"], "title": "LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit", "comment": "13 pages, 4 figures", "summary": "Large Vision-Language Models (VLMs) exhibit impressive multi-modal\ncapabilities but suffer from prohibitive computational and memory demands, due\nto their long visual token sequences and massive parameter sizes. To address\nthese issues, recent works have proposed training-free compression methods.\nHowever, existing efforts often suffer from three major limitations: (1)\nCurrent approaches do not decompose techniques into comparable modules,\nhindering fair evaluation across spatial and temporal redundancy. (2)\nEvaluation confined to simple single-turn tasks, failing to reflect performance\nin realistic scenarios. (3) Isolated use of individual compression techniques,\nwithout exploring their joint potential. To overcome these gaps, we introduce\nLLMC+, a comprehensive VLM compression benchmark with a versatile,\nplug-and-play toolkit. LLMC+ supports over 20 algorithms across five\nrepresentative VLM families and enables systematic study of token-level and\nmodel-level compression. Our benchmark reveals that: (1) Spatial and temporal\nredundancies demand distinct technical strategies. (2) Token reduction methods\ndegrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)\nCombining token and model compression achieves extreme compression with minimal\nperformance loss. We believe LLMC+ will facilitate fair evaluation and inspire\nfuture research in efficient VLM. Our code is available at\nhttps://github.com/ModelTC/LightCompress.", "AI": {"tldr": "LLMC+\u662f\u4e00\u4e2a\u5168\u9762\u7684VLM\u538b\u7f29\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u5305\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\u3001\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4ee5\u53ca\u8054\u5408\u538b\u7f29\u6280\u672f\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u6a21\u5757\u4e0d\u53ef\u6bd4\u3001\u4efb\u52a1\u5355\u4e00\u548c\u6280\u672f\u5b64\u7acb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLLMC+\u5de5\u5177\u5305\uff0c\u652f\u630120\u591a\u79cd\u7b97\u6cd5\uff0c\u6db5\u76d6\u4e94\u79cd\u4ee3\u8868\u6027VLM\u5bb6\u65cf\uff0c\u7cfb\u7edf\u7814\u7a76\u4ee4\u724c\u7ea7\u548c\u6a21\u578b\u7ea7\u538b\u7f29\u3002", "result": "\u53d1\u73b0\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\u9700\u4e0d\u540c\u7b56\u7565\uff0c\u4ee4\u724c\u51cf\u5c11\u65b9\u6cd5\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8054\u5408\u538b\u7f29\u6280\u672f\u53ef\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "conclusion": "LLMC+\u4e3aVLM\u538b\u7f29\u63d0\u4f9b\u4e86\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\uff0c\u5e76\u542f\u53d1\u4e86\u672a\u6765\u9ad8\u6548VLM\u7684\u7814\u7a76\u3002"}}
{"id": "2508.09987", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09987", "abs": "https://arxiv.org/abs/2508.09987", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GPT-4o\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u5728\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51faEcho-4o-Image\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u65b0\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u5728\u7a00\u6709\u573a\u666f\u548c\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u7684\u4f18\u52bf\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7GPT-4o\u751f\u6210180K\u89c4\u6a21\u7684\u5408\u6210\u6570\u636e\u96c6Echo-4o-Image\uff0c\u5e76\u57fa\u4e8e\u6b64\u5fae\u8c03Bagel\u6a21\u578b\u3002\u63d0\u51faGenEval++\u548cImagine-Bench\u4e24\u4e2a\u65b0\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "Echo-4o\u5728\u6807\u51c6\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5408\u6210\u6570\u636e\u5bf9\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u4e5f\u6709\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u6570\u636e\u80fd\u6709\u6548\u8865\u5145\u771f\u5b9e\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5177\u6709\u5f3a\u53ef\u8fc1\u79fb\u6027\u3002"}}
