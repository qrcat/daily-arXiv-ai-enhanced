<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 提出基于深度学习的实时吸烟检测系统，用于监控消防出口区域，使用改进的YOLOv8模型在低光环境下达到78.9%召回率和83.7% mAP，在Jetson Xavier NX上实现52-97ms推理速度。


<details>
  <summary>Details</summary>
Motivation: 由于消防安全要求，需要开发实时吸烟检测系统来监控消防出口区域，防止因吸烟引发的火灾风险。

Method: 使用8,124张图像和2,708个低光样本的数据集，评估YOLOv8、YOLOv11、YOLOv12模型，并基于YOLOv8开发定制模型，添加结构以适应监控场景挑战。

Result: 提出的模型表现最佳，召回率78.90%，mAP@50为83.70%，在Jetson Xavier NX上推理速度为52-97ms/帧，适合实时操作。

Conclusion: 该系统为公共安全监控和自动合规提供了强大且适应性强的平台，能够有效检测消防出口区域的吸烟行为。

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 使用纯程序生成数据训练表征模型，通过视觉记忆机制在视觉相似性、分类和语义分割任务上实现零样本性能，与真实数据训练的模型性能接近。


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉模型对真实世界图像的依赖，探索完全基于程序生成数据训练模型的可能性，实现模型与真实图像的完全隔离。

Method: 仅使用程序生成数据训练表征模型，构建显式的参考图像嵌入数据库（视觉记忆），在测试时通过检索机制实现零样本推理。

Result: 在NIGHTS视觉相似性任务上与真实数据模型相差1%，在CUB200和Flowers102细粒度分类上分别超越8%和15%，在ImageNet-1K分类上相差10%，在COCO分割任务上R²相差10%。

Conclusion: 程序数据模型能够实现与真实数据模型相近的性能，但物体部件表征相似性不足导致记忆检索错误，这是性能差距的主要原因。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 本研究首次系统评估了单个和融合的眼科基础模型，提出了FusionFM评估框架和两种融合方法，发现DINORET和RetiZero在眼科和全身性疾病预测中表现最佳，门控融合策略对某些疾病有适度改进。


<details>
  <summary>Details</summary>
Motivation: 眼科领域已出现多个基础模型，但缺乏系统评估来确定哪个模型性能最佳、在不同任务中的表现如何以及融合多个模型是否能带来改进。

Method: 提出FusionFM评估套件和两种融合方法，在标准化多国数据集上评估4个先进基础模型（RETFound、VisionFM、RetiZero、DINORET），涵盖眼科疾病检测和全身性疾病预测，使用AUC和F1指标。

Result: DINORET和RetiZero在眼科和全身性疾病任务中表现最优，RetiZero在外部数据集上泛化能力更强；门控融合策略在青光眼、AMD和高血压预测中有适度改进；全身性疾病（特别是高血压）的外部队列预测仍具挑战。

Conclusion: 研究提供了基于证据的眼科基础模型评估，展示了模型融合的优势，并指出了增强临床适用性的策略。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF是一个统一的多模态深度学习框架，通过融合点云和多视角图像来重建多种牙颌面硬组织，解决了现有单模态方法的局限性，在几何精度、结构完整性和空间准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 牙颌面硬组织缺损严重影响患者生理功能、面部美观和心理健康，当前深度学习模型仅限于单组织和单模态输入，导致泛化性差且在解剖保真度、计算效率和跨组织适应性之间存在权衡。

Method: 提出UniDCF框架，通过点云和多视角图像的多模态融合编码，利用各模态互补优势，并加入基于分数的去噪模块来优化表面平滑度。构建了包含6,609名患者的口内扫描、CBCT和CT数据的最大多模态数据集。

Result: UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进方法。临床模拟显示重建设计时间减少99%，临床医生接受率超过94%。

Conclusion: UniDCF实现了快速、自动化、高保真的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者治疗效果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是Ovis2的升级版本，专注于原生分辨率视觉感知和强大多模态推理。它采用原生分辨率视觉Transformer处理图像，避免固定分辨率切片的退化，并引入反思推理能力。模型通过五阶段课程训练，在OpenCompass多模态排行榜上取得78.3分，在子40B参数范围内达到开源MLLM的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决固定分辨率图像处理导致的细节损失和全局布局破坏问题，特别是在视觉密集内容（如复杂图表）中，同时提升多模态推理能力，超越传统的线性思维链推理。

Method: 1. 集成原生分辨率视觉Transformer处理可变分辨率图像
2. 训练模型进行反思推理（自检查和修订）
3. 采用五阶段课程训练：基础视觉和多模态预训练、大规模指令调优、DPO和GRPO对齐与推理增强
4. 使用多模态数据打包和混合并行实现高效扩展

Result: 1. Ovis2.5-9B在OpenCompass平均得分78.3，显著超越前代Ovis2-8B
2. Ovis2.5-2B得分73.9，在其规模级别达到SOTA
3. 在STEM基准测试中取得领先结果
4. 在接地和视频任务上表现强劲
5. 在复杂图表分析方面达到开源SOTA

Conclusion: Ovis2.5通过原生分辨率处理和反思推理机制，在多模态理解和推理能力上实现了显著提升，特别是在视觉密集内容处理方面表现出色，为资源受限的端侧部署提供了高性能解决方案。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE是首个公开的视频到文本电商属性值提取数据集，涵盖14个领域和172个属性，包含224k训练数据和25k评估数据，通过CLIP-MoE过滤系统保证质量，并建立了全面的基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决现有AVE数据集仅限于文本到文本或图像到文本设置，缺乏对产品视频支持、多样化属性覆盖和公开可用性的问题。

Method: 提出VideoAVE数据集，使用基于CLIP的混合专家过滤系统(CLIP-MoE)去除不匹配的视频-产品对，并建立基准测试评估最先进的视频视觉语言模型。

Result: 视频到文本AVE仍然是一个具有挑战性的问题，特别是在开放设置中，现有模型在利用有效时间信息方面仍有改进空间。

Conclusion: VideoAVE为视频属性值提取提供了首个公开数据集和基准，展示了该领域的挑战性，为开发更先进的视觉语言模型提供了基础。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用二阶几何特征（曲率大小、曲率符号和梯度方向）的MLP分类器在手写字符识别中取得优异效果，证明手工特征也能实现深度学习优势


<details>
  <summary>Details</summary>
Motivation: 研究二阶几何特征是否足以替代CNN进行手写字符识别，探索可解释的手工特征在深度学习中的潜力

Method: 使用三个手工特征图（平面曲率大小、曲率符号和梯度方向）作为输入，构建多层感知机(MLP)分类器

Result: 在MNIST数字上达到97%准确率，在EMNIST字母上达到89%准确率

Conclusion: 曲率基表示对手写字符图像具有强大判别能力，深度学习优势可以通过可解释的手工工程特征实现

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 本文提出双重方法改进多模态仇恨检测：通过提示优化框架提升模型性能，以及通过多模态数据增强管道生成反事实中性表情包来减少虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 现代网络充斥着多模态内容，仇恨表情包的检测面临挑战，因为有害意图往往通过文本和图像的微妙互动以幽默或讽刺的形式呈现。现有的视觉语言模型缺乏细粒度监督支持，容易受到隐性仇恨言论的影响。

Method: 1) 提出提示优化框架，系统变化提示结构、监督粒度和训练模态；2) 引入多模态数据增强管道，通过多智能体LLM-VLM设置生成2,479个反事实中性表情包，隔离并重写仇恨模态。

Result: 结构化提示即使在小型模型中也提高了鲁棒性，InternVL2在二元和缩放设置中实现了最佳F1分数。数据增强管道成功减少了虚假相关性，提高了分类器的泛化能力。

Conclusion: 提示结构和数据组成与模型大小同样重要，有针对性的数据增强可以支持更可信和上下文敏感的仇恨检测，为构建合成数据训练鲁棒公平的视觉语言模型提供了新方向。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 该论文研究了高斯曲率在3D表面建模中的作用，发现其能提供稀疏紧凑的表面描述，可作为几何先验改进3D重建，并可能作为立体视觉方法的无监督度量指标。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的计算机视觉方法缺乏显式的3D几何模型，无法直接分析、跨模态迁移或进行系统性修改。研究者希望探索高斯曲率这一不变量在3D建模中的潜在价值。

Method: 使用Middlebury立体数据集进行研究，分析高斯曲率在3D表面建模中的特性，包括其稀疏性、紧凑性，以及作为几何先验的潜力。

Result: 发现高斯曲率能提供稀疏紧凑的3D表面描述；现有单目和立体方法似乎隐式考虑了高斯曲率；高斯曲率可作为几何先验改进3D表面重建；可能作为立体方法的无监督度量指标。

Conclusion: 高斯曲率在3D表面建模中具有重要价值，能提供可分析、可迁移的几何表示，为改进现有深度学习方法提供了新的几何视角和工具。

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [10] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 本研究系统评估了多种图像到图转换方法对基于GNN的图级异常检测的影响，发现颜色特征贡献最佳性能，结合形状和纹理特征能进一步提升检测效果，在无监督、弱监督和全监督设置下均取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 虽然GNN已广泛应用于图像衍生图表示的各种下游任务，但尚无研究系统比较不同图像到图转换方法对GNN图级异常检测效果的影响。

Method: 系统评估多种分割方案、边构建策略以及基于颜色、纹理和形状描述符的节点特征集，使用最先进的GLAD模型在皮肤镜图像上进行广泛实验。

Result: 颜色描述符单独使用时性能最佳，结合形状和纹理特征能持续提升检测效果。最佳无监督配置达到0.805 AUC-ROC，弱监督提升至0.872，全监督达到0.914 AUC-ROC。

Conclusion: 图像到图转换方法的选择对GNN异常检测性能有显著影响，多特征融合能有效提升检测效能，无需依赖预训练骨干网络即可达到竞争性性能。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [11] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 这篇综述论文系统性地分类和评估了Transformer架构在无人机系统中的应用，包括注意力机制、CNN-Transformer混合模型、强化学习Transformers和大语言模型，提供了统一的分类法、比较分析和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的快速发展，它们在无人机感知、决策和自主性方面的应用日益广泛，但缺乏系统性的综述来整合这些进展并指导未来研究。

Method: 采用系统性文献综述方法，对Transformer在无人机领域的应用进行分类和评估，包括构建统一的分类法、进行性能比较分析、总结关键数据集和评估指标。

Result: 提出了Transformer在无人机应用的统一分类体系，识别了在精准农业、自主导航等新兴应用中的进展，同时发现了计算效率和实时部署方面的关键挑战。

Conclusion: 该综述为研究人员和从业者提供了Transformer驱动无人机技术的全面指南，指出了当前研究的空白和未来发展方向，有助于推动该领域的进一步发展。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [12] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat是一种针对3D高斯泼溅技术的黑盒攻击方法，通过利用标准着色方法创建视角特定的伪装，在特定视角下嵌入对抗性内容，无需访问模型架构或权重。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅技术在安全关键任务中的快速应用，需要研究攻击者如何篡改图像造成危害，暴露这种技术在自主导航等关键系统中的安全风险。

Method: 利用标准3DGS着色方法创建视角特定的伪装（颜色和纹理随视角变化），在场景对象中嵌入仅在特定视角可见的对抗性内容，实现黑盒攻击。

Result: 实验表明ComplicitSplat能成功攻击多种流行检测器（单阶段、多阶段和基于transformer的模型），在真实物体捕获和合成场景中都有效。

Conclusion: 这是首个针对下游目标检测器的3DGS黑盒攻击，揭示了自主导航等关键应用系统中的新型安全风险。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [13] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文评估了医学影像基础模型ProFound在前列腺MRI中的标签效率，发现图像质量分布及其在微调和测试集之间的不匹配显著影响模型性能，强调需要评估和对齐质量分布以实现基础模型的数据和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 研究医学影像基础模型在标签效率方面的表现，特别是在前列腺多参数MRI中，探讨图像质量变化如何影响标签高效的微调以及微调模型的泛化能力。

Method: 使用在前列腺MRI大数据集上预训练的领域特定视觉基础模型ProFound，系统性地变化微调和评估集中的高/低质量图像比例，测量微调模型的泛化性。

Result: 图像质量分布及其微调-测试不匹配显著影响模型性能：a) 微调和测试集间高/低质量图像比例变化导致下游性能显著差异；b) 微调集中足够高质量图像对保持强性能至关重要，但匹配的微调-测试分布重要性因下游任务而异。当质量比例一致时，微调所需标注数据远少于从头训练，但标签效率取决于图像质量分布。

Conclusion: 需要评估和对齐微调与部署之间的质量分布，并为特定下游任务制定微调数据的质量标准，以充分实现基础模型的数据和计算效率优势。量化微调和部署中的图像质量具有重要价值。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [14] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing是一个基于跨层张量环分解的视觉语言微调框架，通过整合多样化适配器实现超轻量参数高效适配，在减少90%训练参数的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法存在两个主要限制：1）由于忽略跨层冗余导致的压缩率有限；2）同质适配器的表示能力有限。需要解决跨层冗余问题并提升适配器的表示多样性。

Method: 利用张量级低秩性将适配器表示为层共享的张量核心和层特定切片，通过泛化感知微调指导多样化秩驱动适配器协作处理不同表示需求的任务。

Result: 实验表明AdaRing在减少平均训练参数90%的情况下实现了最先进的性能。

Conclusion: 提出的跨层张量环分解框架有效解决了适配器冗余问题，通过多样化适配器协作实现了参数高效的高性能视觉语言模型适配。

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [15] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 提出了一种名为EVTP-IV的视觉token剪枝方法，通过选择空间代表性强的token子集来加速指令视觉分割任务的推理，在保持精度的同时实现5倍视频和3.5倍图像处理速度提升


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指令视觉分割任务中推理成本过高，特别是视频处理时成为主要瓶颈。实证分析发现token子集覆盖度与分割性能强相关

Method: 基于k-center算法整合空间信息来确保更好覆盖度的视觉token剪枝方法EVTP-IV，通过信息论分析支持设计

Result: 在标准IVS基准测试中，仅使用20%的token就能实现视频任务5倍加速和图像任务3.5倍加速，同时保持可比精度，且在不同剪枝比例下始终优于最先进的剪枝基线

Conclusion: EVTP-IV方法通过有效的视觉token剪枝显著提升了指令视觉分割任务的推理效率，为解决MLLMs推理成本问题提供了有效解决方案

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [16] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 提出了基于纯CNN的大核调制网络（LKMN），通过增强部分大核块和交叉门前馈网络，在保持低延迟的同时实现非局部特征提取，在轻量级超分辨率任务中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决资源受限场景下图像超分辨率任务中卷积神经网络缺乏非局部特征捕获能力而Transformer推理速度慢的问题，寻求性能与延迟的平衡

Method: 使用增强部分大核块（EPLKB）通过通道混洗、通道注意力和部分通道大核条带卷积来提取非局部特征，以及交叉门前馈网络（CGFN）通过可学习缩放因子和交叉门策略动态调整和融合特征

Result: 在Manga109数据集上4倍放大时，比DAT-light模型PSNR提升0.23dB，推理速度快4.8倍，在轻量级超分辨率模型中达到最佳性能

Conclusion: LKMN证明了纯CNN架构可以在保持高效推理的同时实现强大的非局部建模能力，为轻量级超分辨率任务提供了新的解决方案

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用仅包含水平和垂直Sobel导数作为输入的MLP网络，在MNIST和EMNIST数据集上达到了接近CNN的性能，证明了边缘信息对于手写字符识别的重要性


<details>
  <summary>Details</summary>
Motivation: 探索一阶边缘图是否足以驱动全连接MLP进行手写字符识别，作为CNN的替代方案

Method: 仅使用水平和垂直Sobel导数作为输入，训练多层感知机(MLP)在MNIST和EMNIST Letters数据集上进行分类

Result: 在MNIST数字上达到98%准确率，在EMNIST字母上达到92%准确率，接近CNN性能但内存占用更小且特征更透明

Conclusion: 手写字符图像中的类判别信息大部分已被一阶梯度捕获，边缘感知MLP是HCR的一个有吸引力的选择

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了在线视频定位新任务OVG-HQ，支持文本、图像、视频片段等多模态查询，解决了传统视频定位在流媒体和视觉查询场景的局限。


<details>
  <summary>Details</summary>
Motivation: 传统视频定位任务在处理流媒体视频和使用视觉线索查询时存在不足，需要支持多模态查询和在线处理能力。

Method: 提出OVG-HQ-Unify统一框架，包含参数记忆块(PMB)保持历史知识和跨模态蒸馏策略平衡模态学习，构建了QVHighlights-Unify多模态数据集。

Result: 实验表明OVG-HQ-Unify优于现有模型，提供了在线混合模态视频定位的鲁棒解决方案。

Conclusion: 该工作填补了在线多模态视频定位的研究空白，提出了有效的框架和评估指标，为实际应用提供了有力支持。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl是一个轻量级非侵入式插件，通过检测-抑制范式来增强文本到图像模型的安全性，在保持生成质量的同时有效防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型安全方法在安全性和保真度之间的权衡问题，以及基于概念替换的方法可能导致的语义不连贯问题。

Method: 采用检测-抑制范式：首先精确定位不安全内容，然后抑制有害语义而非硬性替换，让生成过程自然地解析为安全的上下文感知替代方案。使用DPO训练策略，利用图像级偏好数据学习细粒度的抑制行为。

Result: 在安全有效性和保真度保持方面显著优于最先进方法，实验证明该方法具有优异性能。

Conclusion: 解耦的基于抑制的控制是构建更负责任生成模型的高效且可扩展的方向。

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP是一个轻量级框架，利用单像素的时序和光谱信息进行土地利用分类，减少对空间上下文和大规模文本监督的依赖


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型在遥感应用中依赖大空间图块增加计算成本，以及缺乏文本监督数据的问题

Method: 利用Sentinel-2影像的光谱和时序信息，通过跨视角学习与地理标记的地面照片进行语义对齐，使用单像素输入结合时序光谱线索

Result: 证明单像素输入结合时序和光谱信息足以进行专题制图，为大规模遥感应用提供可扩展和高效的替代方案

Conclusion: 该方法在LULC、作物类型和生态系统类型分类任务中表现良好，代码已开源

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 使用GAN生成合成MRI数据增强脑肿瘤分割训练，混合数据集（40%真实+60%合成）在整体肿瘤边界描绘方面表现更好，但肿瘤核心区域分割精度仍有不足。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分割中肿瘤异质性、标注数据稀缺和类别不平衡的问题，探索合成数据在医学影像分析中的潜力。

Method: 使用预训练GAN模型生成合成MRI数据，结合BraTS 2020真实数据构建混合数据集，训练U-Net分割网络进行实验比较。

Result: 混合训练模型与纯真实数据模型在定量指标（Dice系数、IoU等）上表现相当，但40%真实+60%合成的混合数据集在肿瘤边界描绘方面有定性改善。

Conclusion: 合成数据可作为脑肿瘤分割的有效数据增强策略，但需要解决肿瘤核心区域的分割精度问题和类别不平衡问题。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 这篇论文是关于深度学习点云去噪的综述研究，将点云去噪分为离群点去除和表面噪声恢复两个步骤，总结了现有方法的贡献并提出了分类体系。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的点云数据存在各种模态和强度的噪声，点云去噪作为预处理步骤对下游任务性能至关重要。尽管深度学习点云去噪方法在性能上超越了传统方法，但目前缺乏系统的综述研究。

Method: 将点云去噪制定为两步过程：离群点去除和表面噪声恢复，涵盖大多数点云去噪场景和需求。总结现有方法的主要贡献，提出针对去噪任务的分类体系，并在相似性、差异性和各自优势方面进行比较。

Result: 提出了一个全面的点云去噪分类框架，系统总结了深度学习点云去噪方法的发展，识别了该领域的关键挑战。

Conclusion: 论文填补了深度学习点云去噪综述的空白，为后续研究提供了见解，并讨论了研究局限性和未来发展方向。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose是一个无需重新训练的6D位姿跟踪框架，通过视觉-惯性里程计、深度信息2D跟踪器和VIO引导的卡尔曼滤波器，在相机和物体快速移动场景中实现鲁棒的实时6D位姿跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于静态或准静态场景，当相机和物体同时快速移动时性能显著下降，需要解决快速运动场景下的6D位姿跟踪鲁棒性问题。

Method: 提出三个协同组件：1）视觉-惯性里程计补偿相机运动引起的ROI偏移；2）深度信息2D跟踪器校正大物体平移引起的ROI偏差；3）VIO引导的卡尔曼滤波器预测物体旋转并生成候选位姿，通过分层细化获得最终位姿。形成闭环系统确保精确的位姿初始化和跟踪。

Result: 仿真和真实世界实验证明该方法有效，能够实现相机和物体快速移动场景下的实时鲁棒6D位姿跟踪。

Conclusion: DynamicPose框架成功解决了快速运动场景下的6D位姿跟踪挑战，通过多组件协同工作和闭环系统设计，显著提升了跟踪鲁棒性和实时性能。

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 该论文提出了一种基于知识蒸馏的点云多尺度特征近似方法，通过单邻域近似多尺度特征，并设计可迁移特征嵌入机制来补偿多样性损失，同时引入中心加权IoU来缓解定位偏差，实现了计算成本的节约。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云目标检测至关重要，但传统多尺度特征学习需要多次邻域搜索和尺度感知层，计算成本高，不利于轻量化模型和有限计算资源的研究。

Method: 1) 基于知识蒸馏从单邻域近似多尺度特征；2) 设计可迁移特征嵌入机制，使用类别感知统计作为可迁移特征；3) 引入中心加权交并比(CWIOU)来缓解定位优化中的中心偏移问题。

Result: 在公开数据集上的大量实验证明了该方法的有效性，能够显著节省计算成本。

Conclusion: 该方法成功实现了从单邻域近似多尺度特征，通过可迁移特征补偿和中心加权定位优化，在保证检测性能的同时大幅降低了计算复杂度。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG是首个统一理解和生成3D模态的框架，使用LLM处理文本和3D表示，通过潜在扩散模型生成高质量3D内容，支持空间视觉问答和3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 尽管近期统一架构在图像理解和生成方面取得显著进展，但3D任务的整合仍然具有挑战性且探索不足，需要开发能够同时处理3D理解和生成的统一框架。

Method: 采用LLM理解和解码句子与3D表示，核心提出空间解码器利用潜在扩散模型生成高质量3D表示，并提出几何语义学习策略预训练视觉编码器，联合捕获输入的语义和几何线索。

Result: 大量实验结果表明该方法在视觉表示、空间理解和3D生成方面具有优越性。

Conclusion: UniUGG框架成功实现了3D模态的统一理解和生成，通过创新的空间解码器和几何语义学习策略，在多个3D任务上表现出色，为3D多模态学习提供了有效解决方案。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH是一个基于时刻感知的Referring Video Object Segmentation框架，通过Moment-guided Dual-path Propagation和Object-level Selective Supervision解决现有方法中的语义错位问题，在MeViS基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Referring Video Object Segmentation方法存在语义错位问题，主要原因是训练时对所有可见对象进行无差别帧采样和监督，而不考虑它们与文本查询的实际相关性。

Method: 提出了SAMDWICH框架，包含：1）新标注的MeViS-M数据集，手动标注每个对象被表达式引用的时间时刻；2）Moment-guided Dual-path Propagation（MDP）传播策略，通过时刻中心记忆机制在相关和不相关帧上进行训练；3）Object-level Selective Supervision（OSS）对象级过滤策略，只监督与表达式时间对齐的对象。

Result: 在具有挑战性的MeViS基准测试中实现了最先进的性能，特别是在涉及多样化表达式的复杂场景中表现出色。

Conclusion: 通过时刻感知的监督和选择性训练策略，SAMDWICH显著增强了视频-文本对齐和参考理解能力，有效解决了语义错位问题。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++是一个协作学习框架，通过在异构架构、不同训练时刻和多重参数采样中提取交叉信息，实现了高精度边缘检测与低计算成本的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习边缘检测方法计算成本高、模型复杂的问题，使其能够在资源受限设备上部署，同时保持高精度。

Method: 提出协作学习框架，利用异构架构、训练时刻多样性和参数采样的交叉信息，从集成学习角度增强特征提取能力。

Result: 在BSDS500、NYUD和Multicue数据集上实验证明，该方法在定量和定性评估中都优于现有方法，并提供不同计算需求的多个模型版本。

Conclusion: PEdger++成功实现了边缘检测精度与计算效率的平衡，具有很好的适应性，适合在不同资源约束条件下部署。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 本文提出了一个新颖的多分辨率多模态微表情数据集，使用同步RGB和事件相机在可变光照条件下记录，并通过脉冲神经网络和条件变分自编码器展示了事件数据在微表情识别和帧重建方面的优势。


<details>
  <summary>Details</summary>
Motivation: 微表情分析在人机交互和驾驶员监控系统等领域有重要应用，但仅依赖RGB相机难以准确捕捉细微快速的面部运动，存在时间分辨率限制和运动模糊问题。事件相机具有微秒级精度、高动态范围和低延迟的优势，但缺乏公开的事件基Action Units数据集。

Method: 构建了同步RGB和事件相机的多模态微表情数据集，在可变光照条件下记录。使用脉冲神经网络进行Action Unit分类，使用条件变分自编码器进行帧重建任务。

Result: 事件数据在Action Unit分类中达到51.23%准确率（RGB仅为23.12%），高分辨率事件输入的帧重建达到SSIM=0.8513和PSNR=26.89dB的优秀指标。

Conclusion: 事件基数据在微表情识别和帧重建方面表现出色，证明了事件相机在微表情分析中的潜力和应用价值。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON是首个基于生成式多模态大语言模型的产品表示学习框架，通过引导式专家混合模块、核心语义区域检测和专业化负采样策略，解决了产品多模态表示学习的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有判别式双流架构难以建模产品多图像与文本的多对一对齐关系，而生成式MLLMs在产品表示学习方面具有巨大潜力但面临多模态建模缺失、背景噪声干扰和评估标准缺乏等挑战。

Method: 提出MOON模型：(1)引导式MoE模块进行多模态和方面特定内容建模；(2)检测产品图像核心语义区域以减少背景噪声干扰；(3)引入专业化负采样策略增加负样本难度和多样性；(4)发布大规模多模态基准MBE。

Result: 在零样本设置下，MOON在新建基准和公共数据集上均表现出竞争力，在跨模态检索、产品分类和属性预测等下游任务中展现强大泛化能力。案例研究和可视化验证了模型有效性。

Conclusion: MOON成功证明了生成式MLLMs在产品表示学习中的潜力，通过创新的多模态建模方法和噪声处理技术，为产品理解任务提供了有效的解决方案。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive是一个针对动态驾驶场景的实例感知3D高斯泼溅框架，通过SAM生成的掩码作为伪真值指导2D特征学习，在3D层面引入正则化隐式编码实例身份，无需数据预处理即可实现动态开放世界驾驶场景的3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有背景元素统一为单一表示，阻碍了实例级理解和灵活场景编辑；现有方法主要针对室内场景，不适用于室外驾驶场景；需要依赖预处理实例ID或复杂管道来映射连续特征到离散身份。

Method: 使用SAM生成的掩码作为伪真值，通过对比损失和伪监督目标指导2D特征学习；在3D层面引入正则化隐式编码实例身份，通过体素损失强制一致性；使用轻量级静态码本桥接连续特征和离散身份。

Result: 定量和定性实验证明了InstDrive的有效性，据作者所知，这是第一个在动态开放世界驾驶场景中实现3D实例分割的框架。

Conclusion: InstDrive框架成功解决了动态驾驶场景中实例级3D重建的挑战，无需复杂预处理即可实现实例感知的3D高斯泼溅重建。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出WiseLVAM全自动框架，通过轮廓感知扫描线放置和AMM模式增强，实现左心室线性测量的自动化和临床可靠性


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法直接从B超图像估计标志点，即使沿心室壁的小偏移也会导致显著测量误差，降低临床可靠性

Method: 提出轮廓感知扫描线放置方法，使用弱监督B超标志点检测器估计左心室轮廓，推断左心室长轴和基底水平；然后构建WiseLVAM框架，结合B超图像的结构感知和AMM模式的运动感知

Result: 实现了全自动的左心室线性测量，同时保持手动适应性，提高了测量的鲁棒性和准确性

Conclusion: WiseLVAM框架有潜力为常规临床应用提供实用解决方案，结合结构感知和运动感知来增强测量的可靠性

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU是一个结合频域特征提取和量子检索增强生成的医学视觉问答模型，在VQA-RAD数据集上表现出色，特别擅长处理需要图像-文本推理的复杂临床问题。


<details>
  <summary>Details</summary>
Motivation: 解决需要同时理解图像和文本的复杂临床问题仍然是医疗AI的主要挑战，现有模型在处理这类多模态推理任务时存在性能瓶颈。

Method: 使用快速傅里叶变换将医学图像和文本特征转换到频域以提取更有意义的信息并过滤噪声，结合量子启发的检索系统从外部医学知识源获取相关事实，并将这些信息与频域特征融合进行推理。

Result: 在VQA-RAD数据集上的评估显示，Q-FSRU模型超越了之前的模型，特别是在需要图像-文本推理的复杂病例上表现优异。

Conclusion: 频域处理和量子信息检索的结合不仅提升了模型性能，还增强了可解释性，为构建智能、透明且实用的医生辅助AI工具提供了有前景的途径。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一个基于视频检索增强的运动生成框架，通过从大规模视频数据库中检索相关2D人体运动信号来解决运动大语言模型的数据稀缺问题，显著提升了文本输入条件下的3D运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 运动大语言模型由于标注数据有限而面临严重的域外/词汇外问题，需要利用大规模野外视频数据库来增强3D运动生成能力。

Method: 设计了Gemini Motion Video Retriever机制进行有效的运动中心视频检索，以及Motion-centric Dual-alignment DPO Trainer来缓解检索结果不佳导致的错误传播问题。

Result: 实验结果表明，VimoRAG显著提升了仅使用文本输入的运动大语言模型的性能。

Conclusion: 视频检索增强方法有效解决了运动生成中的数据稀缺问题，为运动大语言模型提供了新的性能提升途径。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 自动化对象检测器性能评估的新方法PCR，通过分析NMS前后检测框的空间一致性和可靠性来估计性能，无需手动标注。


<details>
  <summary>Details</summary>
Motivation: 解决对象检测器在实际应用中性能评估依赖成本高明的手动标注问题，提高评估效率。

Method: 提出PCR指标，聚焦于NMS前后检测框的空间一致性和重叠框的信心度可靠性，并构建包含不同严重程度图像腐化的元数据集。

Result: 实验结果显示PCR比现有自动评估方法更准确地估计检测性能，构建的元数据集覆盖更广泛的检测性能范围。

Conclusion: 该研究提供了一种高效、可靠的对象检测器自动评估方案，有力促进实际应用中的模型部署和评测。

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式视角解决事件边界检测问题，能够产生多样化的合理边界预测。


<details>
  <summary>Details</summary>
Motivation: 传统的事件边界检测方法专注于确定性预测，忽略了事件边界主观性带来的解决方案多样性问题。

Method: 提出扩散模型DiffGEBD，通过时间自相似性编码相邻帧的相关变化，然后以编码特征为条件迭代地将随机噪声解码为合理的事件边界，使用分类器自由引导控制多样性。

Result: 在Kinetics-GEBD和TAPOS两个标准基准测试中取得了强劲性能，能够生成多样且合理的事件边界。

Conclusion: 扩散模型为通用事件边界检测提供了一种有效的生成式解决方案，能够处理事件边界的主观多样性问题。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 提出基于多阶段卷积神经网络的方法，通过配对高低精度激光扫描仪数据来量化误差模式，结合传统几何处理和神经网络优化，显著提升低端设备在粗糙室内环境中的测量精度。


<details>
  <summary>Details</summary>
Motivation: 解决高低端激光扫描仪在粗糙室内环境中因设备限制和环境因素导致的位置误差问题，为高精度几何建模和翻新提供更准确的空间测量数据。

Method: 使用高精度扫描仪作为参考，与低精度扫描仪在相同环境中配对测量，建立测量差异与空间分布的统计关系，结合传统几何处理和神经网络细化开发校正框架。

Result: 在粗糙室内房间数据集上，均方误差降低超过70%，峰值信噪比提升约6分贝，使低端设备无需硬件改造即可接近高端设备的测量精度水平。

Conclusion: 该方法成功将系统误差量化转化为监督学习问题，在保持关键几何特征的同时实现精确校正，为低成本高精度测量提供了有效解决方案。

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 该论文提出了一个理论框架来分析扩散模型中的量化误差传播，并基于此提出了时间步感知的累积误差补偿方案，显著提升了低精度扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成方面取得了突破性进展，但其迭代去噪过程计算密集，部署面临挑战。后训练量化(PTQ)虽然能加速采样，但迭代特性导致量化误差在生成过程中逐步累积，影响输出质量。

Method: 1. 开发理论框架，数学化建模扩散模型中的误差传播，推导每步量化误差传播方程并建立首个累积误差的闭式解。2. 基于理论提出时间步感知的累积误差补偿方案。

Result: 在多个图像数据集上的广泛实验表明，该补偿策略有效缓解了误差传播，显著提升了现有PTQ方法，在低精度扩散模型上达到了最先进的性能。

Conclusion: 提出的理论框架和误差补偿方案为解决扩散模型量化中的误差累积问题提供了有效解决方案，为低精度扩散模型的实用部署奠定了基础。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med是一个针对有限3D医学数据（如CT扫描和放射学报告）的视觉语言预训练框架，通过结合单模态自监督学习、新型TriBERT语言编码器和分层对比学习，在仅使用38,875对数据的情况下实现了优异的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域中体积模态（如CT扫描）与文本配对数据的大规模收集困难且耗时，这限制了视觉语言模型在下游任务中的性能表现。

Method: 提出VELVET-Med框架：1）在VLP中融入单模态自监督学习；2）设计TriBERT语言编码器学习多级文本语义；3）采用分层对比学习捕获多级视觉-语言对应关系。

Result: 仅使用38,875对扫描-报告数据，在3D分割、跨模态检索、视觉问答和报告生成等多个下游任务中达到最先进性能。

Conclusion: 该方法通过有效的预训练目标和模型架构设计，在有限数据条件下成功挖掘了体积医学图像和临床叙述中的丰富空间和语义关系，提高了编码器的泛化能力和可迁移性。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3是一个端到端的多模态推理框架，通过动态视觉工具交互和交错式视觉语言推理链，显著提升多模态大语言模型的长链思维能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在长链思维推理方面能力不足，受OpenAI o3模型启发，需要开发能够模拟人类"图像思维"的迭代视觉转换和语言推理能力

Method: 提出监督微调框架，集成动态工具交互（裁剪、缩放、重用），通过"观察-推理-行动"循环生成高质量交错视觉语言推理链，创建TWI-Tools-146K数据集

Result: 在多个基准测试中表现优异，超越现有方法。重用和放大原图显著改善视觉推理和细粒度感知，基于精确视觉定位的图像裁剪有效聚焦关键区域

Conclusion: Simple o3建立了强大且计算高效的多模态推理范式，首次深入分析了不同交错推理策略对模型性能的影响

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit是一种混合虚拟试穿方法，通过两阶段流程解决现有方法难以保持服装细节的问题，第一阶段通过流场对齐服装，第二阶段通过保真度保持模块合成最终结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的无扭曲虚拟试穿方法虽然改善了感知质量，但无法保持服装的精细细节（如logo和印刷文本），这对品牌完整性和客户信任至关重要。

Method: 提出两阶段混合管道：第一阶段使用学习流场将目标服装与人像对齐；第二阶段通过保真度保持试穿模块，使用保留区域输入和修复掩码来合成最终输出，只在必要区域重新生成。

Result: 广泛的定性结果显示DualFit实现了视觉无缝的试穿效果，同时忠实地保持了高频服装细节，在重建准确性和感知真实性之间取得了有效平衡。

Conclusion: DualFit通过混合方法成功解决了虚拟试穿中服装细节保持的问题，为在线时尚零售提供了更可靠的解决方案。

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个三层次量化感知防御框架，通过特征错位惩罚、梯度感知不一致惩罚和联合量化感知训练，有效降低QNN中基于补丁的对抗攻击在未知量化位宽间的迁移成功率超过40%。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络(QNNs)虽然在计算和内存效率上优势明显，且能削弱传统像素级攻击，但对基于补丁的局部高显著性对抗攻击的跨位宽迁移性防御不足。现有防御方法要么过拟合固定量化设置，要么无法解决这种跨位宽泛化漏洞。

Method: TriQDef包含三个核心组件：1)特征错位惩罚(FDP)通过惩罚中间表示的感知相似性来强制语义不一致；2)梯度感知不一致惩罚(GPDP)通过边缘IoU和HOG余弦度量最小化结构性和方向性一致性，显式错位不同位宽的输入梯度；3)联合量化感知训练协议，在多量化级别共享权重训练方案中统一这些惩罚。

Result: 在CIFAR-10和ImageNet上的大量实验表明，TriQDef在未见过的补丁和量化组合上将攻击成功率(ASR)降低了40%以上，同时保持了高清洁准确率。

Conclusion: 研究强调了破坏语义和感知梯度对齐对于减轻QNN中补丁迁移性的重要性，TriQDef框架有效解决了跨位宽对抗攻击迁移问题。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 提出一种针对视觉语言模型的微调方法，在保持预训练模型通用能力的同时优化细粒度开放集视觉检索性能，解决了灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 大规模对比预训练的视觉语言模型在细粒度开放集视觉检索任务中表现不佳，需要领域特定的微调，但传统微调会导致灾难性遗忘，丧失模型的通用视觉和跨模态能力

Method: 受持续学习启发，系统分析标准正则化技术，提出高效的组合策略；关注验证集设计和超参数调优等关键方面，确保可复现性和跨数据集泛化能力

Result: 在细粒度和粗粒度图像-图像、图像-文本检索基准上取得优异结果，无需使用文本数据或原始文本编码器即可保持视觉-文本对齐

Conclusion: 该方法有效平衡了领域适应和预训练知识保留，为视觉语言模型的细粒度微调提供了实用解决方案

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR是一种用于心脏电影MRI重建的双分支隐式神经表示方法，通过在k空间坐标位置嵌入和局部多尺度特征表示之间进行交叉分支交互，实现了比基线模型更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的INR方法主要关注基于坐标的位置嵌入来学习映射，但忽略了目标点及其邻域上下文特征表示的重要性，这限制了心脏电影MRI重建的质量。

Method: 提出KP-INR双分支方法：一个分支处理k空间坐标的位置嵌入，另一个分支学习该坐标处的局部多尺度k空间特征表示，通过交叉分支交互来近似目标k空间值。

Result: 在CMRxRecon2024数据集上的实验证实，KP-INR相比基线模型具有改进的性能，特别是在具有挑战性的笛卡尔k空间数据上表现出色。

Conclusion: KP-INR通过结合位置嵌入和局部特征表示的双分支方法，在心脏电影MRI重建领域展现出巨大潜力，为快速采集技术下的高质量图像恢复提供了有效解决方案。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 提出了FB-Mem方法，通过图像分割量化扩散模型中的局部记忆现象，发现记忆比现有认知更普遍，现有缓解方法效果有限


<details>
  <summary>Details</summary>
Motivation: 现有检测方法只能识别完全复制的记忆，无法量化局部区域的部分记忆和超越特定提示-图像对的记忆模式

Method: 提出基于分割的FB-Mem度量方法，对生成图像中的记忆区域进行分类和量化，并采用聚类方法进行更强的缓解

Result: 发现记忆现象比之前理解的更普遍：单提示生成可能关联多个训练图像簇，现有缓解方法无法消除局部记忆（特别是前景区域）

Conclusion: 建立了有效的扩散模型记忆测量框架，证明了当前缓解方法的不足，提出了基于聚类的更强缓解方法

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk是一个新颖的情感说话头合成框架，通过VAE生成3D面部标志点，结合情感标签嵌入和ResNet-based LDM产生情感标志点，再通过tri-plane attention NeRF合成高度真实的情感说话头，在情感准确性、可控性和身份保持方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在唇同步和图像质量方面表现出色，但在生成准确可控的情感表情同时保持主体身份方面存在不足，这限制了人工社交智能的发展。

Method: 使用变分自编码器(VAE)从驱动音频生成3D面部标志点，通过ResNet-based标志点变形模型(LDM)将情感标签嵌入与标志点拼接，产生情感标志点。这些标志点和面部混合形状系数共同条件化新颖的三平面注意力神经辐射场(NeRF)来合成情感说话头。

Result: 大量实验表明，RealTalk在情感准确性、可控性和身份保持方面优于现有方法。

Conclusion: RealTalk框架显著提升了情感说话头合成的性能，推动了社交智能AI系统的发展。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse是一个基于提示的RF信号仿真框架，通过语言引导生成室内场景和人体运动，使用相位相干射线追踪模拟真实RF信号，解决了RF感知数据采集难题。


<details>
  <summary>Details</summary>
Motivation: RF传感作为隐私保护的室内感知替代方案面临高质量数据采集困难，特别是在动态多样的室内环境中。

Method: 提出语言引导的4D世界生成器，包括状态感知因果变换器生成空间约束下的人体运动，以及相位相干射线追踪模拟器生成准确连贯的RF信号。

Result: 实验证明在条件人体运动生成方面有效，相位相干性成功应用于波束成形和呼吸监测。两个案例研究显示WaveVerse首次实现RF成像数据生成，在数据有限和充足场景下均获得性能提升。

Conclusion: WaveVerse提供了一个可扩展的RF信号仿真框架，解决了RF感知数据采集的核心挑战，为RF成像和人体活动识别等应用提供了有效的数据生成解决方案。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 提出了一种统一的特征提升方法，将多视角图像特征（如DINO、CLIP）附加到基于splat的3D表示中，通过稀疏线性逆问题求解，并引入正则化策略提升语义保真度


<details>
  <summary>Details</summary>
Motivation: 解决多视角图像特征提升到3D表示中的不一致性问题，为3D场景理解提供丰富的通用属性描述符

Method: 将特征提升问题表述为稀疏线性逆问题，引入Tikhonov Guidance正则化和Post-Lifting Aggregation聚类过滤策略

Result: 在开放词汇3D分割基准测试中达到最先进性能，优于基于训练、分组和启发式的前沿方法，且能在几分钟内生成提升特征

Conclusion: 该方法提供了一种高效、可证明最优误差上界的特征提升解决方案，显著提升了3D场景理解的语义表示质量

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 基于深度学习的YOLOv11优化模型，用于棉花病害检测，通过C2PSA模块、动态类别加权和改进数据增强技术，显著提升小目标检测精度和田间性能，mAP50达到0.820，推理速度158FPS


<details>
  <summary>Details</summary>
Motivation: 解决棉花病害检测中的三个关键挑战：早期斑点检测精度低（5mm²以下斑点漏检率35%）、田间条件性能下降（准确率下降25%）、多病害场景错误率高（34.7%）

Method: 采用C2PSA模块增强小目标特征提取，动态类别加权处理样本不平衡，Mosaic-MixUp缩放改进数据增强，基于YOLOv11进行深度优化

Result: 在4,078张图像数据集上测试：mAP50达到0.820（提升8.0%），mAP50-95达到0.705（提升10.5%），推理速度158 FPS

Conclusion: 开发的移动部署系统可实现实时病害监测和精准施药，为农业应用提供有效的智能监测解决方案

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 提出了一种结合生成网络和物理信息神经模拟的生成式神经物理框架，用于快速、高保真的3D超声计算机断层扫描，解决了传统射线重建方法在强散射情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 超声计算机断层扫描（USCT）是一种无辐射、高分辨率的成像方式，但在肌肉骨骼成像中受到传统射线重建方法忽略强散射的限制。

Method: 通过从少量跨模态图像中学习超声波传播的紧凑替代模型，将波动建模的准确性与深度学习的效率和稳定性相结合。

Result: 在合成和体内数据（乳房、手臂、腿部）上，在10分钟内重建了组织参数的3D图谱，对肌肉和骨骼的生物力学特性敏感，分辨率与MRI相当。

Conclusion: 通过克服强散射状态下的计算瓶颈，该方法将USCT推向肌肉骨骼疾病的常规临床评估。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 提出视觉动作提示(VAP)，通过将动作渲染为视觉骨架表示，在保持跨域动态可迁移性的同时实现复杂高自由度交互的精确视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有动作驱动视频生成方法面临精度与泛化性的权衡：文本、原始动作或粗糙掩码方法缺乏精度，而智能体中心动作信号虽然精确但缺乏跨域迁移能力。

Method: 将动作渲染为精确的视觉提示(视觉骨架)作为领域无关表示，从人-物交互(HOI)和灵巧机器人操作数据构建骨架，通过轻量级微调将视觉骨架集成到预训练视频生成模型中。

Result: 在EgoVid、RT-1和DROID数据集上的实验证明了该方法的有效性，能够精确控制复杂交互动作同时保持跨域动态学习能力。

Conclusion: 视觉动作提示提供了一种平衡动作精度和动态可迁移性的统一动作表示方法，为复杂高自由度交互的视频生成提供了有效解决方案。

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [51] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 提出了WXSOD数据集和WFANet方法，用于解决复杂天气条件下的显著目标检测问题，在17种SOD方法中表现最优


<details>
  <summary>Details</summary>
Motivation: 现有SOD方法在自然场景中表现良好，但缺乏针对天气噪声影响的研究，主要原因是缺少像素级标注的数据集

Method: 构建包含14,945张RGB图像的WXSOD数据集，提出两分支网络WFANet：天气预测分支挖掘天气相关特征，显著性检测分支融合语义特征和天气特征

Result: WFANet在WXSOD数据集上实现了优越性能，超越了17种对比方法

Conclusion: WXSOD数据集填补了天气噪声SOD研究的空白，WFANet方法有效提升了复杂天气条件下的检测性能

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [52] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5在多模态空间智能方面取得显著进步但仍不及人类水平，研究发现专有模型在最困难问题上并无决定性优势


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进多模态模型在空间理解和推理方面的能力，这是实现通用人工智能的关键能力

Method: 提出统一的空间任务分类法，在8个关键基准测试上评估最先进的专有和开源模型，消耗超过10亿个token，并进行定性评估

Result: GPT-5展现出前所未有的空间智能强度，但在广泛任务中仍落后于人类表现；专有模型在最困难问题上没有明显优势

Conclusion: 多模态模型在空间智能方面仍有显著局限，需要进一步研究来弥合与人类能力的差距

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [53] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: 提出SCTR框架，通过超像素单元和不对称低秩张量分解，解决了传统低秩张量表示在空间变化和离散网格限制方面的问题，在多个数据集上实现3-5 dB的PSNR提升


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量表示方法存在两个关键局限：(1)假设整体数据低秩，但现实场景中存在显著空间变化；(2)局限于离散网格数据，限制了灵活性和适用性

Method: 使用超像素作为基本建模单元，提出不对称低秩张量分解(ALTF)，其中超像素特定因子矩阵通过共享神经网络和专用头部进行参数化，分离全局模式学习和局部适应

Result: 在多个基准数据集上的广泛实验表明，SCTR在多光谱图像、视频和彩色图像上比现有LRTR方法实现了3-5 dB的PSNR改进

Conclusion: SCTR框架能够连续灵活地建模多维数据，超越了传统基于网格的约束，在表达能力和模型效率之间取得了良好平衡

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [54] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 提出了区域级上下文感知多模态理解(RCMU)任务，通过RCVIT方法增强MLLMs的区域文本信息整合能力，构建了RCMU数据集和RC&P-Bench基准，RC-Qwen2-VL模型在多个任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs主要关注通用视觉理解，忽视了整合对象相关文本上下文的能力，缺乏区域级上下文感知多模态理解能力

Method: 提出Region-level Context-aware Visual Instruction Tuning (RCVIT)方法，将对象信息和边界框坐标整合到模型输入中；构建RCMU大规模视觉指令调优数据集和RC&P-Bench评估基准

Result: RC-Qwen2-VL模型在多个RCMU任务中表现优异，在多模态RAG和个性化对话中成功应用

Conclusion: 该方法有效提升了MLLMs的区域级上下文感知能力，为多模态理解提供了新的技术路径和评估标准

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [55] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 提出SNNSIR，一种完全脉冲驱动的脉冲神经网络用于立体图像恢复，通过脉冲残差基本块、立体卷积调制和立体交叉注意力模块，在保持低功耗的同时实现竞争性恢复性能


<details>
  <summary>Details</summary>
Motivation: 现有混合SNN-ANN模型仍依赖浮点运算，与SNN的二进制事件驱动特性不兼容。需要开发完全脉冲驱动的架构来实现低功耗和硬件友好的立体图像恢复

Method: 1) 轻量级脉冲残差基本块(SRBB)增强信息流；2) 脉冲立体卷积调制(SSCM)通过元素乘法和跨视图感知调制简化非线性；3) 脉冲立体交叉注意力(SSCA)实现双向特征交互

Result: 在雨纹去除、雨滴去除、低光增强和超分辨率等多个立体图像恢复任务上取得竞争性性能，同时显著降低计算开销

Conclusion: SNNSIR展示了实时低功耗立体视觉应用的潜力，为完全脉冲驱动的立体图像恢复提供了有效解决方案

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [56] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对自动驾驶硬件平台的动态可适应语义分割网络，通过三层控制机制和贝叶斯优化来实现计算资源的高效分配和任务特定优化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶平台面临多样化的驾驶场景和硬件资源限制，需要在嵌入式设备上部署时充分考虑计算成本，根据硬件计算能力和特定场景需求定制语义分割网络。

Method: 采用三层控制机制（宽度乘数、分类器深度、分类器核）实现细粒度模型组件控制，结合贝叶斯优化和代理模型在有限计算预算下高效探索超参数空间。

Result: 实现了任务特定学习适应(TSLA)，能够根据不同的自动驾驶任务生成定制化配置，最大化计算容量和模型精度，优化硬件利用率。

Conclusion: 该方法能够有效解决自动驾驶场景特定和任务特定的需求，通过自动参数搜索适应不同的计算复杂度和精度要求，为嵌入式自动驾驶平台提供了高效的部署解决方案。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [57] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出CLAIR方法，通过CLIP模型生成噪声伪标签并利用置信度分数进行精炼，设计多种对比损失来编码类感知潜在空间并缓解域差异，学习跨域映射函数来对齐图像特征，增强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型能够轻松为大量未标记数据生成伪标签，无监督零样本跨域图像检索变得不再那么相关，因此转向研究使用CLIP等大型基础模型生成噪声伪标签的弱监督零样本跨域图像检索。

Method: 提出CLAIR方法：1）用CLIP文本和图像特征相似度计算置信度分数来精炼噪声伪标签；2）设计实例间和簇间对比损失编码类感知潜在空间；3）设计域间对比损失缓解域差异；4）学习闭式跨域映射函数，仅使用CLIP文本嵌入将图像特征从一个域投影到另一个域；5）引入可学习提示增强零样本泛化能力。

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上的大量实验表明，CLAIR方法相比现有最先进方法始终表现出优越性能。

Conclusion: CLAIR方法通过精炼噪声伪标签、设计多种对比损失和学习跨域映射函数，有效解决了弱监督零样本跨域图像检索问题，在多个数据集上取得了state-of-the-art性能。

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [58] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 本文提出了一种改进3D高斯泼溅(3DGS)致密化策略的方法，通过边缘感知评分、长轴分割策略和防过拟合技术，在不增加训练或推理开销的情况下提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然实现了实时渲染，但其致密化策略往往导致重建质量不理想，需要从何时致密化、如何致密化以及如何缓解过拟合三个角度进行全面改进。

Method: 提出边缘感知评分来有效选择候选高斯进行分割，引入长轴分割策略减少几何失真，设计恢复感知剪枝、多步更新和生长控制等技术来应对过拟合问题。

Result: 该方法在不增加额外训练或推理开销的情况下提升了渲染保真度，使用更少的高斯数量实现了最先进的性能表现。

Conclusion: 通过系统性的致密化流程改进，该方法显著提升了3D高斯泼溅的重建质量和渲染效果，为实时神经渲染提供了更优的解决方案。

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [59] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 提出基于神经细胞自动机(NCA)的弱监督分割方法NCA-WSS，无需分割标签即可从分类特征图中提取分割掩码，在白细胞显微镜图像分割任务上显著优于现有弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 白细胞检测和分割是医学诊断的关键步骤，但获取大量标注数据耗时且昂贵，需要开发弱监督方法来减少标注需求。

Method: 利用神经细胞自动机(NCA)在分类过程中生成的特征图，无需重新训练即可提取分割掩码，实现弱监督分割。

Result: 在三个白细胞显微镜数据集上评估，NCA-WSS方法显著优于现有的弱监督分割方法。

Conclusion: NCA在弱监督框架下具有分类和分割的双重潜力，为医学图像分析提供了可扩展且高效的解决方案。

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [60] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 将注意力池化机制与神经细胞自动机(NCA)结合，提升显微镜图像分类性能，在保持参数效率和可解释性的同时显著超越现有NCA方法


<details>
  <summary>Details</summary>
Motivation: 神经细胞自动机在图像分类中具有鲁棒性和可解释性优势，但与传统复杂架构存在性能差距，需要提升特征提取能力

Method: 集成注意力池化机制到NCA中，通过关注信息最丰富的区域来优化特征提取

Result: 在8个不同的显微镜图像数据集上验证，显著优于现有NCA方法，与传统轻量级CNN和ViT架构相比性能更好且参数更少

Conclusion: 基于NCA的模型具有作为可解释图像分类替代方案的潜力，注意力池化的集成有效提升了性能

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [61] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive是一种基于多普勒效应的雷达点云时间聚合方法，通过径向移动和动态聚合持续时间来增强点云密度并减少散射，显著提升雷达目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 雷达在自动驾驶中具有长距离检测优势，但点云稀疏性（尤其在远距离）影响检测精度。现有时间聚合方法会引入动态物体散射，降低检测性能。

Method: 提出DoppDrive方法：1）根据多普勒动态分量径向移动历史帧点云以消除径向散射；2）基于多普勒和角度为每个点分配独特聚合持续时间以减少切向散射；3）作为检测前的点云密度增强步骤，可与任何检测器兼容。

Result: 该方法在各种检测器和数据集上显著提高了目标检测性能，有效解决了雷达点云稀疏和散射问题。

Conclusion: DoppDrive通过多普勒驱动的时间聚合策略，在增强雷达点云密度的同时最小化散射，为雷达目标检测提供了有效的预处理解决方案。

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [62] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 提出了一种几何感知的深度学习框架，从单视角RGB视频中联合去除头戴显示器遮挡并重建完整3D面部几何


<details>
  <summary>Details</summary>
Motivation: 头戴显示器遮挡了用户上半部面部，影响视频录制和社交XR应用中的面部表情与眼神交流体验

Method: 集成GAN视频修复网络（由密集面部标志点和无遮挡参考帧引导）和SynergyNet模块回归3DMM参数，结合密集标志点优化

Result: 成功从RGB面部视频中去除HMD遮挡，保持面部身份真实感，生成逼真的3D面部几何输出，在不同标志点密度下保持鲁棒性

Conclusion: 该框架能有效解决HMD遮挡问题，为社交XR应用提供高质量的面部重建解决方案

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [63] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出SDD检测器，通过重建学习在细粒度视觉层面对齐伪造痕迹和语义概念空间，利用预训练视觉语言模型的概念知识来提升伪造图像检测性能


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，确保数字媒体可信度需要强大的伪造检测能力。现有方法中伪造空间与语义概念空间的不对齐问题限制了检测性能

Method: 提出语义差异感知检测器(SDD)：1)语义标记采样模块减少与伪造痕迹和语义概念无关的特征干扰；2)基于视觉重建的概念级伪造差异学习模块；3)低级伪造特征增强模块整合学习到的概念级差异

Result: 在两个标准图像伪造数据集上的实验表明，SDD相比现有方法取得了更优越的结果

Conclusion: SDD通过有效对齐伪造痕迹和语义概念空间，显著提升了伪造图像检测性能，为解决伪造检测中的空间不对齐问题提供了有效方案

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [64] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat是一个即插即用的任务驱动特征增强模块，专门针对水下目标检测任务，通过端到端训练的多尺度特征增强网络提升检测性能，在保持实时处理速度的同时实现了最先进的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 水下环境的严重图像退化会影响目标检测模型的性能，传统的图像增强方法通常没有针对下游检测任务进行优化，需要一种专门的任务驱动特征增强方法。

Method: 提出AquaFeat模块，集成多尺度特征增强网络，与检测器的损失函数进行端到端训练，确保增强过程明确指导以优化与检测任务最相关的特征。

Result: 在YOLOv8m上集成AquaFeat后，在挑战性水下数据集上达到最先进的精度(0.877)和召回率(0.624)，以及竞争力的mAP分数(mAP@0.5为0.677，mAP@[0.5:0.95]为0.421)，处理速度为46.5 FPS。

Conclusion: AquaFeat提供了一个有效且计算效率高的解决方案，适用于海洋生态系统监测和基础设施检查等实际应用，在保持实用处理速度的同时实现了显著的准确性提升。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [65] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: MBMamba是一个基于Mamba架构的图像去模糊网络，通过内存缓冲机制保留历史信息进行特征融合，并引入Ising启发的正则化损失来保持图像结构一致性，在保持原始架构的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在图像去模糊中展现出潜力，但其flatten-and-scan策略会导致局部像素遗忘和通道冗余，现有改进方法增加了计算复杂度影响实时性能，需要在不改变原始架构的情况下提升2D空间信息聚合能力。

Method: 提出MBMamba网络：1）设计内存缓冲机制保留历史信息用于后续融合，可靠建模相邻特征相关性；2）引入Ising启发的正则化损失，模拟物理系统像素间"相互吸引"的能量最小化，保持图像结构和连贯性。

Result: 实验结果表明，该方法在广泛使用的基准测试上优于最先进的方法。

Conclusion: MBMamba通过内存缓冲和Ising正则化损失，在不改变Mamba原始架构的情况下有效解决了局部像素遗忘和通道冗余问题，提升了图像去模糊性能。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [66] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出EgoLoc方法，用于在自我中心视频中精确定位手与物体接触和分离的时间点，无需对象掩码和动作分类标注，实现了零样本的时序交互定位。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互行为模式（如何交互），但对手与目标物体接触和分离的关键时刻定位（何时交互）这一更精细的问题研究不足，这对混合现实和机器人运动规划至关重要。

Method: 提出EgoLoc方法，通过手部动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步优化。

Result: 在公共数据集和新基准测试上的综合实验表明，EgoLoc能够实现可信的时序交互定位，并在自我中心视觉和机器人操作任务中有效促进多个下游应用。

Conclusion: EgoLoc消除了对物体掩码和动词-名词分类的需求，实现了可推广的零样本实现，为自我中心视频中的精细时序交互定位提供了有效解决方案。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [67] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 提出一种简单的视觉离线强化学习方法，通过数据增强和扩散模型生成合成训练数据来改善泛化性能，在连续和离散动作空间中都显著减少了泛化差距。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习使用预收集数据集训练智能体，但视觉数据中的噪声、干扰和伪相关等问题导致策略泛化能力差，特别是在训练数据多样性不足时容易过拟合。

Method: 采用两步法：首先对原始离线数据进行增强以增加多样性，然后使用扩散模型在潜在空间中生成额外的合成训练数据，无需改变现有模型无关离线RL算法。

Result: 在Visual D4RL（连续动作空间）和Procgen（离散动作空间）上的实验表明，该方法显著提高了泛化能力，增加了训练数据多样性并减少了测试时的泛化差距，同时保持计算效率。

Conclusion: 该方法为生成合成数据训练更通用智能体提供了有前景的方向，能够有效解决视觉离线RL中的泛化挑战。

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [68] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 提出了IPGPhormer框架，通过图Transformer结构同时捕获肿瘤微环境的长程空间关系和局部上下文依赖，在生存分析任务中实现了更好的预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡长程空间关系建模与局部上下文依赖，且缺乏内在可解释性，限制了病理图像生存分析的临床实用性。

Method: 基于图Transformer架构，在组织和细胞层面提供无需后处理标注的可解释性，能够建模肿瘤微环境特征及其空间依赖关系。

Result: 在四个公开基准数据集上的综合评估表明，IPGPhormer在预测准确性和可解释性方面均优于最先进方法。

Conclusion: IPGPhormer为癌症预后评估提供了一个有前景的工具，为病理学中更可靠和可解释的决策支持系统铺平了道路。

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [69] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 提出ViT-EnsembleAttack方法，通过对ViT模型进行对抗性增强（多头丢弃、注意力分数缩放、MLP特征混合）来提升集成攻击的迁移性，并引入自动重加权和步长放大模块，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有集成攻击研究主要关注优化集成权重或路径，忽略了通过增强集成模型本身来提升对抗迁移性，且ViT模型的集成攻击研究较少。

Method: 对每个ViT代理模型应用三种对抗增强策略：多头丢弃、注意力分数缩放、MLP特征混合，使用贝叶斯优化参数，并集成增强后的模型生成对抗样本，还包含自动重加权和步长放大模块。

Result: 大量实验表明，ViT-EnsembleAttack显著提升了ViT集成攻击的对抗迁移性，大幅超越现有方法。

Conclusion: 该方法通过模型对抗增强有效提升了集成攻击的迁移性能，为ViT模型的对抗攻击提供了新的解决方案。

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [70] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT是一个通过大型语言模型分解复杂文本指令来提升文本到图像生成模型性能的框架，在LongBench-T2I基准测试中显著改善了图像生成质量


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂长文本指令时存在困难，经常无法准确渲染细节、空间关系和特定约束，需要更好的方法来理解和执行复杂指令

Method: DeCoT框架分为两个核心阶段：1）复杂指令分解和语义增强，使用LLM将原始指令分解为结构化语义单元；2）多阶段提示集成和自适应生成，将这些单元转换为适合T2I模型的分层或优化提示

Result: 在LongBench-T2I数据集上，DeCoT显著提升了主流T2I模型的性能，特别是在"文本"和"构图"等挑战性方面。与Infinity-8B集成时平均得分3.52，优于基线3.44

Conclusion: DeCoT有效弥合了用户意图与T2I模型需求之间的差距，实现了更忠实和准确的图像生成，消融研究确认了各组件的重要性

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [71] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP是一个联邦学习框架，利用CLIP的多尺度视觉特征和客户端特定风格统计，生成上下文感知的提示词，在保护数据隐私的同时提升分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法仅使用最终层特征，忽略了丰富的多尺度视觉线索和客户端数据的领域特定风格变化，限制了模型的泛化性能。

Method: 从CLIP视觉编码器提取低、中、高层特征，结合客户端批处理统计的风格指标，融合视觉细节与文本上下文生成鲁棒且非冗余的提示词token。

Result: 在多个图像分类数据集上的实验表明，FedCSAP在准确性和泛化能力方面均优于现有的联邦提示学习方法。

Conclusion: FedCSAP通过有效利用多尺度视觉特征和客户端风格信息，在联邦学习框架下实现了更好的分类性能和跨域泛化能力。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [72] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR是一种无需微调的推理时策略，通过生成多角度描述来增强大型视觉语言模型的上下文理解能力，在复杂视觉推理任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在需要深度上下文理解、多角度分析或细节识别的复杂视觉推理任务中表现有限，主要依赖于单次图像编码和提示，难以充分捕捉细微的视觉信息

Method: 三阶段方法：1) 生成N个多样互补的描述或初步推理路径；2) 智能整合这些描述与原始问题构建上下文增强提示；3) 使用增强提示指导最终深度推理和答案生成

Result: 在GQA、VQA-CP v2和ScienceQA等挑战性VQA数据集上一致优于基线方法，定量结果显示准确性显著提升，特别是在需要鲁棒上下文理解的任务上，人工评估也确认了生成答案的连贯性和完整性改进

Conclusion: 通过利用LVLMs固有的生成能力来丰富输入上下文，可以有效释放其在复杂多模态任务中的潜在推理潜力，无需修改模型参数即可实现性能提升

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [73] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD是一个专为自动驾驶设计的视觉语言框架，通过引入初步场景交互和专家适配器，显著提升了现有VLM在驾驶推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在车载多视角图像和场景推理文本上微调VLM，但缺乏自动驾驶所需的整体细致场景识别和强大空间感知能力，特别是在复杂情况下。

Method: 提出LMAD框架，模拟现代端到端驾驶范式，结合全面场景理解和任务专业化结构。引入初步场景交互和专门专家适配器，在相同驾驶任务结构中更好地对齐VLM与自动驾驶场景。

Result: 在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLM在驾驶推理任务中的性能。

Conclusion: LMAD为可解释自动驾驶设立了新标准，完全兼容现有VLM并无缝集成规划导向的驾驶系统。

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [74] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 提出了S5框架，首个面向遥感半监督语义分割的可扩展解决方案，通过大规模无标签数据预训练和MoE微调方法，在多个遥感基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有半监督语义分割研究依赖小规模数据集和模型，限制了实际应用。遥感领域存在大量未充分利用的无标签地球观测数据，需要开发可扩展框架来充分利用这些数据

Method: 1) 构建RS4P-1M数据集，整合熵基过滤和多样性扩展的数据选择策略；2) 在不同规模RS基础模型上进行大规模预训练；3) 采用基于Mixture-of-Experts的多数据集微调方法，用更少参数适应多个遥感基准

Result: 在土地覆盖分割和物体检测任务上显著提升性能，在所有基准测试中达到最先进水平，证明了扩展半监督学习在遥感应用中的可行性

Conclusion: S5框架成功解锁了大规模无标签遥感数据的潜力，通过可扩展的半监督学习方法显著提升了遥感基础模型的性能和泛化能力，为遥感分析提供了实用的解决方案

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [75] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: 提出了SRMA-Mamba网络，通过整合空间解剖Mamba模块和空间反向注意力模块，实现了对肝硬化MRI体积数据的精确三维病理肝脏分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肝硬化早期检测对慢性肝病预后至关重要，但现有方法未能充分利用MRI体积数据中的空间解剖细节，限制了临床效果和可解释性。

Method: 设计了SRMA-Mamba网络，包含SABMamba模块（在肝硬化组织内选择性Mamba扫描并整合三平面解剖信息）和SRMA模块（利用粗分割图和分层编码特征逐步细化肝硬化细节）。

Result: 大量实验表明SRMA-Mamba超越了最先进方法，在3D病理肝脏分割中表现出卓越性能。

Conclusion: SRMA-Mamba通过有效建模MRI体积中的空间解剖关系，为肝硬化精确分割提供了有效的解决方案，代码已开源。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [76] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN是一个文本到动态全景场景生成框架，通过双分支生成模型和几何对齐重建模型，实现了360度沉浸式4D场景的生成，解决了现有方法在动态全景生成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR技术的快速发展，对高质量沉浸式动态场景的需求日益增长。现有生成工作主要集中在静态场景或窄视角动态场景，无法提供真正的360度全视角沉浸体验。

Method: 1) 双分支生成模型：全景分支负责全局视图生成，透视分支负责局部视图生成，通过双向交叉注意力机制进行信息交换；2) 几何对齐重建模型：基于3D高斯泼溅，通过度量深度图对齐时空点云，使用估计位姿初始化场景相机。

Result: 大量实验证明了所提出设计的有效性，TiP4GEN在生成视觉吸引人且运动连贯的动态全景场景方面表现出优越性。

Conclusion: TiP4GEN框架成功实现了文本到动态全景场景的生成，提供了细粒度内容控制和几何一致性的全景4D场景，为沉浸式虚拟环境创建提供了有效解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [77] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 通过比较生物和人工视觉系统对视觉幻觉的响应，揭示两者在构建视觉现实方面的关键差异，发现AI既有类似人类的幻觉效应，也有独特的像素级敏感性和幻觉现象


<details>
  <summary>Details</summary>
Motivation: 理解生物和人工视觉系统在视觉幻觉处理上的差异，可以为开发更鲁棒、可解释且与人类对齐的AI视觉系统提供指导

Method: 系统比较人类和AI对经典视觉幻觉（颜色、大小、形状、运动）的响应，分析AI特有的幻觉现象

Result: 发现AI会产生类似人类的幻觉效应（通过针对性训练或模式识别副产品），同时存在独特的AI幻觉（像素级敏感性、幻觉现象）

Conclusion: 通过视觉幻觉比较揭示了AI特有的感知脆弱性，为未来开发既保留有益人类感知偏差又避免损害信任和安全失真的视觉系统提供了见解

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [78] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 该论文发现现有VQA-NLE系统存在解释不一致和缺乏真正理解的问题，提出了基于图像扰动的新攻击策略和基于外部知识的防御方法，揭示了当前系统的安全可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答自然语言解释系统存在黑盒模型透明度不足的问题，会产生不一致的解释并在不理解上下文的情况下得出结论，暴露了推理流程或解释生成机制的弱点。

Method: 利用现有对抗策略扰动问题，并提出新颖的图像最小化扰动策略来诱导矛盾或虚假输出，同时引入基于外部知识的缓解方法来增强模型鲁棒性。

Result: 在两个标准基准测试和两个广泛使用的VQA-NLE模型上的广泛评估证明了攻击的有效性和基于知识防御的潜力。

Conclusion: 研究揭示了当前VQA-NLE系统在安全性和可靠性方面存在的紧迫问题，知识驱动的防御方法显示出改善系统鲁棒性的潜力。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [79] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT是一个基于视觉语言大模型的框架，通过模拟放射科医生的思维链过程，实现胸部X光片的智能诊断和可解释报告生成，在保持竞争力的诊断准确率的同时提供高质量的解释性报告。


<details>
  <summary>Details</summary>
Motivation: 解决胸部X光片诊断中深度学习模型黑盒性质导致的临床采用障碍，需要开发既准确又可解释的AI系统来辅助医疗决策。

Method: 提出X-Ray-CoT框架：首先提取多模态特征和视觉概念，然后使用基于LLM的组件配合结构化思维链提示策略进行推理，生成详细自然语言诊断报告。

Result: 在CORDA数据集上达到80.52%的平衡准确率和78.65%的F1分数，略优于现有黑盒模型，并能生成高质量的可解释报告。

Conclusion: 该工作代表了医学影像领域向可信赖和临床可操作AI系统迈出的重要一步，多模态融合和思维链推理对于构建稳健透明的医疗AI至关重要。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [80] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA提出了一种新的多模态学习方法，无需对齐预训练，通过将文本嵌入映射到视觉表示空间进行融合，在推理任务上表现优异，计算需求减少45%。


<details>
  <summary>Details</summary>
Motivation: 挑战传统多模态学习需要昂贵对齐预训练的假设，探索更高效的多模态融合方法，避免大规模图像-文本对齐数据集的需求。

Method: 将文本嵌入映射到连续视觉表示空间，在transformer中间层通过选择性注意力机制进行动态融合，无需对齐预训练。

Result: 在9个多模态基准测试中显示：推理和认知任务显著提升（MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, 认知推理: +27.2%），感知任务下降（名人识别: -49.5%, OCR: -21.3%），计算需求减少45%。

Conclusion: 首次证明对齐预训练对有效多模态学习并非必要，特别是复杂推理任务，为高效多模态架构开辟了新研究方向。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [81] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 提出基于微调视觉语言模型联盟和推理大语言模型的决策支持系统，用于自动化H反射肌电图波形分析和诊断，提高神经肌肉评估的准确性和标准化


<details>
  <summary>Details</summary>
Motivation: 传统H反射肌电图波形分析存在变异性和解释偏差，限制了可靠性和标准化，需要自动化解决方案来提高神经肌肉诊断的准确性

Method: 使用多个在标注H反射EMG波形图像上微调的视觉语言模型提取电生理特征，通过基于共识的方法聚合诊断输出，并由专门的推理大语言模型进行精炼

Result: 实验结果表明该混合系统能够提供高精度、一致性和可解释的H反射评估，显著推进神经肌肉诊断的自动化和标准化

Conclusion: 这是首个将微调VLM联盟与推理LLM集成用于基于图像的H反射分析的工作，为下一代AI辅助神经肌肉评估平台奠定了基础

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [82] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CNN、Transformer和卷积Kolmogorov-Arnold网络(CKAN)的混合模型，用于皮肤癌分类任务，在多个数据集上取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类是医学图像分析中的关键任务，需要精确区分恶性和非恶性病变以实现早期诊断和治疗。传统方法在特征表示和模型设计方面存在局限，需要更强大的模型来捕捉空间和上下文特征。

Method: 采用顺序和并行混合的CNN-Transformer架构，结合卷积Kolmogorov-Arnold网络(CKAN)进行非线性特征融合。使用迁移学习和广泛的数据增强技术，CNN提取局部空间特征，Transformer建模全局依赖关系。

Result: 在HAM10000数据集上达到92.81%准确率和92.47% F1分数，PAD-UFES数据集上达到97.83%准确率和97.83% F1分数，BCN20000数据集上达到91.17%准确率和91.79% F1分数，展现了优异的分类性能和泛化能力。

Conclusion: 混合CNN-Transformer架构能有效捕捉空间和上下文特征，CKAN的集成通过可学习激活函数增强了特征融合能力。该研究强调了特征表示和模型设计在推进稳健准确的医学图像分类中的重要性。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [83] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR是一个负责任的人工智能系统，用于糖尿病视网膜病变筛查，在准确性和公平性方面显著优于FDA批准的EyeArt系统


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是工作年龄人群视力丧失的主要原因，早期检测可降低95%的视力丧失风险，但眼科医生短缺和检查及时性问题阻碍了检测。AI模型虽有潜力，但受限于低质量数据和偏见问题

Method: 开发RAIS-DR系统，在整个AI生命周期中融入伦理原则，整合高效的卷积模型进行预处理、质量评估和三个专门的DR分类模型

Result: 在1,046名患者的本地数据集上评估，RAIS-DR相比EyeArt系统F1分数提高5-12%，准确率提高6-19%，特异性提高10-20%。公平性指标显示在不同人口统计亚组中表现公平

Conclusion: RAIS-DR是一个强大且符合伦理的DR筛查解决方案，有潜力减少医疗保健差距，代码和权重已开源

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [84] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: 本文提出了LangVision-LoRA-NAS框架，将神经架构搜索与LoRA结合，为视觉语言模型动态搜索最优的LoRA秩配置，在提升性能的同时降低微调成本。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA实现使用固定秩，限制了在不同多模态任务中的灵活性和效率。需要一种能够根据具体任务动态调整秩的方法来平衡性能和计算效率。

Method: 集成神经架构搜索(NAS)与LoRA技术，通过NAS动态搜索针对特定多模态任务的最优LoRA秩配置。使用LLaMA-3.2-11B模型在多个数据集上进行实验验证。

Result: 在LLaMA-3.2-11B-Vision-Instruct模型上，LangVision-LoRA-NAS显著提升了模型性能，同时降低了微调成本。

Conclusion: LangVision-LoRA-NAS框架通过动态秩优化，为视觉语言模型的高效微调提供了有效的解决方案，在性能和计算效率之间取得了良好平衡。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [85] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 使用交叉视图变换器(CVT)将相机图像映射到鸟瞰图(BEV)的三个通道：道路、车道标记和规划轨迹，在模拟器中验证了泛化能力和不同相机配置的效果


<details>
  <summary>Details</summary>
Motivation: 鸟瞰图(BEV)为自动驾驶感知提供了结构化的俯视抽象表示，需要研究如何从相机图像有效生成准确的BEV地图

Method: 采用交叉视图变换器(CVT)架构，在模拟器中训练模型将相机图像映射到BEV的三个通道，比较了不同相机布局(四摄像头)和两种损失函数(焦点损失和L1损失)

Result: 仅使用一个城镇的训练数据，采用L1损失的四摄像头CVT模型在新城镇测试中表现最稳健，能够生成相当准确的BEV地图

Conclusion: 交叉视图变换器在将相机输入映射到BEV地图方面显示出良好前景，L1损失和四摄像头配置的组合提供了最佳性能

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [86] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo是一个基于协同训练的多模态个性化表情识别方法，通过选择相关源主体并利用多模态互补信息进行主体特异性适应，在BioVid和StressID数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多源域适应方法往往忽视多模态信息或将多个源混合为单一域，限制了主体多样性，无法显式捕捉主体特异性特征。个性化表情识别需要适应主体间的高度变异性。

Method: 基于协同训练的多模态主体特异性选择和适应方法。选择与目标相关的源主体，使用主导模态生成伪标签进行类感知学习，结合类无关损失从低置信度目标样本中学习，对齐各模态源特征并仅组合置信的目标特征。

Result: 在BioVid和StressID多模态表情识别数据集上的实验结果表明，MuSACo优于无监督域适应（混合）和最先进的多源域适应方法。

Conclusion: MuSACo通过有效利用多模态和多源域的互补信息，成功解决了个性化表情识别中的主体特异性适应问题，特别适用于数字健康中的情感计算应用。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [87] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL框架利用视觉语言模型的语义对齐能力，将图像伪造检测构建为提示驱动的视觉推理任务，通过整体场景评估和区域异常检测两种方法实现跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展加剧了视觉伪造检测和解释的挑战，现有方法在跨域泛化方面存在困难，需要既能检测伪造又能提供推理和定位的鲁棒框架。

Method: 提出REVEAL框架，采用两种方法：(1)整体场景级评估：基于图像的物理、语义、透视和整体真实性；(2)区域异常检测：将图像分割成多个区域并分别分析。利用视觉语言模型的语义对齐能力进行提示驱动的视觉推理。

Result: 在不同领域数据集（Photoshop、DeepFake和AIGC编辑）上进行实验，与竞争基线比较，并分析了模型提供的推理能力。

Conclusion: REVEAL框架通过视觉语言模型的语义对齐和双重检测方法，有效解决了图像伪造检测的跨域泛化问题，提供了可解释的伪造检测和定位能力。

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [88] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出SFAC算法，仅需两张图像即可实现老照片着色，通过特征分布对齐和结构保持机制解决领域差距问题


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在大规模数据集上训练，但直接应用于老照片着色存在挑战，因为缺乏真实标签且自然灰度图像与老照片之间存在领域差距

Method: 基于CNN的SFAC算法，使用特征分布对齐损失建立语义对应，通过特征级感知约束和像素级冻结-更新金字塔结构保持机制防止结构失真

Result: 大量实验证实了该方法在老照片着色方面的有效性，定性和定量指标均表现良好

Conclusion: SFAC算法成功解决了老照片着色的领域差距问题，无需大数据依赖，仅需两张图像即可实现高质量的着色效果

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [89] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 提出了USDRL框架，一个基于骨架的动作理解基础模型，通过Transformer编码器、多粒度特征解相关和多视角一致性训练，在25个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作理解方法缺乏可扩展性和泛化能力，没有能够适应广泛动作理解任务的基础模型

Method: USDRL框架包含：1）基于Transformer的密集时空编码器（DSTE）；2）多粒度特征解相关（MG-FD）；3）多视角一致性训练（MPCT）

Result: 在9个骨架动作理解任务的25个基准测试中显著优于当前最先进方法

Conclusion: 该工作拓宽了骨架动作理解的研究范围，鼓励更多关注密集预测任务

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [90] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 提出了MCOUT方法，在联合潜在空间中进行连续推理，而不是使用自然语言，显著提升多模态推理性能


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型推理方法（如思维链）在多模态场景中效果不佳，难以动态对齐音频、视觉和文本信息，需要探索新的推理范式

Method: 开发了MCOUT方法，使用连续隐藏向量表示推理状态，迭代细化并与视觉和文本嵌入对齐。包含两个变体：MCOUT-Base重用语言模型最后隐藏状态，MCOUT-Multi集成多模态潜在注意力机制

Result: 在MMMU、ScienceQA和MMStar等基准测试中，MCOUT持续提升多模态推理性能，准确率最高提升8.23%，BLEU分数最高提升8.27%

Conclusion: 潜在连续推理是推进大型多模态模型超越语言绑定思维链的有前景方向，为类人反射式多模态推理提供了可扩展框架

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [91] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一个基于扩散模型的新型端到端自动驾驶框架，通过并行生成驾驶决策序列显著降低延迟，支持双向推理和渐进式生成，在nuScenes数据集上超越现有自回归VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型的自回归自动驾驶系统存在推理延迟高、无法进行双向推理的问题，不适合动态安全关键环境。

Method: 采用掩码扩散模型实现驾驶决策序列的并行生成，支持双向推理和渐进式易先生成策略。

Result: 在nuScenes数据集上，ViLaD在规划准确性和推理速度方面均优于最先进的自回归VLM基线，失败率接近零，并在真实自动驾驶车辆上验证了实用性。

Conclusion: ViLaD框架为端到端自动驾驶提供了一个新的范式，通过扩散模型解决了自回归方法的延迟和推理限制，具有实际应用价值。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [92] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 本文提出了ViDA-UGC数据集和CoT评估框架，针对用户生成内容(UGC)图像建立了首个大规模视觉失真评估指令调优数据集，通过人类标注和思维链框架生成详细质量描述，显著提升了多模态大语言模型的图像质量分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前可解释图像质量评估方法存在两个主要问题：一是对UGC和AIGC图像使用相同的失真标准进行评估不够合理，二是缺乏详细的图像质量分析来监控质量和指导图像恢复。

Method: 构建了包含11K图像的ViDA-UGC数据集，采用失真导向的构建流程，结合人类主题标注和思维链(CoT)评估框架，指导GPT-4o生成质量描述。还创建了ViDA-UGC-Bench基准测试集，包含476张图像和6,149个问答对。

Result: 实验结果表明，ViDA-UGC数据集和CoT框架能持续增强多种基础MLLM在图像质量分析方面的能力，在ViDA-UGC-Bench和Q-Bench基准测试上甚至超越了GPT-4o的表现。

Conclusion: 该研究为UGC图像质量评估提供了首个大规模数据集和有效的评估框架，解决了现有方法在UGC图像评估中的不足，显著提升了多模态大语言模型的图像质量分析性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [93] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 提出了OpenMoCap模型和CMU-Occlu数据集，解决了光学运动捕捉中大规模标记遮挡问题，通过射线追踪模拟真实遮挡模式，并利用标记-关节链推理机制实现鲁棒运动捕捉。


<details>
  <summary>Details</summary>
Motivation: 光学运动捕捉系统在真实应用中面临大规模标记遮挡问题，现有模型缺乏反映真实遮挡模式的训练数据集和捕获标记间长程依赖关系的训练策略。

Method: 引入CMU-Occlu数据集（使用射线追踪技术模拟真实遮挡模式），提出OpenMoCap模型（基于标记-关节链推理机制，同时优化和构建标记与关节间的深度约束）。

Result: OpenMoCap在多种场景下 consistently 优于竞争方法，CMU-Occlu数据集为鲁棒运动求解的未来研究提供了基础。

Conclusion: OpenMoCap模型和CMU-Occlu数据集有效解决了运动捕捉中的遮挡问题，模型已集成到MoSen系统中实际部署，代码已开源。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [94] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES是一种基于小波的通用视觉基元表示方法，通过小波的空间-频率局部化优势有效捕捉低频和高频信息，并提供快速渲染。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示方法依赖频率指导或复杂神经网络解码，导致频谱损失或渲染速度慢，需要一种既能保持高质量又能快速渲染的通用视觉表示方法。

Method: 基于小波变换构建视觉基元表示，利用小波的空间-频率局部化特性，并开发了基于小波的可微分光栅化器来实现快速视觉渲染。

Result: 在2D图像表示、5D静态和6D动态新视角合成等多种视觉任务上，WIPES相比基于INR的方法具有更高的渲染质量和更快的推理速度，在渲染质量上也优于基于高斯的方法。

Conclusion: WIPES作为一种视觉基元表示方法，通过小波变换有效解决了现有方法在频谱保持和渲染速度方面的局限性，为多维度视觉信号表示提供了高质量的解决方案。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [95] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 提出了首个可解释的创意评估与选择范式Creative4U，基于多模态大语言模型，通过推理选择方法在电商广告创意图像评估中取得显著效果


<details>
  <summary>Details</summary>
Motivation: 电商平台需要评估大量AIGC生成的创意图像质量，现有方法主要关注创意排名，无法满足可解释创意选择的需求

Method: 构建CreativePair数据集（8k标注图像对），提出Creative4U系统，采用Reason-to-Select RFT方法（CoT-SFT监督微调 + GRPO强化学习）

Result: 离线和在线实验均证明方法有效性，能够准确评估和选择创意图像

Conclusion: 该研究为创意图像评估提供了首个可解释的解决方案，代码和数据集将公开以推动研究和工业应用

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [96] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 基于云端协同的视觉-语言模型框架SpotVLM，通过将大模型延迟输出作为历史上下文来指小模型的实时推理，解决云端延迟波动问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有云端协同方案无法处理云端延迟波动，且没有充分利用大模型延迟但准确的响应来指导实时推理。

Method: 提出Context Transfer框架，将大模型延迟输出作为历史上下文导入小模型。设计SpotVLM，包含上下文替换模块和视觉聚焦模块来精炼文本输入和提升视觉基准一致性。

Result: 在4个数据集的3个实时视觉任务上进行广泛实验，验证了框架的有效性。

Conclusion: 该新框架为未来视觉-语言模型系统的更有效和延迟感知协同策略奠定了基础。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [97] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 提出了一种两阶段PMRF管道，用于从非对比MRI合成对比增强脑MRI，无需使用钆造影剂，在保持结构保真度的同时实现逼真的纹理


<details>
  <summary>Details</summary>
Motivation: 传统对比增强MRI需要钆造影剂，增加成本和时间，存在环境问题和患者风险，需要开发无造影剂的替代方案

Method: 两阶段方法：首先使用3D U-Net预测体素后验均值，然后通过时间条件3D整流流进行细化，结合真实纹理而不损害结构保真度

Result: 在360个测试样本上，最佳精炼输出达到轴向FID 12.46和KID 0.007（比后验均值降低68.7%），体积MSE为0.057（比后验均值高27%）

Conclusion: 该方法能真实恢复病变边缘和血管细节，有效解决了感知-失真权衡问题，适合临床部署

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [98] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 提出BEE方法解决持续测试时适应的探索-利用平衡问题，通过多级一致性正则化和互补锚点回放机制，在多个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法存在两个主要问题：1）基于深层输出调整预测，但域偏移主要影响浅层特征，导致探索缓慢；2）单一模型在探索过程中会遗忘历史知识，无法利用先前经验处理相似域

Method: 采用均值教师框架，提出多级一致性正则化（MCR）损失对齐师生模型中间特征加速适应，使用互补锚点回放（CAR）机制重用历史检查点恢复多样化域知识

Result: 在多个基准测试中显著优于最先进方法

Conclusion: BEE方法有效解决了CTTA中的探索-利用平衡问题，通过MCR和CAR机制实现了快速适应和知识保留的良好平衡

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [99] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd是首个从大场景视频中进行时空一致3D人群重建的框架，通过粗到细的群体引导运动优化策略解决遮挡问题，并贡献了VirtualCrowd虚拟数据集。


<details>
  <summary>Details</summary>
Motivation: 当前方法从静态图像重建3D人群缺乏时间一致性且无法解决遮挡问题，需要开发能够处理大场景视频中动态人群重建的方法。

Method: 采用粗到细的群体引导运动优化策略，结合VAE人体运动先验和分段级群体引导优化，利用异步运动一致性损失实现遮挡恢复。

Result: 实验结果表明该方法在大场景动态人群重建任务中达到了最先进的性能。

Conclusion: 提出的DyCrowd框架能够有效解决大场景视频中动态人群的时空一致重建问题，特别是在处理遮挡方面表现出色。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [100] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 提出了一种两阶段的人体去遮挡方法，先通过扩散先验和遮挡关节热图完成掩码重建，再利用文本特征和Stable Diffusion进行RGB补全，在严重遮挡下仍能有效恢复人体外观


<details>
  <summary>Details</summary>
Motivation: 深度学习模型难以准确预测被遮挡区域，人体去遮挡任务需要同时恢复被遮挡的身体结构和外观，现有基于扩散的方法在潜在空间转换时会导致可见区域像素级退化

Method: 两阶段方法：1) 掩码补全阶段使用基于扩散的人体先验和遮挡关节热图提供空间线索；2) RGB补全阶段使用重建的掩码作为条件输入，结合VQA模型提取的文本特征和CLIP编码，对Stable Diffusion解码器进行微调以避免像素退化

Result: 方法在严重遮挡下仍能有效重建人体外观，在掩码和RGB补全方面均优于现有方法，生成的去遮挡图像能提升下游任务（如2D姿态估计和3D人体重建）的性能

Conclusion: 该方法通过结合人体结构先验、空间线索和文本特征，成功解决了人体去遮挡问题，为人体中心的下游任务提供了高质量的输入数据

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [101] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型是否能理解和预测Wölfflin的五项艺术风格原则，发现预训练CLIP无法捕捉这些细微风格元素，通过微调CLIP创建了WP-CLIP模型，在GAN生成画作和Pandora-18K数据集上验证了其跨风格泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有度量标准无法有效预测Wölfflin五项艺术风格原则，需要能够解读色彩、构图等关键视觉元素的自动化分析方法。视觉语言模型在抽象图像属性评估方面展现潜力，但需要验证其是否能理解艺术风格原则。

Method: 使用标注的真实艺术图像数据集对预训练CLIP进行微调，为每项Wölfflin原则预测评分，构建WP-CLIP模型。

Result: 预训练CLIP无法固有地捕捉Wölfflin的细微风格元素，但微调后的WP-CLIP模型在GAN生成画作和Pandora-18K艺术数据集上表现出良好的跨风格泛化能力。

Conclusion: 视觉语言模型在自动化艺术分析方面具有巨大潜力，通过针对性微调可以使其理解和预测复杂的艺术风格原则。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [102] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 这篇论文提出了Vision-G1模型，通过多域RL训练和数据课程策略，在多个视觉推理测试集上达到独创性能效果，超越了GPT-4o等专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的训练集中在数学和逻辑推理等范围有限的任务上，导致模型在广泛领域的推理能力普遍性不足，且缺乏多域数据的整合方法。

Method: 从46个数据源构建了8个维度的RL准备视觉推理数据集，采用影响函数数据选择和难度筛选策略获取高质量样本，通过多轮次RL数据课程迭代训练Vision-G1模型。

Result: Vision-G1在多个视觉推理测试集上达到独创性能效果，超过同等规模的VLM模型，甚至超过GPT-4o和Gemini-1.5 Flash等专有模型。

Conclusion: 通过多域数据集成和RL课程训练，可以有效提升VLM模型的普遍推理能力，为视觉推理领域提供了新的解决方案。

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [103] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV是一个多无人机协作3D检测框架，通过自适应实例感知BEV表示学习，在低分辨率BEV输入下实现优越的精度-计算权衡


<details>
  <summary>Details</summary>
Motivation: 多无人机协作3D检测在覆盖范围和遮挡处理方面具有优势，但在资源受限的无人机平台上计算面临挑战，需要高效的感知方法

Method: 提出Box-Guided Refinement Module (BG-RM) 和 Instance-Background Contrastive Learning (IBCL)，通过精炼-对比范式学习自适应实例感知BEV表示，仅精炼前景实例相关的BEV网格

Result: 在Air-Co-Pred数据集上，AdaBEV在不同模型规模下均实现优越的精度-计算权衡，在低分辨率下优于其他最先进方法，接近上限性能

Conclusion: AdaBEV通过自适应实例感知表示学习，有效解决了多无人机3D检测中的计算效率问题，在保持低分辨率输入的同时实现高性能

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [104] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME方法通过源域数据增强、域判别器和专用域检测器来处理测试时适应中的动态域偏移问题，特别是在驾驶场景中的天气变化，通过多检测器集成和NMS进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应(TTA)中模型需要动态适应变化目标域的挑战，特别是在真实驾驶场景中频繁发生的天气域偏移问题。

Method: 利用源域数据增强到目标域，引入域判别器和专用域检测器来缓解剧烈域偏移，训练多个检测器并通过非极大值抑制(NMS)整合预测结果。

Result: 在SHIFT基准测试上显示出显著性能提升，证明了方法的有效性。

Conclusion: 提出的TTA-DAME方法能够有效处理动态域偏移问题，特别是在驾驶场景中的天气变化适应方面表现优异。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [105] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了多级知识蒸馏(MLKD)和动态自监督损失(SSL)两个组件，用于解决类别增量重复学习(CIR)问题，在CVPR CLVISION挑战赛中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 传统的类别增量学习假设每个任务都包含未见过的类别，而类别增量重复学习(CIR)更符合现实场景，即先前训练过的类别会在未来任务中重复出现，并且可以轻松从外部来源获取大量未标记数据。

Method: 1. 多级知识蒸馏(MLKD)：从多个先前模型跨多个视角（包括特征和logits）蒸馏知识，使模型能够保持各种先前知识
2. 动态自监督损失(SSL)：利用未标记数据加速新类学习，同时通过动态权重保持训练重点在主要任务上

Result: 所提出的两个组件显著提高了CIR设置下的性能，在CVPR第五届CLVISION挑战赛中获得了第二名

Conclusion: 该方法通过有效利用未标记数据和多级知识蒸馏技术，在类别增量重复学习场景中实现了高稳定性和可塑性

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [106] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 论文研究了自动驾驶车辆不同相机传感器配置导致的跨传感器域差距问题，提出了CamShift数据集来模拟这一差距，发现BEVFormer等基于密集BEV表示的模型最鲁棒，并提出基于神经渲染的数据驱动传感器适配管道来缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆因车型限制导致相机传感器配置差异，训练在一个传感器配置上的感知模型在其他配置上性能会下降，需要解决这种跨传感器域差距问题。

Method: 创建CamShift数据集模拟紧凑型车和SUV的传感器差异，评估不同3D目标检测器的跨传感器性能，提出基于神经渲染的传感器适配管道来转换数据集以匹配不同相机配置。

Result: BEVFormer等密集BEV表示模型对传感器配置变化最鲁棒，提出的神经渲染适配方法显著提升了所有检测器的性能，大幅缓解了跨传感器域差距。

Conclusion: 跨传感器域差距是重要问题，BEV架构具有天然鲁棒性，基于神经渲染的数据适配是有效的解决方案，能够实现不同传感器配置车辆间的数据重用。

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [107] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI驱动的新闻多样性导致多级漂移，显著降低了当前基于LVLM的多模态虚假信息检测系统的鲁棒性，性能平均下降14.8%，推理过程不稳定。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具带来的新闻内容多样性对多模态虚假信息检测系统构成新挑战，需要系统研究其对检测鲁棒性的影响。

Method: 构建DriftBench大规模基准测试集（16,000个新闻实例，6类多样化），设计三个评估任务：真实性验证鲁棒性、对抗性证据污染敏感性、推理一致性分析。

Result: 6个最先进的LVLM检测器在多样化内容下性能显著下降（平均F1下降14.8%），推理轨迹不稳定，对抗性证据注入下表现更差。

Conclusion: 现有MMD系统存在根本性漏洞，在GenAI时代迫切需要更具弹性的检测方法。

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [108] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 基于CNN的实时手语识别系统，通过摄像头捕捉手势并实时翻译为文本和语音，帮助听障人士沟通


<details>
  <summary>Details</summary>
Motivation: 解决听障人士在日常环境中的沟通障碍，提升他们的自主性和社会融入

Method: 使用卷积神经网络(CNN)在Sign Language MNIST数据集上训练，实时通过摄像头捕捉手势并进行分类识别，结合文本转语音技术

Result: 实验显示系统具有高准确性和实时性能（存在一些延迟），证明其作为辅助工具的实用性

Conclusion: 该系统为听障人士提供了一个可访问、可靠且用户友好的沟通辅助工具，具有实际应用价值

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [109] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 提出Dual Contrastive Denoising Score框架，利用文本到图像扩散模型的生成先验，通过双对比损失实现真实图像编辑，既能灵活修改内容又能保持结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在编辑真实图像时面临两个挑战：用户难以准确描述图像所有细节的完美文本提示，以及模型在修改特定区域时经常意外改变不需要修改的区域。

Method: 基于对比学习方法，在潜在扩散模型的self-attention层中间表示中引入双对比损失，利用丰富的空间信息，无需依赖辅助网络。

Result: 通过大量实验证明，该方法在真实图像编辑方面优于现有方法，同时保持直接使用预训练文本到图像扩散模型的能力，无需进一步训练。

Conclusion: 提出的框架成功解决了真实图像编辑中的关键挑战，实现了灵活的内容修改和结构保持，为零样本图像到图像翻译提供了有效解决方案。

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [110] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文分析了稀疏视角下3D高斯泼溅技术出现外观伪影的原因，提出了量化高斯纠缠程度的共适应评分(CA)，并提出了两种轻量级策略来缓解共适应问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在密集视角下表现优异，但在稀疏视角场景中，虽然训练视角渲染效果真实，但在新视角下会出现外观伪影。本文旨在探究这些伪影的根源。

Method: 提出了共适应评分(CA)来量化高斯之间的纠缠程度，通过计算同一视点下不同随机高斯子集渲染结果的像素级方差。基于分析提出了两种策略：随机高斯丢弃和透明度乘性噪声注入。

Result: 分析发现高斯之间的共适应程度随着训练视角数量的增加而自然缓解。提出的两种轻量级策略在各种方法和基准测试中都验证了有效性。

Conclusion: 当前方法优化的高斯过度纠缠，导致忽略了场景的真实外观分布。本文对共适应效应的深入理解将为稀疏视角3D高斯泼溅技术提供更全面的认识。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [111] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 提出FDIKP网络，通过频域表示增强核估计，结合双分支逆核预测策略和位置自适应卷积，在单图像散焦去模糊任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间特征进行核估计，在严重模糊区域性能下降，因为局部高频细节缺失。频域在模糊建模方面具有更好的判别能力

Method: FDIKP网络：1) 双分支逆核预测策略(DIKP)提高核估计精度和稳定性；2) 位置自适应卷积(PAC)增强解卷积适应性；3) 双域尺度循环模块(DSRM)融合解卷积结果并逐步提升去模糊质量

Result: 大量实验表明该方法优于现有方法

Conclusion: 频域表示能有效增强核建模的结构可识别性，提出的FDIKP网络在单图像散焦去模糊任务中表现出色

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [112] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 提出DCSCR网络，结合传统图像集分类方法与深度模型，同时学习帧级和概念级特征表示，通过类特定协同表示度量学习解决小样本图像集分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有传统方法基于原始像素特征分类，忽略特征学习重要性；深度方法能学习深度特征但在度量集合距离时无法自适应调整特征，导致小样本场景性能有限。

Method: DCSCR包含全卷积深度特征提取模块、全局特征学习模块和类特定协同表示度量学习模块，通过新的CSCR对比损失函数自适应学习概念级特征表示和集合间距离相似性。

Result: 在多个知名小样本图像集分类数据集上的广泛实验表明，该方法相比现有最先进算法具有更好的性能。

Conclusion: DCSCR网络有效解决了小样本图像集分类中的特征学习和距离度量问题，实现了同时学习帧级和概念级特征表示的能力。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [113] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 提出基于Mamba的双尺度融合和双路径扫描网络，通过选择性传播上下文信息来有效去除阴影，在阴影去除基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 阴影去除任务中，阴影区域的变换与正常光照区域差异显著，需要有效整合非局部上下文线索和自适应建模区域特定变换

Method: 提出双尺度融合Mamba块(DFMB)融合原始特征和低分辨率特征增强多尺度表示，双路径Mamba组(DPMG)通过水平扫描捕获全局特征并采用掩码感知自适应扫描策略

Result: 在阴影去除基准测试中显著优于现有最先进方法

Conclusion: 该方法通过选择性上下文传播和自适应区域建模，有效解决了阴影去除中的变换差异问题

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [114] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA是一个基于深度学习的框架，用于在急性缺血性卒中机械取栓术中分类数字减影血管造影的最小强度投影图像属性，提高下游图像分割性能


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型在机械取栓术中辅助应用时，图像质量差会降低性能表现，需要自动化的图像质量控制和分类工具

Method: 使用预训练的ResNet骨干网络进行微调，训练单独的分类器来预测9个图像属性（如对比剂存在、投影角度、运动伪影严重程度等），基于1758张标注的荧光镜MinIP图像

Result: 模型在所有标签上都表现出色，ROC-AUC范围0.91-0.98，精确度范围0.70-1.00。在分割任务中，通过过滤低质量图像，分割成功率从42%提升到69%

Conclusion: CLAIRE-DSA作为自动化工具在急性缺血性卒中患者的DSA系列图像属性分类方面显示出强大潜力，支持临床和研究应用中的图像标注和质量控制

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [115] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 本文针对CdZnTe半导体图像标注中低对比度缺陷边界的问题，提出了基于组内一致性增强的半监督语义分割框架ICAF，通过多视图信息交互有效解决了传统方法在"多对一"关系下的局限性


<details>
  <summary>Details</summary>
Motivation: CdZnTe半导体图像标注面临低对比度缺陷边界的挑战，需要标注者交叉参考多个视图。这些视图共享同一个真实标注，形成独特的"多对一"关系，使得传统半监督分割方法在低对比度区域容易产生误差累积和确认偏差

Method: 提出Intra-group Consistency Augmentation Framework (ICAF)：1) 实验验证CdZnTe组内一致性约束，建立基于Intra-group View Sampling的组导向基线；2) 引入Pseudo-label Correction Network (PCN)，包含View Augmentation Module动态合成边界感知视图，以及View Correction Module进行视图间信息交互以强调显著区域并减少噪声

Result: 在CdZnTe数据集上仅使用2%的组标注数据（5‰），采用DeepLabV3+和ResNet-101骨干网络，达到了70.6%的mIoU

Conclusion: ICAF框架通过组内一致性增强和多视图信息交互，有效解决了CdZnTe材料图像分割中的低对比度边界问题，为类似"多对一"关系的半监督分割任务提供了新的解决方案

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [116] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack是一个针对无人机视角下多目标跟踪的框架，通过多尺度特征增强、速度自适应卡尔曼滤波、群体运动补偿和时空记忆预测等技术，显著提升了复杂城市交通环境中小目标的跟踪精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机视角下的多目标跟踪在智能交通系统中具有重要应用价值，但面临着小目标尺度变化、遮挡、非线性交叉运动和运动模糊等挑战，需要开发更稳定和准确的跟踪方法。

Method: 提出SocialTrack框架，包含：1）多尺度特征增强的小目标检测器；2）速度自适应立方卡尔曼滤波(VACKF)用于轨迹预测；3）群体运动补偿策略(GMCS)建模社会群体运动先验；4）时空记忆预测(STMP)利用历史轨迹信息预测未来状态。

Result: 在UAVDT和MOT17数据集上的实验表明，SocialTrack在多个关键指标上优于现有最先进方法，特别是在MOTA和IDF1等核心性能指标上有显著提升。

Conclusion: SocialTrack框架具有优异的鲁棒性和适应性，同时具有高度模块化和兼容性，可以与现有跟踪器无缝集成以进一步提升性能。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [117] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出了一种基于多风格图像的潜在扩散模型方法，通过图像提示适配器和特征统计对齐来改进图像风格迁移，解决了现有方法在风格匹配准确性和内容-风格解耦方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在图像风格迁移中存在三个主要问题：1) 风格匹配不准确；2) 可使用的风格图像数量有限；3) 内容和风格不期望地纠缠在一起。需要一种能够更好表示风格特征并防止内容泄露的方法。

Method: 提出利用多风格图像来更好地表示风格特征。设计了结合图像提示适配器和去噪过程中特征统计对齐的方法，在去噪UNet的交叉注意力和自注意力层进行干预。使用聚类从大量风格样本注意力特征中提取小型代表性集合进行统计对齐。

Result: 实验证明该方法在风格化任务中达到了最先进的性能。

Conclusion: 通过多风格图像、图像提示适配器和统计特征对齐的综合方法，有效解决了现有风格迁移方法的局限性，实现了更好的风格表示和内容-风格解耦。

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [118] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 本研究使用深度学习分析谷歌街景图像，开发了一种新颖的方法来全球范围内估算自行车和摩托车的出行模式份额，实现了较高的预测精度。


<details>
  <summary>Details</summary>
Motivation: 交通方式影响健康，但全球范围内的自行车和摩托车行为比较数据稀缺，需要高效的数据采集方法来补充传统调查数据。

Method: 使用YOLOv4模型在185个全球城市的谷歌街景图像中检测自行车和摩托车，采用beta回归模型建立城市级模式份额预测模型，控制人口密度变量。

Result: 模型预测精度较高：摩托车检测与模式份额相关性0.78，自行车相关性0.51；预测R²值分别为0.614和0.612，中位绝对误差分别为1.3%和1.4%。

Conclusion: 计算机视觉结合街景图像能有效捕捉出行模式，为传统数据源提供有价值的补充，特别是在中东、拉丁美洲和东亚等数据稀缺地区。

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [119] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: 使用计算机视觉方法对食双星光变曲线进行分类，通过预训练的ResNet50和ViT模型在合成数据上进行微调，采用极坐标和hexbin可视化新方法，主分类准确率>96%，但斑点检测效果不佳


<details>
  <summary>Details</summary>
Motivation: 开发自动化方法对大规模巡天中的食双星进行形态学分类，解决传统人工分类效率低的问题

Method: 使用预训练的卷积神经网络和视觉变换器模型，在合成数据集上微调，创新性地将相位折叠光变曲线转换为极坐标结合hexbin可视化，采用分层分类策略

Result: 主分类在多个波段验证数据上准确率>96%，在OGLE、DEBCat、WUMaCat观测数据上测试准确率>94%（TESS达100%），但斑点检测任务表现很差

Conclusion: 计算机视觉在食双星形态分类方面潜力巨大，但自动斑点检测仍需进一步研究，模型对细微光度特征的识别能力有限

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [120] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出了一种新颖的图像生成方法NVG，通过将图像分解为结构化序列，从全局布局到细节逐步细化生成，在ImageNet上实现了优于VAR系列的FID分数


<details>
  <summary>Details</summary>
Motivation: 传统图像生成方法缺乏对生成过程的细粒度控制，需要一种能够分层级、结构化生成图像的新框架

Method: Next Visual Granularity (NVG)生成框架，将图像分解为共享空间分辨率但包含不同粒度token的结构化序列，从空图像开始逐步细化生成

Result: 在ImageNet数据集上训练NVG模型，FID分数显著优于VAR系列（3.30->3.03, 2.57->2.44, 2.09->2.06），表现出清晰的缩放行为

Conclusion: NVG框架提供了分层表示和细粒度控制能力，在图像生成任务中展现出优越性能和潜力

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [121] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: CVPR 2025事件视觉研讨会举办的时空实例分割挑战赛概述，包含任务描述、数据集、挑战细节和结果分析，以及前5名团队的方法介绍


<details>
  <summary>Details</summary>
Motivation: 推动事件相机和灰度相机数据融合的时空实例分割技术发展，为复杂动态场景中的精确像素级分割提供解决方案

Method: 基于时空对齐的事件相机和灰度相机数据，预测定义物体类别的精确像素级分割掩码，挑战赛采用统一评估框架

Result: 提供了挑战赛的完整结果分析，展示了前5名团队的最优性能方法，相关代码和资源已开源

Conclusion: 该挑战赛成功推动了事件视觉领域的发展，为时空实例分割任务建立了基准，促进了相关技术的进步和创新

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [122] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA是一个基于深度学习的海底图像恢复模型，通过双频增强自注意力空间和频率调制器，在保留空间结构的同时增强低频和高频信息，有效解决水下视觉退化问题。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光散射、吸收和浑浊等问题，导致图像清晰度下降和颜色信息失真，影响海洋生物多样性分析、生态评估和自主探索的准确性。

Method: 提出DEEP-SEA模型，采用双频增强自注意力空间和频率调制器，自适应地在频域中细化特征表示，同时处理空间信息以更好地保持结构。

Result: 在EUVP和LSUI数据集上的综合实验表明，该模型在恢复精细图像细节和结构一致性方面优于现有最先进方法。

Conclusion: DEEP-SEA通过有效缓解水下视觉退化，有潜力提高水下监测平台的可靠性，实现更准确的生态观测、物种识别和自主导航。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [123] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 提出MMPDA框架解决多模态欺骗检测中的领域偏移问题，通过渐进式领域适应方法在特征和决策层面进行对齐，在MMDD挑战赛中获得第二名，准确率和F1分数均优于其他团队


<details>
  <summary>Details</summary>
Motivation: 解决多模态欺骗检测中源域和目标域之间的领域偏移问题，特别是在多样化的多模态数据集之间进行知识迁移

Method: 多源多模态渐进式领域适应(MMPDA)框架，通过在特征和决策层面逐步对齐源域和目标域来桥接领域差异

Result: 在竞赛第二阶段达到60.43%的准确率和56.99%的F1分数，F1分数比第一名团队高5.59%，准确率比第三名团队高6.75%

Conclusion: MMPDA框架有效解决了多模态欺骗检测中的跨领域适应问题，在MMDD挑战赛中取得了优异的性能表现，证明了该方法的有效性

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [124] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出CoMuCo方法，通过多视图协作优化解决视觉语言模型在跨域小样本任务中的性能限制问题


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自然图像上预训练后，在标准图像数据集上表现良好，但在成像域与自然图像不同的跨域任务中效果有限

Method: CoMuCo策略使用两个功能互补的专家模块提取多视图特征，结合基于先验知识的一致性约束和信息几何的共识机制来增强特征学习的鲁棒性

Result: 在现有和新提出的基准测试上进行了广泛实证评估，表明CoMuCo在小样本任务中始终优于当前方法

Conclusion: CoMuCo是一种有效的视觉语言模型微调策略，能够显著提升跨域小样本识别性能，并建立了新的跨域小样本基准

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [125] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: MPS-Tuning是一种新的视觉语言模型微调方法，通过保持语义流形的几何结构来提升少样本分类性能，同时增强类别可分性


<details>
  <summary>Details</summary>
Motivation: 现有VLMs微调方法往往忽略数据分布的几何结构，可能导致语义表示失真，需要一种能保持流形结构的正则化方法

Method: 将特征空间数据分布视为语义流形，通过对齐微调前后的Gram矩阵来保持宏观和微观拓扑结构，并优化图像-文本模态的配对相似性来增强类别可分性

Result: 大量实验表明MPS-Tuning显著提升模型性能，同时有效保持语义流形结构

Conclusion: MPS-Tuning通过几何结构约束成功解决了VLMs微调中的语义失真问题，为少样本分类提供了有效的解决方案

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [126] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S²-Guidance是一种新的扩散模型引导方法，通过随机块丢弃构建子网络来优化模型预测，解决了CFG方法中的次优预测问题，在文本到图像和文本到视频生成任务中表现优于CFG和其他先进引导策略。


<details>
  <summary>Details</summary>
Motivation: 研究发现Classifier-free Guidance (CFG)方法在扩散模型中存在次优预测问题，导致语义不连贯和低质量输出，需要一种更好的引导方法来提升生成质量。

Method: 提出S²-Guidance方法，利用前向过程中的随机块丢弃构建随机子网络，有效引导模型远离低质量预测，朝向高质量输出。

Result: 在文本到图像和文本到视频生成任务上的大量实验表明，S²-Guidance在定性和定量评估中都优于CFG和其他先进引导策略。

Conclusion: S²-Guidance通过利用模型自身的子网络来优化预测，提供了一种有效的扩散模型引导方法，能够显著提升生成样本的质量和提示符遵循度。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [127] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG是一种基于非负矩阵分解的一次性剪枝方法，通过梯度掩码机制在训练过程中严格保持目标稀疏度，在CIFAR数据集上实现了可比或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络规模庞大导致部署困难，现有剪枝方法往往需要复杂的迭代过程、专用标准或在训练中难以有效保持稀疏度。

Method: 使用非负矩阵分解(NMF)识别重要权重结构进行一次性剪枝，然后采用精确的梯度掩码机制确保只有未剪枝权重被更新，严格保持目标稀疏度。

Result: 在CIFAR-10和CIFAR-100数据集上，使用ResNet56、ResNet34和ResNet18网络，ONG在不同稀疏度水平下实现了与现有稳定稀疏化方法相当或更优的性能。

Conclusion: ONG提供了一种清晰的目标稀疏度实现机制，能够在保持剪枝后结构完整性的同时，实现高效的模型压缩。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [128] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一个0.5B参数的潜在流匹配变换器模型，能够根据临床报告生成整个3D CT体积，在时间一致性、图像多样性和文本-图像对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于临床报告生成整个CT体积可以加速医学研究，通过数据增强、隐私保护合成和减少患者数据监管约束，同时保留诊断信号。CT-RATE数据集的发布使得训练大型文本条件CT体积生成模型成为可能。

Method: 使用FLUX的A-VAE定义潜在空间，依赖CT-Clip文本编码器编码临床报告。采用自定义自回归方法生成一致的整个CT体积：模型首先从纯文本预测第一个切片序列，然后基于先前生成的切片序列和文本来预测后续序列。

Result: 与最先进的生成CT模型相比，CTFlow在时间一致性、图像多样性和文本-图像对齐方面表现出优越性，通过FID、FVD、IS分数和CLIP分数进行评估。

Conclusion: CTFlow成功实现了基于临床报告的整个3D CT体积生成，为医学影像数据增强和隐私保护提供了有效的解决方案。

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [129] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU是一个多阶段跨模态融合的3D检测框架，通过深度补全网络将像素信息投影到3D空间获得伪点云，设计双边跨视图增强3D骨干网络，并引入迭代体素点感知细粒度池化模块，在KITTI、nuScenes和Waymo数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态3D检测方法大多关注单阶段或部分阶段融合，导致特征提取不足和性能欠佳，需要解决3D空间信息与2D语义信息对齐的挑战。

Method: 1) 通过深度补全网络将像素信息投影到3D空间获得伪点云；2) 设计双边跨视图增强3D骨干网络（S2D分支和ResVC分支）；3) 引入迭代体素点感知细粒度池化模块；4) 设计IoU联合预测分支和新的proposals生成技术。

Result: 在KITTI、nuScenes和Waymo数据集上的大量实验表明该方法具有优越性能。

Conclusion: CMF-IOU框架通过多阶段跨模态融合有效解决了3D空间和2D语义信息的对齐问题，在多个基准数据集上取得了优异的表现。

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [130] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench是首个同时评估语义和空间对齐的布局引导文本到图像生成基准，包含7个挑战性场景，评估对象生成、颜色保真度、属性识别、对象间关系和空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要评估文本对齐，而忽略了布局对齐，无法全面评估模型的空间保真度，这在合成数据生成等应用中至关重要。

Method: 构建包含7个挑战性场景的文本-布局对数据集，提出结合布局对齐分数的评估协议来评估空间准确性。

Result: 使用7Bench评估了多个最先进的扩散模型，揭示了它们在不同对齐任务中的优势和局限性。

Conclusion: 7Bench填补了布局引导文本到图像生成中联合评估语义和空间对齐的空白，为模型评估提供了更全面的基准。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [131] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD是一个针对高分辨率图像异常检测的通用框架，通过双分支架构和多分辨率特征融合策略，解决了传统方法在检测细微异常区域时的精度和效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测方法主要针对低分辨率场景，高分辨率图像的传统下采样会导致细微异常区域的漏检，现有方法在检测精度和效率上难以满足工业实际需求。

Method: 采用双分支架构整合不同尺度的异常线索，结合多分辨率特征融合策略处理细粒度纹理变化，使用检测器池和多种检测器分配策略实现自适应检测。

Result: 在专门构建的高分辨率异常检测基准测试（MVTec-HD、VisA-HD和RealIAD-HD）上进行了广泛实验，证明了HiAD的优越性能。

Conclusion: HiAD框架能够有效检测高分辨率图像中不同大小的异常区域，在有限计算资源下实现了检测精度和效率的平衡，代码已开源。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [132] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG是一个两阶段训练框架，通过提升编码器和解码器的泛化能力来缓解增量学习中的灾难性遗忘问题，特别在小内存场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法通常只关注编码器或解码器中的一个组件，限制了缓解灾难性遗忘的效果，特别是在小内存场景下性能更差

Method: 采用两阶段训练：第一阶段通过特征增强训练集成编码器学习泛化表示，提升解码器泛化能力；第二阶段使用知识蒸馏策略压缩集成编码器，开发新的更泛化的编码器

Result: 在三个基准数据集上的广泛实验显示SEDEG具有优越性能，消融研究确认了各组件有效性

Conclusion: SEDEG通过顺序提升编码器和解码器的泛化能力，有效缓解了增量学习中的灾难性遗忘问题，特别是在小内存场景下表现突出

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [133] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net架构的全自动纤维束分割框架，用于猕猴示踪数据的大规模分析，显著提高了稀疏束检测能力并降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 现有的解剖示踪数据标注方法劳动密集型且自动化方法存在局限性，无法有效处理稀疏纤维束和需要复杂的连续切片后处理，阻碍了大尺度分析。

Method: 采用大尺寸补丁的U-Net架构，结合前景感知采样和半监督预训练，实现了对独立切片的自动化纤维束分割。

Result: 相比现有最佳方法，稀疏束检测提高了20%以上，误报率降低了40%，消除了将末端误标为束的常见错误。

Conclusion: 该框架将促进解剖示踪数据的大规模自动化分析，为验证和优化扩散MRI纤维束成像方法生成更多真实数据。

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [134] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen是一个端到端的视频重照明框架，基于大规模视频生成模型，通过文本描述控制光照和背景，在保持前景一致性的同时实现和谐的视频重照明效果。


<details>
  <summary>Details</summary>
Motivation: 视频重照明是一个具有挑战性但很有价值的任务，需要在替换视频背景的同时相应地调整前景光照并实现和谐融合。现有方法缺乏高质量的多光照条件配对视频数据，且难以保持前景属性（如反照率）的一致性和时间帧间的照明一致性。

Method: 构建混合真实和合成视频的大规模数据集；使用3D渲染引擎创建合成视频对；采用HDR-based光照模拟补充真实视频；设计联合训练课程，注入domain-aware adapter解耦重照明和域外观分布的学习。

Result: 实验结果表明，Lumen能够有效地将输入编辑为具有一致光照和严格前景保持的电影级重照明视频，在前景保持和视频一致性评估方面优于现有方法。

Conclusion: Lumen框架通过大规模混合数据集和domain-aware adapter设计，成功解决了视频重照明中的前景保持和时间一致性问题，为视频编辑提供了有效的解决方案。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [135] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem是一种新颖的语义引导掩码方法，通过Grad-CAM指导关节掩码，使用混合高阶运动作为重建目标，提升了自监督骨架动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督骨架动作识别方法关注有限关节和低阶运动模式，限制了模型对复杂运动模式的理解能力。

Method: 提出语义引导掩码方法MaskSem，利用基于相对运动的Grad-CAM指导关节掩码，并使用混合高阶运动（速度和加速度）作为重建目标。

Result: 在NTU60、NTU120和PKU-MMD数据集上的实验表明，MaskSem结合普通transformer提升了骨架动作识别性能。

Conclusion: 该方法更适合人机交互应用，能更全面地描述动态运动过程，增强对运动模式的理解。

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [136] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed是一个用于开放式医学视觉问答的强化学习框架，通过结合领域知识和自适应语义奖励来提升医学推理质量，在多个基准测试中显著提升了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的强化学习应用不足，现有方法主要针对封闭式VQA，而开放式医学VQA更能反映临床实践但研究有限。基于模型的语义奖励存在奖励崩塌问题，需要更好的奖励判别性。

Method: ARMed框架：首先通过监督微调在思维链数据上融入领域知识，然后应用强化学习结合文本正确性和自适应语义奖励来增强推理质量。

Result: 在6个医学VQA基准测试中，ARMed在域内任务上提升32.64%，在域外基准上提升11.65%，显著提高了准确性和泛化能力。

Conclusion: 奖励判别性在医学强化学习中至关重要，语义引导的奖励有望实现稳健且具有临床意义的多模态推理。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [137] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: DLaBella29团队在MICCAI 2025 ToothFairy3挑战赛中提出基于3D SegResNet的深度学习管道，用于CBCT牙齿多类别分割，在验证集上达到平均Dice系数0.87


<details>
  <summary>Details</summary>
Motivation: 自动化CBCT牙齿结构分割可有效辅助病理识别（如牙髓或根尖周病变）和头颈癌患者放射治疗规划，提高患者护理质量

Method: 使用MONAI Auto3DSeg框架和3D SegResNet架构，采用5折交叉验证训练，关键预处理包括图像重采样和强度裁剪，采用两阶段分割策略：第一阶段多标签STAPLE集成融合，第二阶段对下颌骨进行紧密裁剪以分割细小神经结构

Result: 在ToothFairy3挑战赛样本外验证集上获得平均Dice系数0.87的优异分割性能

Conclusion: 该方法展示了自动化牙齿分割在放射肿瘤学中改善患者护理的相关性和应用价值，为临床诊断和治疗规划提供了有效工具

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [138] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR是一种新颖的端到端架构，使用两个解耦的解码器分别处理头部定位和注视预测任务，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端注视目标检测模型使用单一解码器同时处理头部定位和注视预测，导致表示纠缠。需要解耦这两个任务以获得更好的性能。

Method: 提出GazeDETR架构，包含两个解耦的解码器：一个用于头部定位（利用局部信息），一个用于注视预测（结合局部和全局信息），使用连贯注意力场。

Result: 在GazeFollow、VideoAttentionTarget和ChildPlay数据集上取得最先进的结果，显著优于现有的端到端模型。

Conclusion: 解耦头部定位和注视预测任务的双解码器架构能够学习独特的表示，有效提升注视目标检测性能。

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [139] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 提出了Compact Attention框架，通过自适应分块策略、时变窗口和自动化配置搜索，在保持视觉质量的同时实现注意力计算1.6~2.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的计算需求对基于transformer的视频生成构成关键挑战，现有方法无法充分利用视频数据固有的时空冗余性。

Method: 提出硬件感知的加速框架Compact Attention，包含自适应分块策略近似多样化空间交互模式、时变窗口基于帧邻近度调整稀疏度、自动化配置搜索算法优化稀疏模式。

Result: 在单GPU设置上实现注意力计算1.6~2.5倍加速，同时保持与全注意力基线相当的视觉质量。

Conclusion: 通过结构化稀疏性利用为高效长视频生成提供了原则性方法。

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [140] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出了一种无需标注数据的零样本神经架构搜索代理，通过奇异值分解和外在曲率来评估网络架构的收敛性、泛化性和表达能力，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本NAS代理通常依赖标注数据，且主要关注收敛性、泛化性或表达能力中的单一属性，无法在真实无标注场景中全面评估网络性能。

Method: 利用神经网络层特征的奇异值分解(SVD)和网络输出的外在曲率，设计了一个简化的调和平均数作为代理指标，仅需单个无标注数据样本即可计算。

Result: 在NAS-Bench-101、NAS-Bench-201、TransNAS-Bench-101-micro等多个基准测试中表现出优越的相关性性能，同时在DARTS和AutoFormer搜索空间的NAS任务中表现优异且高效。

Conclusion: 该方法成功解决了零样本NAS中对标注数据的依赖问题，同时综合考虑了网络的收敛性、泛化性和表达能力，为实际应用提供了有效的架构评估方案。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [141] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文对多模态视觉目标跟踪(MMVOT)进行了全面综述，涵盖数据收集、模态对齐、模型设计和评估等关键方面，分析了六种MMVOT任务，并首次揭示了现有数据集中对象类别的长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 智慧城市发展产生了大量多模态数据，需要从多模态分析角度研究视觉目标跟踪这一关键任务，探讨多模态跟踪是否总能提供优于单模态的解决方案。

Method: 通过分类现有MMVOT方法（基于处理可见光和X模态的不同方式），分析数据收集、对齐和标注挑战，并对六个MMVOT任务进行系统性综述。

Result: 揭示了现有MMVOT数据集中对象类别的长尾分布特性，发现与RGB数据集相比明显缺乏动物类别，提供了338篇参考文献的全面调研。

Conclusion: 多模态跟踪并不总是优于单模态跟踪，需要根据具体应用场景判断其优势，同时指出了当前数据集在类别分布上的局限性。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [142] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 本文通过实证研究表明，增强特征多样性可以显著改善开放集识别和持续学习性能，特征多样性有助于检测新类别和保持旧知识。


<details>
  <summary>Details</summary>
Motivation: 开放集识别和持续学习是机器学习中的两个关键挑战，虽然已有许多启发式方法通过促进特征多样性来解决这些问题，但很少有研究直接探讨特征多样性的具体作用。

Method: 通过实证研究分析特征多样性在开放集识别和持续学习中的作用，提供实验证据来验证特征多样性的影响。

Result: 增强特征多样性能够改善开放集样本的识别能力，同时也有利于持续学习中旧知识的保持和新知识的整合。

Conclusion: 特征多样性在开放集识别和持续学习中发挥着重要作用，这一发现可为这两个领域的实践方法和理论理解提供新的研究方向。

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [143] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm是一个通信高效的协作感知框架，通过4D雷达多普勒和查询驱动稀疏方案，在保持精度的同时将带宽降低90%


<details>
  <summary>Details</summary>
Motivation: 解决协作感知中密集BEV特征图传输带宽过高的问题，克服遮挡和传感器范围限制

Method: 构建运动中心动态地图区分动静物体，生成参考查询和探索查询，仅交换查询特定BEV特征，通过多尺度门控可变形注意力融合

Result: 在OPV2V-R和Adver-City-R数据集上，带宽比全图共享降低90%，在不同交通密度和遮挡情况下匹配或超越现有基线

Conclusion: SlimComm框架有效解决了协作感知的带宽瓶颈问题，为实际部署提供了可行的解决方案

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [144] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0是一个实时交互式世界模型，通过few-step自回归扩散生成高质量长视频，速度达到25FPS


<details>
  <summary>Details</summary>
Motivation: 现有交互式世界模型依赖双向注意力和冗长推理步骤，严重限制实时性能，难以模拟需要即时更新的真实世界动态

Method: 包含三个关键组件：1)虚幻引擎和GTA5环境的大规模数据生产管道；2)支持帧级鼠标键盘输入的动作注入模块；3)基于因果架构的few-step蒸馏实现实时流式视频生成

Result: 能够生成高质量分钟级视频，在多样化场景中以超快速度25FPS运行

Conclusion: 该模型在交互式世界建模方面取得了显著进展，开源模型权重和代码库以推动相关研究

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [145] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了EgoTwin框架，用于联合生成第一人称视角视频和人体运动，解决了视角对齐和因果交互两大挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然外中心视角视频合成取得了很大进展，但第一人称视角视频生成仍然研究不足，需要同时建模第一人称视角内容和穿戴者身体运动引起的相机运动模式。

Method: 提出了基于扩散变换器架构的EgoTwin框架，引入以头部为中心的运动表示，并采用控制论启发的交互机制在注意力操作中显式捕捉视频与运动之间的因果交互。

Result: 构建了大规模真实世界同步文本-视频-运动三元组数据集，设计了新颖的评估指标，大量实验证明了EgoTwin框架的有效性。

Conclusion: EgoTwin框架成功解决了第一人称视频与人体运动联合生成的关键挑战，为这一新兴领域提供了有效的解决方案。

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [146] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR是一个分层特征适应框架，通过参数高效适配器解决多中心心脏MRI重建中的域偏移问题，在CMRxRecon2025数据集上表现出优异的跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习心脏MRI重建在多临床中心部署时面临显著的域偏移挑战，不同扫描仪配置和成像协议导致性能下降。

Method: 使用分层特征适应框架：协议级适配器处理序列特定特征，中心级适配器处理扫描仪相关变化，基于变分展开骨干网络。通用适配器通过随机训练学习中心不变适应实现泛化。采用多尺度SSIM损失、频域增强和对比度自适应加权进行优化。

Result: 在CMRxRecon2025数据集（5+中心、10+扫描仪、9种模态）上的综合评估显示，该方法在保持重建质量的同时实现了优越的跨中心泛化性能。

Conclusion: HierAdaptMR通过分层适配器设计有效解决了多中心心脏MRI重建的域偏移问题，为临床部署提供了可行的解决方案。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [147] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 提出了一种新颖的多尺度扫描可视化技术，通过语义分割和视觉语言模型识别重要物体，生成球形代理来指导用户采集图像，以改善3D高斯溅射等视图合成算法的输入质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视图合成算法（如3D高斯溅射）在渲染质量和速度方面已取得很大进展，但缺乏有效指导用户采集高质量输入图像的方法。人类操作者往往匆忙、缺乏耐心或不理解场景结构，难以实现均匀密集的视图采样要求。

Method: 利用语义分割和类别识别技术，通过视觉语言模型对物体进行排序，为高排名的重要物体生成球形代理，在扫描过程中实时指导用户获取更好的视图覆盖，特别是针对视角相关的材质特性。

Result: 在真实场景中相比传统视图采样策略表现出优越性能，能够有效识别需要扩展图像覆盖的重要物体，提升视图合成的质量。

Conclusion: 该方法通过多尺度扫描可视化技术成功解决了高质量视图合成所需的输入图像采集问题，为虚拟现实等应用提供了实用的图像采集指导方案。

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [148] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 提出了第一个大规模人体形状编辑数据集和基于扩散模型的Odo方法，能够通过语义属性引导实现真实的人体形状变换，在保持身份、服装和背景一致性的同时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前人体形状编辑方法依赖3D可变形模型或图像变形，存在身体比例不真实、纹理扭曲和背景不一致等问题，且缺乏大规模公开数据集进行训练和评估。

Method: 构建包含18,573张图像的大规模数据集，提出Odo端到端扩散方法，结合冻结UNet保持外观细节和ControlNet使用目标SMPL深度图引导形状变换。

Result: 方法显著优于基线，顶点重建误差低至7.5mm（基线为13.6mm），能够生成真实且准确匹配目标形状的结果。

Conclusion: 该工作填补了人体形状编辑领域的数据集空白，提出的Odo方法实现了高质量、可控的人体形状编辑，为后续研究提供了重要基础。

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [149] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 提出两阶段多模态框架，利用MIMIC-Eye数据集提升胸部X光疾病分类和区域感知放射报告生成。第一阶段通过注视引导对比学习改善分类性能，第二阶段通过模块化管道生成区域对齐报告。


<details>
  <summary>Details</summary>
Motivation: 利用放射科医生的眼动追踪数据来增强疾病分类的准确性和生成报告的临床相关性，提高医疗AI系统的可解释性和实用性。

Method: 两阶段方法：1）注视引导对比学习架构，整合视觉特征、临床标签、边界框和眼动信号，使用多术语注视注意力损失；2）模块化报告生成管道，提取置信度加权的诊断关键词，通过结构化提示生成区域对齐句子。

Result: 注视数据整合使F1分数从0.597提升至0.631（+5.70%），AUC从0.821提升至0.849（+3.41%），同时改善了精确率和召回率。报告生成在临床关键词召回率和ROUGE重叠度方面均有提升。

Conclusion: 整合注视数据不仅能提升分类性能，还能增强生成医疗报告的可解释性，证明了多模态方法在医疗影像分析中的有效性。

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [150] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 使用Stable Diffusion生成ID卡真实样本的合成图像，以解决Presentation Attack Detection系统训练数据不足的问题，提升检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前ID卡防伪检测系统面临真实样本图像数量有限和攻击手段多样化的挑战，大多数算法只关注生成攻击样本，而忽略了真实样本的稀缺性。

Method: 采用Stable Diffusion技术生成ID卡真实样本的合成版本，通过在从头训练的系统和一个商业解决方案中评估这些新生成的图像。

Result: 防伪检测系统将生成的合成图像识别为真实样本，这对检测性能和数据限制产生了积极影响。

Conclusion: 这是首批提出使用合成图像来模拟真实ID卡样本的研究之一，为解决训练数据稀缺问题提供了有效途径，并能提升防伪检测系统的泛化能力。

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [151] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 本文提出了一个新的遥感视觉问答数据集Chessboard和可解释模型Checkmate，通过细粒度视觉推理解决RSVQA中的可解释性和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 当前遥感视觉问答模型缺乏可解释性，存在数据集偏见导致捷径学习的问题，需要开发更透明和可信的决策系统。

Method: 创建包含3,123,253个问题的Chessboard数据集，答案分布均衡且每个答案与图像中的特定单元格相关联；开发Checkmate模型能够识别与决策最相关的图像单元格。

Result: 通过多个模型架构的广泛实验证明，该方法提高了RSVQA系统的透明度和可信度。

Conclusion: Chessboard数据集和Checkmate模型为遥感视觉问答提供了更好的可解释性和偏见缓解方案，支持更可信的决策制定。

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [152] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 提出DMS方法，利用扩散模型的几何先验合成沿极线方向的新视角图像，解决自监督立体匹配和单目深度估计中的遮挡和缺失像素问题


<details>
  <summary>Details</summary>
Motivation: 自监督方法使用立体图像作为监督信号时，在遮挡区域和出框区域存在对应像素缺失的问题，导致光度重建存在模糊性，需要进一步研究解决

Method: 微调Stable Diffusion模型，通过方向提示生成关键位置的新视角图像（左-左视角、右-右视角以及左右相机之间的中间视角），为遮挡像素提供补充，实现显式光度重建

Result: 在多个基准数据集上实现最先进性能，异常值减少高达35%

Conclusion: DMS是一种即插即用的模型无关方法，仅需未标记的立体图像对即可显著提升自监督立体匹配和单目深度估计性能

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [153] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: 本研究比较了RT-DETR-L和RT-DETR-X两种模型在沙滩垃圾检测中的性能，发现RT-DETR-X精度略高但计算成本显著，RT-DETR-L在速度与精度间取得更好平衡，更适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 海岸污染是全球性环境问题，需要可扩展的自动化监测解决方案。本研究旨在探索最先进的端到端目标检测模型RT-DETR在沙滩垃圾自动检测和计数中的应用效果。

Method: 使用公开可用的海岸碎片数据集，对RT-DETR-Large和RT-DETR-Extra-Large两种模型变体进行严格的比较分析，评估其检测精度和推理时间。

Result: RT-DETR-X模型获得略高的精度（mAP@50: 0.816, mAP@50-95: 0.612），但推理时间较长（34.5ms）；RT-DETR-L模型精度稍低（mAP@50: 0.810, mAP@50-95: 0.606）但推理速度快得多（20.1ms）。

Conclusion: RT-DETR-L模型在处理速度和检测精度之间提供了更好的平衡，是更实用和高效的实时现场部署解决方案，为基于Transformer的先进检测器在环境保护中的应用提供了重要见解。

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [154] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion是一个无需训练的框架，用于在不同骨骼拓扑结构的角色之间进行动画迁移，仅需目标骨骼上的少量示例运动即可实现高效可靠的跨拓扑运动迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同骨骼拓扑结构角色间动画迁移的挑战，当前缺乏大规模配对运动数据集限制了数据驱动方法的发展，且拓扑不一致性阻碍了直接的一对一骨骼对应关系建立。

Method: 提出训练免费的Motion2Motion框架，通过访问源骨骼和目标骨骼之间的稀疏骨骼对应关系，仅需目标骨骼上的一个或几个示例运动即可工作。

Result: 通过全面的定性和定量评估，证明Motion2Motion在相似骨骼和跨物种骨骼迁移场景中都能实现高效可靠的性能，并成功集成到下游应用和用户界面中。

Conclusion: 该方法具有工业应用潜力，代码和数据已公开，为跨拓扑运动迁移提供了实用的解决方案。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse是一个新颖的3D场景重建框架，通过融合多视角扫描数据来重建交互式高斯场景，解决了物体遮挡和传感器覆盖限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景重建方法存在多阶段流程复杂、需要密集扫描、容易出错且难以扩展的问题，特别是在处理物体遮挡和传感器覆盖限制方面表现不佳。

Method: 构建分割感知的高斯场，强制执行双向光度和语义一致性，引入伪中间场景状态进行统一对齐，采用协作共剪枝策略来优化几何结构。

Result: 实验验证了该框架对新场景配置的强泛化能力，能够实现高保真渲染和对象级场景操作，无需密集观测或复杂流程。

Conclusion: IGFuse为真实世界3D重建和真实到仿真的转换提供了有效的解决方案，在克服传统方法局限性方面表现出色。

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是首个从单张图像生成4D（动态3D）场景表示的端到端前馈框架，通过微调预训练视频扩散模型实现高效图像到4D生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有4D生成方法依赖计算密集型优化或需要多帧视频输入的问题，提供更高效的解决方案。

Method: 1) 构建大规模4D数据集4DNeX-10M；2) 引入统一6D视频表示联合建模RGB和XYZ序列；3) 提出适配策略将预训练视频扩散模型重新用于4D建模。

Result: 生成高质量动态点云，支持新视角视频合成，在效率和泛化性方面优于现有4D生成方法。

Conclusion: 4DNeX为图像到4D建模提供了可扩展解决方案，为生成式4D世界模型模拟动态场景演化奠定了基础。

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [157] [Complementary bodies in sphere packing](https://arxiv.org/abs/2508.11633)
*Philip W. Kuchel*

Main category: cs.CG

TL;DR: 使用符号和图形工具分析立方密堆积中的空隙空间，将空隙划分为球形截断多面体，推导其表面积体积比，并研究截断四面体和八面体的堆积密度


<details>
  <summary>Details</summary>
Motivation: 研究球体堆积中的空隙结构对于几何学研究以及在多孔介质扩散和生物组织建模等应用具有重要意义

Method: 使用Mathematica等工具进行精确可视化和分析，将立方密堆积中的空隙划分为球形截断多面体，推导其几何属性

Result: 得出了球形截断多面体的表面积体积比，证明了截断四面体和八面体如何铺满球体堆积的间隙空间

Conclusion: 这些发现有助于更深入地理解经典堆积问题及其几何补体结构

Abstract: Symbolic and graphical tools, such as Mathematica, enable precise
visualization and analysis of void spaces in sphere packings. In the cubic
close packing (CCP, or face-centred cubic packing; FCC) arrangement these voids
can be partitioned into repeating geometric units we term spherically truncated
polyhedra - bodies analogous to plane-truncated polyhedra but bounded by both
planar and spherical surfaces. These structures are relevant in geometric
studies and applications such as modelling diffusion in porous media and
biological tissues. This work examines the properties of these complementary
bodies, deriving their surface area-to-volume ratios, which are significant in
physical contexts; and we establish a result concerning the packing density of
truncated tetrahedra and octahedra, demonstrating how they tile the
interstitial space surrounding packed spheres. These findings contribute to a
deeper understanding of classical packing problems and their geometrical
complements.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: 本文探讨如何将大型语言模型(LLMs)集成到物理机器人中，以增强机器人理解自然语言的能力，实现与人类更好的协作。


<details>
  <summary>Details</summary>
Motivation: 传统交互式任务学习系统语言理解能力有限，而大型语言模型的出现为提升机器人语言理解能力提供了机会，但将LLMs与物理世界中的机器人集成是一个挑战。

Method: 提出一种以认知代理为核心的AI系统架构，该代理控制物理机器人，与人类和LLM交互，并通过经验积累情境知识。通过使用ChatGPT进行概念验证实验来应对三个具体的语言理解挑战。

Result: 通过简单的概念验证实验展示了LLM在机器人语言理解方面的潜力，但需要进一步开发才能成为可操作的系统。

Conclusion: 将LLM辅助的语言理解集成到机器人助手中，需要开发完整的系统架构来实现与人类使用自然语言进行协作的愿景。

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [159] [Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots](https://arxiv.org/abs/2508.11802)
*Luigi Penco,Beomyeong Park,Stefan Fasano,Nehar Poddar,Stephen McCrory,Nicholas Kitchel,Tomasz Bialek,Dexton Anderson,Duncan Calvert,Robert Griffin*

Main category: cs.RO

TL;DR: 提出了一种实时步态重定向方法，通过预测用户脚步并适应性地调整机器人步态，实现人机运动同步，解决了高速任务中的延迟和地形不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决高速遥操作中人机运动同步的挑战，特别是在用户与机器人环境不匹配的情况下保持机器人的平衡和稳定性。

Method: 通过预测用户脚步位置并重定向到机器人脚步位置，让机器人利用自身动力学进行运动，同时持续适应性地调整步态估计以匹配用户参考，并自主调整机器人步态以适应周围地形。

Result: 在Nadia人形机器人上的实验结果表明，该系统能够有效实现人机运动同步，减少延迟，并处理地形不匹配问题。

Conclusion: 该方法通过步态重定向和预测性调整，成功实现了高速遥操作中的人机同步，提高了机器人的运动稳定性和环境适应性。

Abstract: Achieving seamless synchronization between user and robot motion in
teleoperation, particularly during high-speed tasks, remains a significant
challenge. In this work, we propose a novel approach for transferring stepping
motions from the user to the robot in real-time. Instead of directly
replicating user foot poses, we retarget user steps to robot footstep
locations, allowing the robot to utilize its own dynamics for locomotion,
ensuring better balance and stability. Our method anticipates user footsteps to
minimize delays between when the user initiates and completes a step and when
the robot does it. The step estimates are continuously adapted to converge with
the measured user references. Additionally, the system autonomously adjusts the
robot's steps to account for its surrounding terrain, overcoming challenges
posed by environmental mismatches between the user's flat-ground setup and the
robot's uneven terrain. Experimental results on the humanoid robot Nadia
demonstrate the effectiveness of the proposed system.

</details>


### [160] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: LocoMamba是一个基于Mamba选择性状态空间模型的视觉驱动跨模态深度强化学习框架，通过近线性时间序列建模实现高效训练，在复杂环境中表现出优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法在处理长序列依赖、高延迟和大内存占用方面的局限性，需要开发一个能够高效融合视觉和本体感知信息，同时在复杂地形和障碍物环境中实现稳健导航的框架。

Method: 1) 使用多层感知机嵌入本体感知状态，轻量CNN处理深度图像生成紧凑token；2) 堆叠Mamba层通过选择性扫描融合token，实现近线性时间建模；3) 使用PPO算法在随机化地形和外观下进行端到端策略训练，采用障碍物密度课程学习和紧凑状态中心奖励函数。

Result: 在包含静态/动态障碍物和不平地形的模拟环境中，相比最先进基线方法，LocoMamba获得更高回报和成功率，碰撞更少，对未见地形和障碍物密度表现出更强泛化能力，在相同计算预算下以更少更新次数收敛。

Conclusion: LocoMamba框架通过选择性状态空间模型有效解决了长序列依赖和计算效率问题，为视觉驱动的机器人导航提供了高效且泛化能力强的解决方案。

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [161] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 本文研究了自动驾驶目标检测中的数据偏移问题，提出了基于CycleGAN数据增强和YOLOv5的优化方法，在BDD100K数据集上取得了优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中机器学习模型性能严重依赖于训练和测试数据满足独立同分布假设，但现实中季节变化、天气波动等因素导致数据分布动态变化，造成数据偏移问题。

Method: 系统分析数据偏移问题的复杂性和表现形式，综述数据偏移检测方法，使用偏移检测分析技术进行数据集分类和平衡，构建目标检测模型，将CycleGAN数据增强技术与YOLOv5框架集成进行模型优化。

Result: 在BDD100K数据集上的实验结果表明，该方法相比基线模型取得了更优越的性能。

Conclusion: 通过系统分析数据偏移问题并采用CycleGAN数据增强技术，能够有效提升自动驾驶目标检测模型在现实复杂环境中的鲁棒性和性能表现。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [162] [Bioinspired underwater soft robots: from biology to robotics and back](https://arxiv.org/abs/2508.11883)
*Lei Li,Boyang Qin,Wenzhuo Gao,Yanyu Li,Yiyuan Zhang,Bo Wang,Shihan Kong,Jian Wang,Dekui He,Junzhi Yu*

Main category: cs.RO

TL;DR: 本文提出了一个双向的生物启发软体机器人框架，不仅从生物学获取灵感，还能通过机器人实验反馈验证生物学假设，并引入跨物种通用设计的新范式。


<details>
  <summary>Details</summary>
Motivation: 海洋探索和软体生物研究需要新的机器人技术，但目前生物学对机器人的启发是单向的，缺乏从机器人反馈到生物学的闭环验证机制。

Method: 建立整体性双向框架，整合生物学原理、机器人实现和生物学验证，利用软体机器人作为实验工具来探究生物功能和测试进化假说。

Result: 软体机器人凭借其顺应性在非结构化环境中优于刚性系统，支持海洋探索、操作和医疗应用，并能反馈验证生物学理论。

Conclusion: 通过融合生物学和工程学，软体机器人可以推动海洋探索并深化科学发现，但材料耐久性、驱动效率、自主性和智能性等方面仍存在挑战。

Abstract: The ocean vast unexplored regions and diverse soft-bodied marine organisms
have spurred interest in bio-inspired underwater soft robotics. Recent advances
have enabled new capabilities in underwater movement, sensing, and interaction.
However, these efforts are largely unidirectional, with biology guiding
robotics while insights from robotics rarely feed back into biology. Here we
propose a holistic, bidirectional framework that integrates biological
principles, robotic implementation, and biological validation. We show that
soft robots can serve as experimental tools to probe biological functions and
even test evolutionary hypotheses. Their inherent compliance also allows them
to outperform rigid systems in unstructured environments, supporting
applications in marine exploration, manipulation, and medicine. Looking
forward, we introduce bio-universal-inspired robotics, a paradigm that
transcends species-specific mimicry by identifying convergent principles across
species to inspire more adaptable designs. Despite rapid progress, challenges
persist in material robustness, actuation efficiency, autonomy, and
intelligence. By uniting biology and engineering, soft robots can advance ocean
exploration and deepen scientific discovery.

</details>


### [163] [From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics](https://arxiv.org/abs/2508.11884)
*Havel Liu,Mingzhang Zhu,Arturo Moises Flores Alvarez,Yuan Hung Lo,Conrad Ku,Federico Parres,Justin Quan,Colin Togashi,Aditya Navghare,Quanyou Wang,Dennis W. Hong*

Main category: cs.RO

TL;DR: Kid Cosmo是一个面向娱乐应用的儿童尺寸人形机器人平台，具有28个自由度，能够实现扭矩控制行走和逼真动作生成，在保持角色形象的同时具备技术功能性


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人主要关注功能性设计，而娱乐领域更注重视觉效果和形态，需要开发既能体现角色特征又具备技术功能的表演导向人形机器人

Method: 开发了Kid Cosmo儿童尺寸人形机器人平台，采用本体感受执行器实现扭矩控制行走和逼真动作生成，具备28个自由度，高1.45米，重25公斤

Result: 在全球展示中验证了系统架构的可行性，解决了功能性娱乐机器人的挑战，展示了上下半身同时运动时的稳定性

Conclusion: 证明了优先考虑角色体现和技术功能性的表演导向人形机器人的可行性，为娱乐机器人领域提供了新的研究方向

Abstract: Humanoid robots represent the cutting edge of robotics research, yet their
potential in entertainment remains largely unexplored. Entertainment as a field
prioritizes visuals and form, a principle that contrasts with the purely
functional designs of most contemporary humanoid robots. Designing
entertainment humanoid robots capable of fluid movement presents a number of
unique challenges. In this paper, we present Kid Cosmo, a research platform
designed for robust locomotion and life-like motion generation while imitating
the look and mannerisms of its namesake character from Netflix's movie The
Electric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall
and weighing 25 kg. It contains 28 degrees of freedom and primarily uses
proprioceptive actuators, enabling torque-control walking and lifelike motion
generation. Following worldwide showcases as part of the movie's press tour, we
present the system architecture, challenges of a functional entertainment robot
and unique solutions, and our initial findings on stability during simultaneous
upper and lower body movement. We demonstrate the viability of
performance-oriented humanoid robots that prioritize both character embodiment
and technical functionality.

</details>


### [164] [Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System](https://arxiv.org/abs/2508.11885)
*Haixin Gong,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 开发了一种新型可变形足部模型，集成到完整肌肉骨骼系统中，通过两阶段策略训练实现了更自然步态模拟，相比传统刚性模型在运动学、动力学和步态稳定性方面均有改进。


<details>
  <summary>Details</summary>
Motivation: 现有肌肉骨骼模型过度简化足地接触力学，无法准确模拟人类步态动力学，需要开发更精确的接触界面模型。

Method: 开发了接触丰富且可变形的人足模型，集成到完整肌肉骨骼系统中，采用两阶段策略训练学习自然行走模式，解决多点接触和可变形材料控制难题。

Result: 相比传统刚性模型，在运动学、动力学和步态稳定性指标上均有改进，验证实验证实模拟结果与真实人体生物力学测量数据高度吻合。

Conclusion: 这项工作推进了人类肌肉骨骼系统的接触丰富界面建模，建立了可扩展到需要精确足地交互控制的人形机器人应用的稳健框架。

Abstract: The human foot serves as the critical interface between the body and
environment during locomotion. Existing musculoskeletal models typically
oversimplify foot-ground contact mechanics, limiting their ability to
accurately simulate human gait dynamics. We developed a novel contact-rich and
deformable model of the human foot integrated within a complete musculoskeletal
system that captures the complex biomechanical interactions during walking. To
overcome the control challenges inherent in modeling multi-point contacts and
deformable material, we developed a two-stage policy training strategy to learn
natural walking patterns for this interface-enhanced model. Comparative
analysis between our approach and conventional rigid musculoskeletal models
demonstrated improvements in kinematic, kinetic, and gait stability metrics.
Validation against human subject data confirmed that our simulation closely
reproduced real-world biomechanical measurements. This work advances
contact-rich interface modeling for human musculoskeletal systems and
establishes a robust framework that can be extended to humanoid robotics
applications requiring precise foot-ground interaction control.

</details>


### [165] [Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards](https://arxiv.org/abs/2508.11887)
*Yousra Shleibik,Jordan Sinclair,Kerstin Haring*

Main category: cs.RO

TL;DR: 提出了一种通过视线追踪和视听提示来增强半自动驾驶中驾驶员情境意识的注意力重定向框架


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术向更高水平发展，需要集成系统来支持人类在决策中的参与，特别是在车辆无法识别场景中的对象或元素时需要人类介入的情况下，驾驶员的情境意识对于降低接管过程中的风险至关重要

Method: 提出了一个概念框架，结合实时视线追踪、上下文感知的显著性分析以及同步的视觉和听觉警报，通过目标视觉和听觉线索进行视线操纵来帮助驾驶员保持对新兴危险的关注

Result: 该框架旨在增强情境意识，主动应对潜在危险，并促进人类与自动驾驶系统之间的有效协作

Conclusion: 注意力重定向技术对于提高半自动驾驶场景中的安全性和人机协作效率具有重要意义

Abstract: The advent of autonomous driving systems promises to transform transportation
by enhancing safety, efficiency, and comfort. As these technologies evolve
toward higher levels of autonomy, the need for integrated systems that
seamlessly support human involvement in decision-making becomes increasingly
critical. Certain scenarios necessitate human involvement, including those
where the vehicle is unable to identify an object or element in the scene, and
as such cannot take independent action. Therefore, situational awareness is
essential to mitigate potential risks during a takeover, where a driver must
assume control and autonomy from the vehicle. The need for driver attention is
important to avoid collisions with external agents and ensure a smooth
transition during takeover operations. This paper explores the integration of
attention redirection techniques, such as gaze manipulation through targeted
visual and auditory cues, to help drivers maintain focus on emerging hazards
and reduce target fixation in semi-autonomous driving scenarios. We propose a
conceptual framework that combines real-time gaze tracking, context-aware
saliency analysis, and synchronized visual and auditory alerts to enhance
situational awareness, proactively address potential hazards, and foster
effective collaboration between humans and autonomous systems.

</details>


### [166] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: 提出了AMAD-SRL框架，将符号强化学习集成到无人机认知多智能体架构中，在软件在环环境中验证了75%的任务效率提升


<details>
  <summary>Details</summary>
Motivation: 现代无人机任务需要将结构化符号规划与自适应强化学习无缝集成，传统基于规则的架构在动态复杂环境中适应性不足

Method: 基于AMAD认知多智能体架构扩展，采用符号强化学习（SRL）和PDDL规划语言，在软件在环环境中验证

Result: 实现了模块稳定集成和互操作性，任务效率相比基于覆盖的基线提高了约75%（通过旅行距离减少衡量）

Conclusion: 为处理复杂无人机任务建立了坚实基础，讨论了进一步改进和验证的方向

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [167] [OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation](https://arxiv.org/abs/2508.11898)
*Jilei Mao,Jiarui Guan,Yingjuan Tang,Qirui Hu,Zhihang Li,Junjie Yu,Yongjie Mao,Yunzhe Sun,Shuang Liu,Xiaozhu Ju*

Main category: cs.RO

TL;DR: OmniD是一个多视角融合框架，通过可变形注意力机制将多视角图像合成为统一的鸟瞰图表示，解决了视觉运动策略在训练数据上的过拟合问题，显著提升了分布内、分布外和少样本场景的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉运动策略容易在训练数据上过拟合（如固定相机位置和背景），导致在分布外泛化性能下降，且现有方法难以有效融合多视角信息生成有效的3D表示。

Method: 提出Omni-Vision Diffusion Policy (OmniD)框架，使用基于可变形注意力的Omni-Feature Generator (OFG)选择性提取任务相关特征，抑制视角特定噪声和背景干扰，将多视角观测合成为统一的鸟瞰图表示。

Result: 在分布内、分布外和少样本实验中，OmniD分别比最佳基线模型平均提升了11%、17%和84%的性能。

Conclusion: OmniD通过有效的多视角融合和特征选择机制，显著提升了视觉运动策略的泛化能力，特别是在分布外和少样本场景下表现突出。

Abstract: The visuomotor policy can easily overfit to its training datasets, such as
fixed camera positions and backgrounds. This overfitting makes the policy
perform well in the in-distribution scenarios but underperform in the
out-of-distribution generalization. Additionally, the existing methods also
have difficulty fusing multi-view information to generate an effective 3D
representation. To tackle these issues, we propose Omni-Vision Diffusion Policy
(OmniD), a multi-view fusion framework that synthesizes image observations into
a unified bird's-eye view (BEV) representation. We introduce a deformable
attention-based Omni-Feature Generator (OFG) to selectively abstract
task-relevant features while suppressing view-specific noise and background
distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the
best baseline model for in-distribution, out-of-distribution, and few-shot
experiments, respectively. Training code and simulation benchmark are
available: https://github.com/1mather/omnid.git

</details>


### [168] [Control of Legged Robots using Model Predictive Optimized Path Integral](https://arxiv.org/abs/2508.11917)
*Hossein Keshavarz,Alejandro Ramirez-Serrano,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出MPOPI算法，结合MPPI、CE和CMA方法，提高了腿式机器人的运动控制效率和性能


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在复杂地形中具有独特优势，但现有控制方法尚未达到自然系统的水平，需要更高效的实时运动生成算法

Method: 采用基于采样的模型预测控制策略，结合模型预测路径积分(MPPI)、交叉熵(CE)和协方差矩阵自适应(CMA)方法

Result: MPOPI算法展现出更好的采样效率，使用更少样本就能获得优于典型MPPI算法的运动性能，在四足机器人多场景仿真实验中表现优异

Conclusion: MPOPI可作为随时控制策略，每次迭代都能提升运动能力，为腿式机器人实时全身运动控制提供了有效解决方案

Abstract: Legged robots possess a unique ability to traverse rough terrains and
navigate cluttered environments, making them well-suited for complex,
real-world unstructured scenarios. However, such robots have not yet achieved
the same level as seen in natural systems. Recently, sampling-based predictive
controllers have demonstrated particularly promising results. This paper
investigates a sampling-based model predictive strategy combining model
predictive path integral (MPPI) with cross-entropy (CE) and covariance matrix
adaptation (CMA) methods to generate real-time whole-body motions for legged
robots across multiple scenarios. The results show that combining the benefits
of MPPI, CE and CMA, namely using model predictive optimized path integral
(MPOPI), demonstrates greater sample efficiency, enabling robots to attain
superior locomotion results using fewer samples when compared to typical MPPI
algorithms. Extensive simulation experiments in multiple scenarios on a
quadruped robot show that MPOPI can be used as an anytime control strategy,
increasing locomotion capabilities at each iteration.

</details>


### [169] [ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models](https://arxiv.org/abs/2508.11918)
*Zhichen Lou,Kechun Xu,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: ExploreVLM是一个基于视觉语言模型的闭环任务规划框架，通过逐步反馈机制实现实时计划调整和交互式探索，在探索型任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM方法在交互探索、精确感知和实时计划适应方面存在不足，需要开发能够更好处理动态环境的机器人任务规划系统。

Method: 采用双阶段任务规划器（带自反思机制）和以物体为中心的空间关系图，提供结构化语言基础场景表示，配合执行验证器形成闭环系统。

Result: 在真实世界实验中显著优于最先进的基线方法，特别是在探索型任务中表现突出。消融研究验证了反思规划器和结构化感知的关键作用。

Conclusion: ExploreVLM框架通过闭环规划和结构化感知实现了鲁棒高效的任务执行，为具身智能在动态环境中的实际应用提供了有效解决方案。

Abstract: The advancement of embodied intelligence is accelerating the integration of
robots into daily life as human assistants. This evolution requires robots to
not only interpret high-level instructions and plan tasks but also perceive and
adapt within dynamic environments. Vision-Language Models (VLMs) present a
promising solution by combining visual understanding and language reasoning.
However, existing VLM-based methods struggle with interactive exploration,
accurate perception, and real-time plan adaptation. To address these
challenges, we propose ExploreVLM, a novel closed-loop task planning framework
powered by Vision-Language Models (VLMs). The framework is built around a
step-wise feedback mechanism that enables real-time plan adjustment and
supports interactive exploration. At its core is a dual-stage task planner with
self-reflection, enhanced by an object-centric spatial relation graph that
provides structured, language-grounded scene representations to guide
perception and planning. An execution validator supports the closed loop by
verifying each action and triggering re-planning. Extensive real-world
experiments demonstrate that ExploreVLM significantly outperforms
state-of-the-art baselines, particularly in exploration-centric tasks. Ablation
studies further validate the critical role of the reflective planner and
structured perception in achieving robust and efficient task execution.

</details>


### [170] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的全向双足运动学习框架，通过结合盲控制器和教师策略来训练视觉学生策略，避免了昂贵的渲染成本，实现了高效的全向地形适应运动。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中实现有效的双足运动需要全向地形感知和能够处理此类输入的控制器，但传统方法中渲染全向深度图像的计算成本过高，使得模拟到现实的强化学习不实用。

Method: 结合鲁棒的盲控制器和教师策略来监督基于视觉的学生策略，使用噪声增强的地形数据进行训练以避免RL期间的渲染成本，并引入数据增强技术加速监督训练。

Result: 在仿真和真实世界测试中验证了框架的有效性，展示了全向运动能力，对多样化地形具有适应性，训练速度比传统方法快10倍。

Conclusion: 这是首个基于视觉的全向双足运动演示，展示了在最小化昂贵渲染依赖的情况下实现多样化地形适应的可行性。

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [171] [Toward General Physical Intelligence for Resilient Agile Manufacturing Automation](https://arxiv.org/abs/2508.11960)
*Sandeep Kanta,Mehrdad Tavassoli,Varun Teja Chirkuri,Venkata Akhil Kumar,Santhi Bharath Punati,Praveen Damacharla,Sunny Katyara*

Main category: cs.RO

TL;DR: 这篇综述文章系统性地调查了视觉语言动作(VLA)模型在通用物理智能(GPI)背景下的最新进展，通过结构化消融研究评估了主要实现的工业部署准备度，并提出了将GPI整合到下一代工业生态系统中的研究方向。


<details>
  <summary>Details</summary>
Motivation: 敏捷和以人为本的制造需要能够在非结构化环境中进行情境推理和安全交互的弹性机器人解决方案。虽然GPI已在文献中被描述，但其在当代敏捷制造过程中的实际应用和演变作用尚未得到充分探索。

Method: 系统性地调查VLA模型在GPI背景下的最新进展，对领先实现进行全面的比较分析，并通过结构化消融研究评估其工业部署准备度。将最先进技术组织为五个主题支柱进行分析。

Result: 分析将最先进技术组织为五个主题支柱：多感官表征学习、模拟到现实迁移、规划与控制、不确定性和安全措施、基准测试。

Conclusion: 文章阐明了开放的研究挑战，并提出了将GPI更好地整合到符合工业5.0标准的下一代工业生态系统中的方向。

Abstract: Agile and human-centric manufacturing stipulates resilient robotic solutions
capable of contextual reasoning and safe interaction in unstructured
environments. Foundation models particularly the Vision Language Action (VLA)
models have emerged to fuse multimodal perception, reasoning and physically
grounded action across varied embodiments into unified representation, termed
as General Physical Intelligence (GPI). While GPI has already been described in
the literature but its practical application and evolving role in contemporary
agile manufacturing processes have yet to be duly explored. To bridge this gap,
this practical review systematically surveys recent advancements in VLA models
within GPI context, performs comprehensive comparative analysis of leading
implementations and evaluates their readiness for industrial deployment through
structured ablation study. Our analysis has organized state-of-the-art into
five thematic pillars including multisensory representation learning, sim2real
transfer, planning and control, uncertainty and safety measures and
benchmarking. Finally, we articulate open research challenges and propose
directions to better integrate GPI into next-generation industrial ecosystems
in line with Industry 5.0.

</details>


### [172] [Fully Spiking Actor-Critic Neural Network for Robotic Manipulation](https://arxiv.org/abs/2508.12038)
*Liwen Zhang,Heng Deng,Guanghui Sun*

Main category: cs.RO

TL;DR: 提出基于全脉冲神经网络的混合课程强化学习框架，用于9自由度机械臂的目标抓取任务，通过简化网络结构、时间进度分区课程策略和能耗建模，实现高性能和低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统人工神经网络在机器人控制任务中计算复杂度高、能耗大的问题，利用脉冲神经网络的高推理速度、低能耗和生物合理性优势，为资源受限环境提供高效解决方案。

Method: 使用简化的全脉冲神经网络（仅输入输出层），结合PPO算法和时间进度分区课程策略，引入能耗建模框架、动态两阶段奖励调整机制和优化观测空间。

Result: 在Isaac Gym仿真平台上验证了方法在真实物理约束下的优越性能，相比传统PPO和ANN基线显示出更好的可扩展性和能源效率。

Conclusion: 提出的混合CRL框架在动态机器人操作任务中具有显著优势，为资源受限环境下的高效机器人控制提供了可行方案。

Abstract: This study proposes a hybrid curriculum reinforcement learning (CRL)
framework based on a fully spiking neural network (SNN) for 9-degree-of-freedom
robotic arms performing target reaching and grasping tasks. To reduce network
complexity and inference latency, the SNN architecture is simplified to include
only an input and an output layer, which shows strong potential for
resource-constrained environments. Building on the advantages of SNNs-high
inference speed, low energy consumption, and spike-based biological
plausibility, a temporal progress-partitioned curriculum strategy is integrated
with the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy
consumption modeling framework is introduced to quantitatively compare the
theoretical energy consumption between SNNs and conventional Artificial Neural
Networks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized
observation space further improve learning efficiency and policy accuracy.
Experiments on the Isaac Gym simulation platform demonstrate that the proposed
method achieves superior performance under realistic physical constraints.
Comparative evaluations with conventional PPO and ANN baselines validate the
scalability and energy efficiency of the proposed approach in dynamic robotic
manipulation tasks.

</details>


### [173] [Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs](https://arxiv.org/abs/2508.12043)
*Fei Lin,Tengchao Zhang,Qinghua Ni,Jun Huang,Siji Ma,Yonglin Tian,Yisheng Lv,Naiqi Wu*

Main category: cs.RO

TL;DR: 本文探讨了LLM驱动的无人机群在带宽受限条件下实现自主语义压缩通信的可行性，通过构建4种复杂度的2D仿真场景，评估了9种主流LLM的语义压缩性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群中LLM的快速应用增强了语义理解和自主任务执行能力，但有限通信带宽和高频交互需求对语义信息传输提出了严峻挑战。

Method: 构建四种不同环境复杂度的2D仿真场景，设计集成系统提示和任务指令提示的通信-执行流水线，系统评估9种主流LLM在不同场景下的语义压缩性能，并通过环境复杂度和群规模的消融实验分析适应性和稳定性。

Result: 实验结果表明，基于LLM的无人机群有潜力在带宽受限和多跳链路条件下实现高效的协同通信。

Conclusion: LLM驱动的无人机群能够有效减少通信负载同时保留关键任务语义，为带宽受限环境下的协同通信提供了可行解决方案。

Abstract: The rapid adoption of Large Language Models (LLMs) in unmanned systems has
significantly enhanced the semantic understanding and autonomous task execution
capabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited
communication bandwidth and the need for high-frequency interactions pose
severe challenges to semantic information transmission within the swarm. This
paper explores the feasibility of LLM-driven UAV swarms for autonomous semantic
compression communication, aiming to reduce communication load while preserving
critical task semantics. To this end, we construct four types of 2D simulation
scenarios with different levels of environmental complexity and design a
communication-execution pipeline that integrates system prompts with task
instruction prompts. On this basis, we systematically evaluate the semantic
compression performance of nine mainstream LLMs in different scenarios and
analyze their adaptability and stability through ablation studies on
environmental complexity and swarm size. Experimental results demonstrate that
LLM-based UAV swarms have the potential to achieve efficient collaborative
communication under bandwidth-constrained and multi-hop link conditions.

</details>


### [174] [OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments](https://arxiv.org/abs/2508.12071)
*Amy Phung,Richard Camilli*

Main category: cs.RO

TL;DR: OASIS是一种实时水下3D重建方法，通过光学相机和声纳的融合，结合体素雕刻技术，实现非结构化水下工作空间的实时重建


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注离线重建，而实时空间感知对于自主和有人驾驶水下车辆操作至关重要

Method: 采用"手眼"配置，利用机械臂灵活性捕获多视角数据，融合光学图像和声纳数据，结合体素雕刻技术

Result: 通过水箱实验验证，展示了定性和定量结果，突出了其在水下操作任务中的实用性

Conclusion: OASIS方法能够实现实时水下3D场景重建，为水下操作任务提供有效的空间感知能力

Abstract: High resolution underwater 3D scene reconstruction is crucial for various
applications, including construction, infrastructure maintenance, monitoring,
exploration, and scientific investigation. Prior work has leveraged the
complementary sensing modalities of imaging sonars and optical cameras for
opti-acoustic 3D scene reconstruction, demonstrating improved results over
methods which rely solely on either sensor. However, while most existing
approaches focus on offline reconstruction, real-time spatial awareness is
essential for both autonomous and piloted underwater vehicle operations. This
paper presents OASIS, an opti-acoustic fusion method that integrates data from
optical images with voxel carving techniques to achieve real-time 3D
reconstruction unstructured underwater workspaces. Our approach utilizes an
"eye-in-hand" configuration, which leverages the dexterity of robotic
manipulator arms to capture multiple workspace views across a short baseline.
We validate OASIS through tank-based experiments and present qualitative and
quantitative results that highlight its utility for underwater manipulation
tasks.

</details>


### [175] [Into the Wild: When Robots Are Not Welcome](https://arxiv.org/abs/2508.12075)
*Shaul Ashkenazi,Gabriel Skantze,Jane Stuart-Smith,Mary Ellen Foster*

Main category: cs.RO

TL;DR: 论文报告了在公共服务场所部署社交机器人时遇到的困难，包括技术挑战、用户意外反应和利益相关者的反对，但最终通过建立信任关系成功完成了部署研究


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在公共服务场所（学生服务中心和难民服务点）的实际部署挑战，探索如何克服技术困难和社会接受度问题

Method: 在两个不同的公共服务场所进行实地部署实验：学生服务中心和难民/寻求庇护者服务点，通过建立与工作人员的信任关系来推进机器人部署

Result: 尽管面临技术困难、意外用户反应和利益相关者的反对，但最终成功获得了工作人员的信任，建立了合作关系，完成了机器人部署并进行了研究

Conclusion: 在公共服务场所部署社交机器人时，建立与工作人员的信任关系至关重要，技术挑战和社会接受度问题可以通过持续沟通和关系建设来克服

Abstract: Social robots are increasingly being deployed in public spaces, where they
face not only technological difficulties and unexpected user utterances, but
also objections from stakeholders who may not be comfortable with introducing a
robot into those spaces. We describe our difficulties with deploying a social
robot in two different public settings: 1) Student services center; 2) Refugees
and asylum seekers drop-in service. Although this is a failure report, in each
use case we eventually managed to earn the trust of the staff and form a
relationship with them, allowing us to deploy our robot and conduct our
studies.

</details>


### [176] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: B-COD是一种基于扩散模型的规划器，通过单次前向传播生成轨迹和定位误差代理，结合强化学习在线选择最小传感器子集，在保证任务完成的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在部分可观测环境中持续开启所有传感器能耗过高的问题，寻找在保证任务完成的前提下每个位置所需的最小传感器子集。

Method: 提出Belief-Conditioned One-Step Diffusion (B-COD)规划器，将位姿信念栅格和传感器掩码作为条件输入，通过扩散模型生成轨迹并提供定位误差代理，结合soft-actor-critic在线选择传感器。

Result: 在无人水面车辆实时海洋试验中，B-COD在匹配全开传感器基线性能的同时显著降低了传感能耗。

Conclusion: B-COD通过数据驱动方法有效解决了传感器节能问题，为资源受限的机器人系统提供了实用的在线传感器管理方案。

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [177] [Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)](https://arxiv.org/abs/2508.12170)
*Aryan Gupta*

Main category: cs.RO

TL;DR: 对2020-2024年机器人软件层面能效方法的系统文献综述，涵盖79项研究，分析了应用领域、指标、评估类型、能耗模型、主要能耗组件、软件技术家族及能耗-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 更新和扩展2020年前的证据，系统梳理机器人软件层面能效优化方法的研究现状和发展趋势，为领域提供全面的研究概览和指导。

Method: 采用自动化但经过审核的流程，结合Google Scholar种子搜索、前后向滚雪球法和大语言模型辅助筛选与数据提取，每个自动化步骤有约10%的人工审核，全文决策采用共识加决断机制。

Result: 工业设置主导(31.6%)，探索领域次之(25.3%)；电机/执行器是主要能耗源(68.4%)；仿真评估最常见(51.9%)；物理基础能耗模型占主导(87.3%)；运动轨迹优化是主要技术家族(69.6%)。

Conclusion: 提出了最小报告清单建议，强调跨层设计机会和量化非性能权衡的重要性，提供了包含代码、提示和冻结数据集的复制包。

Abstract: This study presents a systematic literature review of software-level
approaches to energy efficiency in robotics published from 2020 through 2024,
updating and extending pre-2020 evidence. An automated-but-audited pipeline
combined Google Scholar seeding, backward/forward snowballing, and
large-language-model (LLM) assistance for screening and data extraction, with
~10% human audits at each automated step and consensus-with-tie-breaks for
full-text decisions. The final corpus comprises 79 peer-reviewed studies
analyzed across application domain, metrics, evaluation type, energy models,
major energy consumers, software technique families, and energy-quality
trade-offs. Industrial settings dominate (31.6%) followed by exploration
(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of
studies, with computing/controllers a distant second (13.9%). Simulation-only
evaluations remain most common (51.9%), though hybrid evaluations are frequent
(25.3%). Representational (physics-grounded) energy models predominate (87.3%).
Motion and trajectory optimization is the leading technique family (69.6%),
often paired with learning/prediction (40.5%) and computation
allocation/scheduling (26.6%); power management/idle control (11.4%) and
communication/data efficiency (3.8%) are comparatively underexplored. Reporting
is heterogeneous: composite objectives that include energy are most common,
while task-normalized and performance-per-energy metrics appear less often,
limiting cross-paper comparability. The review offers a minimal reporting
checklist (e.g., total energy and average power plus a task-normalized metric
and clear baselines) and highlights opportunities in cross-layer designs and in
quantifying non-performance trade-offs (accuracy, stability). A replication
package with code, prompts, and frozen datasets accompanies the review.

</details>


### [178] [Humanoid Motion Scripting with Postural Synergies](https://arxiv.org/abs/2508.12184)
*Rhea Malhotra,William Chong,Catie Cuan,Oussama Khatib*

Main category: cs.RO

TL;DR: SynSculptor是一个基于姿态协同作用的人形机器人运动分析与编辑框架，无需训练即可生成类人运动，通过PCA提取主要姿态协同作用并构建风格条件协同库。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人运动生成中参考运动收集分析困难、新运动合成复杂以及运动映射到机器人的挑战。

Method: 收集3+小时动作捕捉数据，使用实时操作空间控制器在仿真人形机器人上模仿人类运动，通过PCA提取速度轨迹的主要姿态协同作用，构建风格条件协同库，并利用运动-语言变换器进行运动适应。

Result: 开发了评估生成运动的指标（脚滑动比、运动平滑度、总动量和动能偏差），并与参考运动进行比较验证。

Conclusion: SynSculptor框架成功实现了基于姿态协同作用的训练自由类人运动脚本生成，为人形机器人运动控制提供了有效解决方案。

Abstract: Generating sequences of human-like motions for humanoid robots presents
challenges in collecting and analyzing reference human motions, synthesizing
new motions based on these reference motions, and mapping the generated motion
onto humanoid robots. To address these issues, we introduce SynSculptor, a
humanoid motion analysis and editing framework that leverages postural
synergies for training-free human-like motion scripting. To analyze human
motion, we collect 3+ hours of motion capture data across 20 individuals where
a real-time operational space controller mimics human motion on a simulated
humanoid robot. The major postural synergies are extracted using principal
component analysis (PCA) for velocity trajectories segmented by changes in
robot momentum, constructing a style-conditioned synergy library for free-space
motion generation. To evaluate generated motions using the synergy library, the
foot-sliding ratio and proposed metrics for motion smoothness involving total
momentum and kinetic energy deviations are computed for each generated motion,
and compared with reference motions. Finally, we leverage the synergies with a
motion-language transformer, where the humanoid, during execution of motion
tasks with its end-effectors, adapts its posture based on the chosen synergy.
Supplementary material, code, and videos are available at
https://rhea-mal.github.io/humanoidsynergies.io.

</details>


### [179] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 提出了一种名为自引导动作扩散的高效双向解码变体，通过基于先前决策引导扩散步骤的提议分布，在可忽略的推理成本下实现接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理时动作样本搜索的方法（特别是双向解码）虽然能提高扩散策略的一致性和反应性，但随着采样动作多样性增加，计算成本变得昂贵。

Method: 自引导动作扩散方法，在每个扩散步骤基于先前的决策来引导提议分布，从而提高效率。

Result: 在仿真任务中，该方法在可忽略的推理成本下实现接近最优性能，在严格采样预算下，在动态任务上的成功率比现有方法高出70%。

Conclusion: 自引导动作扩散是一种高效的扩散策略优化方法，能够显著提升性能同时保持低计算成本。

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [180] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: VLAPS是一个将模型搜索嵌入预训练视觉-语言-动作模型推理过程的新框架，通过蒙特卡洛树搜索和环境模型提升机器人任务性能


<details>
  <summary>Details</summary>
Motivation: 预训练的VLA模型在零样本部署时容易产生脆弱行为和安全故障，需要改进在分布外场景下的表现

Method: 使用改进的蒙特卡洛树搜索算法，结合环境模型和VLA策略定义的动作先验，在语言条件机器人任务中进行高效搜索

Result: 在所有实验中显著优于仅使用VLA的基线方法，在语言指定任务上的成功率最高提升67个百分点

Conclusion: VLAPS提供了一个原则性框架来控制测试时计算、利用环境先验知识，并将规划技术集成到VLA推理过程中

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [181] [Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids](https://arxiv.org/abs/2508.12252)
*Kaizhe Hu,Haochen Shi,Yao He,Weizhuo Wang,C. Karen Liu,Shuran Song*

Main category: cs.RO

TL;DR: 提出了Robot-Trains-Robot (RTR)框架，通过机械臂教师主动指导人形机器人学生，实现高效的长时真实世界人形机器人训练，最小化人工干预


<details>
  <summary>Details</summary>
Motivation: 解决基于仿真的强化学习在真实世界人形机器人应用中的局限性，包括安全性、奖励设计和学习效率等挑战，克服sim-to-real差距

Method: RTR框架：机械臂教师提供保护、学习调度、奖励、扰动、故障检测和自动重置；提出新的RL管道，通过在真实世界中优化单个动力学编码潜在变量来促进和稳定sim-to-real迁移

Result: 在两个具有挑战性的真实世界人形任务中验证：精确速度跟踪的行走策略微调，以及从零开始学习人形摆动任务

Conclusion: RTR系统展示了实现真实世界人形机器人学习的有前景能力，为克服sim-to-real差距提供了有效解决方案

Abstract: Simulation-based reinforcement learning (RL) has significantly advanced
humanoid locomotion tasks, yet direct real-world RL from scratch or adapting
from pretrained policies remains rare, limiting the full potential of humanoid
robots. Real-world learning, despite being crucial for overcoming the
sim-to-real gap, faces substantial challenges related to safety, reward design,
and learning efficiency. To address these limitations, we propose
Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher
actively supports and guides a humanoid robot student. The RTR system provides
protection, learning schedule, reward, perturbation, failure detection, and
automatic resets. It enables efficient long-term real-world humanoid training
with minimal human intervention. Furthermore, we propose a novel RL pipeline
that facilitates and stabilizes sim-to-real transfer by optimizing a single
dynamics-encoded latent variable in the real world. We validate our method
through two challenging real-world humanoid tasks: fine-tuning a walking policy
for precise speed tracking and learning a humanoid swing-up task from scratch,
illustrating the promising capabilities of real-world humanoid learning
realized by RTR-style systems. See https://robot-trains-robot.github.io/ for
more info.

</details>


### [182] [Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments](https://arxiv.org/abs/2508.12274)
*Jian Zhao,Yunlong Lian,Andy M Tyrrell,Michael Gienger,Jihong Zhu*

Main category: cs.RO

TL;DR: 提出了一种适用于紧身服装的双手机器人穿衣策略，通过建立球坐标系和使用GMM/GMR模仿学习来适应不同人体手臂姿势


<details>
  <summary>Details</summary>
Motivation: 当前机器人辅助穿衣研究主要关注宽松服装，对紧身服装关注不足。单手机器人穿紧身衣常因袖窿狭窄和刚性减弱而失败

Method: 建立穿衣球坐标系，以方位角作为任务相关特征，使用高斯混合模型(GMM)和高斯混合回归(GMR)进行双手机器人穿衣轨迹的模仿学习

Result: 通过多种实验验证了所提方法的有效性

Conclusion: 该双手机器人穿衣策略能够成功处理紧身服装的穿衣任务，适应不同人体手臂姿势

Abstract: Robot-assisted dressing is a popular but challenging topic in the field of
robotic manipulation, offering significant potential to improve the quality of
life for individuals with mobility limitations. Currently, the majority of
research on robot-assisted dressing focuses on how to put on loose-fitting
clothing, with little attention paid to tight garments. For the former, since
the armscye is larger, a single robotic arm can usually complete the dressing
task successfully. However, for the latter, dressing with a single robotic arm
often fails due to the narrower armscye and the property of diminishing
rigidity in the armscye, which eventually causes the armscye to get stuck. This
paper proposes a bimanual dressing strategy suitable for dressing tight-fitting
clothing. To facilitate the encoding of dressing trajectories that adapt to
different human arm postures, a spherical coordinate system for dressing is
established. We uses the azimuthal angle of the spherical coordinate system as
a task-relevant feature for bimanual manipulation. Based on this new
coordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture
Regression (GMR) for imitation learning of bimanual dressing trajectories,
generating dressing strategies that adapt to different human arm postures. The
effectiveness of the proposed method is validated through various experiments.

</details>


### [183] [A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts](https://arxiv.org/abs/2508.12296)
*Bin Wang,Jiwen Zhang,Song Wang,Dan Wu*

Main category: cs.RO

TL;DR: 提出了一种基于力-视觉融合控制和多任务强化学习的机器人批量精密装配方法，能够处理配合类型和配合量不确定的精密装配任务，通过策略蒸馏整合多个子任务策略，实现鲁棒控制。


<details>
  <summary>Details</summary>
Motivation: 在精密工业应用中，机器人需要对具有过渡配合设计的销孔进行批量装配，但加工误差会导致配合类型（间隙或过盈）和配合量不确定，需要开发鲁棒的顺应控制策略。

Method: 将批量精密装配任务分解为多个确定性子任务，提出力-视觉融合控制器驱动的强化学习方法(FVFC-MTRL)和多任务强化学习训练方法，然后使用多教师策略蒸馏方法将多个训练策略整合到统一的学生网络中。

Result: 真实实验表明，该方法成功构建了适用于不同配合类型和配合量的鲁棒控制策略，MTRL框架显著提高了训练效率，最终的控制策略在力顺应性和成功率方面优于现有方法。

Conclusion: 该方法有效解决了配合不确定的精密装配问题，通过多任务学习和策略蒸馏实现了高效训练和鲁棒控制，在工业精密装配应用中具有重要价值。

Abstract: In some high-precision industrial applications, robots are deployed to
perform precision assembly tasks on mass batches of manufactured pegs and
holes. If the peg and hole are designed with transition fit, machining errors
may lead to either a clearance or an interference fit for a specific pair of
components, with uncertain fit amounts. This paper focuses on the robotic batch
precision assembly task involving components with uncertain fit types and fit
amounts, and proposes an efficient methodology to construct the robust and
compliant assembly control strategy. Specifically, the batch precision assembly
task is decomposed into multiple deterministic subtasks, and a force-vision
fusion controller-driven reinforcement learning method and a multi-task
reinforcement learning training method (FVFC-MTRL) are proposed to jointly
learn multiple compliance control strategies for these subtasks. Subsequently,
the multi-teacher policy distillation approach is designed to integrate
multiple trained strategies into a unified student network, thereby
establishing a robust control strategy. Real-world experiments demonstrate that
the proposed method successfully constructs the robust control strategy for
high-precision assembly task with different fit types and fit amounts.
Moreover, the MTRL framework significantly improves training efficiency, and
the final developed control strategy achieves superior force compliance and
higher success rate compared with many existing methods.

</details>


### [184] [Implementation and evaluation of a prediction algorithm for an autonomous vehicle](https://arxiv.org/abs/2508.12312)
*Marco Leon Rapp*

Main category: cs.RO

TL;DR: 基于动态自行车模型的轨迹预测算法，每5毫秒预测一次，位置偏差仅1.25cm/m，比动力学模型精确82.6%


<details>
  <summary>Details</summary>
Motivation: 为自主驾驶汽车开发高精度轨迹预测算法，提高轨迹预测的准确性和实时性

Method: 对比动力学和动态自行车模型，通过光学位置跟踪新方法测量转向刚度，将动态模型集成到扩展卡尔滤波器中

Result: 动态模型在高速时准确性更优，整体位置偏差仅1.25厘米/米，比动力学模型精确提升82.6%

Conclusion: 动态自行车模型在高速情况下显著提升了轨迹预测精度，新的转向刚度测量方法有效支撑了模型的准确性

Abstract: This paper presents a prediction algorithm that estimates the vehicle
trajectory every five milliseconds for an autonomous vehicle. A kinematic and a
dynamic bicycle model are compared, with the dynamic model exhibiting superior
accuracy at higher speeds. Vehicle parameters such as mass, center of gravity,
moment of inertia, and cornering stiffness are determined experimentally. For
cornering stiffness, a novel measurement procedure using optical position
tracking is introduced. The model is incorporated into an extended Kalman
filter and implemented in a ROS node in C++. The algorithm achieves a
positional deviation of only 1.25 cm per meter over the entire test drive and
is up to 82.6% more precise than the kinematic model.

</details>


### [185] [Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control](https://arxiv.org/abs/2508.12335)
*Yunfan Gao,Florian Messerer,Niels van Duijkeren,Rashmi Dabir,Moritz Diehl*

Main category: cs.RO

TL;DR: 提出了一种基于半无限规划的最优控制和模型预测控制碰撞避免方法，通过局部约简和外部主动集方法处理无限约束问题，支持鲁棒碰撞避免和实时应用


<details>
  <summary>Details</summary>
Motivation: 解决在最优控制和模型预测控制中处理大量环境点与机器人多边形表示之间的碰撞避免问题，传统方法难以处理无限约束条件

Method: 采用半无限规划(SIP)框架，结合局部约简和外部主动集方法，迭代识别最近点障碍物，确定可行机器人形状参数的距离最小化器，求解上层有限约束子问题

Result: 实现了20Hz实时运行的控制器，在真实机器人上展示了快速无碰撞导航能力，并在仿真中验证了3D碰撞避免应用

Conclusion: 该方法能有效处理碰撞避免中的无限约束问题，支持鲁棒控制并实现实时性能，适用于紧密空间导航

Abstract: This paper presents a novel approach for collision avoidance in optimal and
model predictive control, in which the environment is represented by a large
number of points and the robot as a union of padded polygons. The conditions
that none of the points shall collide with the robot can be written in terms of
an infinite number of constraints per obstacle point. We show that the
resulting semi-infinite programming (SIP) optimal control problem (OCP) can be
efficiently tackled through a combination of two methods: local reduction and
an external active-set method. Specifically, this involves iteratively
identifying the closest point obstacles, determining the lower-level distance
minimizer among all feasible robot shape parameters, and solving the
upper-level finitely-constrained subproblems.
  In addition, this paper addresses robust collision avoidance in the presence
of ellipsoidal state uncertainties. Enforcing constraint satisfaction over all
possible uncertainty realizations extends the dimension of constraint
infiniteness. The infinitely many constraints arising from translational
uncertainty are handled by local reduction together with the robot shape
parameterization, while rotational uncertainty is addressed via a backoff
reformulation.
  A controller implemented based on the proposed method is demonstrated on a
real-world robot running at 20Hz, enabling fast and collision-free navigation
in tight spaces. An application to 3D collision avoidance is also demonstrated
in simulation.

</details>


### [186] [SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning](https://arxiv.org/abs/2508.12394)
*Zichen Yan,Rui Huang,Lei He,Shao Guo,Lin Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于视觉强化学习的无人机图像目标导航框架，通过辅助任务增强视觉表示能力，实现端到端的速度控制导航，无需外部定位，并集成深度安全模块进行实时避障。


<details>
  <summary>Details</summary>
Motivation: 现有图像目标导航研究主要针对地面机器人，无人机导航面临更高频率的反馈控制和全局定位需求，需要开发新的sim-to-real框架来解决这些挑战。

Method: 使用视觉强化学习训练端到端策略，通过图像扰动和未来状态预测等辅助任务增强视觉主干网络，集成深度安全模块实现实时避障，无需外部定位或全局建图。

Result: 实现了无人机在未知环境中的自主探索、避障和图像目标寻找的综合导航能力，支持直接速度控制，在杂乱环境中安全导航。

Conclusion: 该框架为无人机图像目标导航提供了有效的sim-to-real解决方案，通过视觉强化学习和辅助任务训练实现了无需外部定位的端到端导航能力，具有实际应用价值。

Abstract: Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an
unknown environment and reaching a location that visually matches a given
target image. While prior works primarily study ImageNav for ground robots,
enabling this capability for autonomous drones is substantially more
challenging due to their need for high-frequency feedback control and global
localization for stable flight. In this paper, we propose a novel sim-to-real
framework that leverages visual reinforcement learning (RL) to achieve ImageNav
for drones. To enhance visual representation ability, our approach trains the
vision backbone with auxiliary tasks, including image perturbations and future
transition prediction, which results in more effective policy training. The
proposed algorithm enables end-to-end ImageNav with direct velocity control,
eliminating the need for external localization. Furthermore, we integrate a
depth-based safety module for real-time obstacle avoidance, allowing the drone
to safely navigate in cluttered environments. Unlike most existing drone
navigation methods that focus solely on reference tracking or obstacle
avoidance, our framework supports comprehensive navigation
behaviors--autonomous exploration, obstacle avoidance, and image-goal
seeking--without requiring explicit global mapping. Code and model checkpoints
will be released upon acceptance.

</details>


### [187] [PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting](https://arxiv.org/abs/2508.12395)
*Zihan Wang*

Main category: cs.RO

TL;DR: 设计并验证了一种采用等离子体矢量推进的超静音飞艇机器人，通过离子风产生推力，具有模块化推进单元和双自由度矢量控制，实现了全包线飞行能力且噪音极低


<details>
  <summary>Details</summary>
Motivation: 开发一种无机械螺旋桨的超静音飞行器，适用于噪音敏感、封闭和近空间应用场景，解决传统推进系统噪音大和机械复杂的问题

Method: 使用氦气升力平台提供持久续航，四层环状不对称电容器产生离子风推力，模块化推进单元可灵活配置，双自由度头部实现推力矢量控制，采用闭环滑移控制方案保证稳定机动

Result: 飞行实验验证了全包线飞行能力（起飞、爬升、悬停、下降和平稳着陆），证明了等离子体矢量推进的可行性、自由度矢量控制的有效性以及控制系统的稳定性

Conclusion: 该等离子体推进超静音飞艇具有低声学特征、结构简单和高机动性等特点，非常适合噪音敏感、封闭环境和近空间应用

Abstract: This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.

</details>


### [188] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 本文探索仅使用机器人内置关节传感器的深度学习手势识别方法，无需外部传感器，通过CNN架构和频谱图表示实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 在HRC领域，传统方法依赖视觉或机器人皮肤等外部传感器，成本高且部署复杂。本研究旨在探索仅使用机器人内置关节传感器的解决方案，以降低成本和提高可扩展性。

Method: 评估多种CNN架构，收集两个数据集研究数据表示和模型架构对识别精度的影响。重点比较频谱图表示方法，包括STFT2DCNN和STT3DCNN模型。

Result: 频谱图表示显著提高识别精度，模型架构影响较小。在Franka Emika Research机器人上实现，STFT2DCNN和STT3DCNN方法在接触检测和手势分类中达到95%以上准确率，且在新姿态下泛化性能更好。

Conclusion: 证明了无需外部传感器的触觉识别可行性，为HRC提供了成本效益高、可扩展的解决方案，推动了该领域的进一步研究。

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [189] [Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation](https://arxiv.org/abs/2508.12439)
*Sunyu Wang,Arjun S. Lakshmipathy,Jean Oh,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 本文提出了一种基于测地线追踪的积分方案，用于在网格上直接进行滚动滑动接触的一阶时间积分，扩展了滚动滑动接触建模到流形网格，实现了对物体真实几何的高保真离散表示的灵巧操作推理。


<details>
  <summary>Details</summary>
Motivation: 现有的滚动滑动接触研究主要关注具有可微分参数化的连续形状，而本文旨在将滚动滑动接触建模扩展到流形网格，使灵巧操作能够基于物体真实几何的高保真离散表示进行推理。

Method: 提出了基于测地线追踪的积分方案，直接在网格上进行滚动滑动接触的一阶时间积分。使用最小二乘优化器规划多指机器人手的灵巧运动，通过最小化接触滑动和旋转来维持最稳定的瞬时抓取。

Result: 在模拟中对五个物体进行了灵巧操作规划，与基于碰撞检测和基于原始形状的基线方法进行比较。结果显示该方法在准确性和精度方面表现最佳，即使对于粗糙网格也是如此。

Conclusion: 该方法在网格表面接触建模方面表现出优越性能，未来工作需要整合多接触和接触力以实现更准确和鲁棒的网格表面接触建模。

Abstract: Reasoning about rolling and sliding contact, or roll-slide contact for short,
is critical for dexterous manipulation tasks that involve intricate geometries.
But existing works on roll-slide contact mostly focus on continuous shapes with
differentiable parametrizations. This work extends roll-slide contact modeling
to manifold meshes. Specifically, we present an integration scheme based on
geodesic tracing to first-order time-integrate roll-slide contact directly on
meshes, enabling dexterous manipulation to reason over high-fidelity discrete
representations of an object's true geometry. Using our method, we planned
dexterous motions of a multi-finger robotic hand manipulating five objects
in-hand in simulation. The planning was achieved with a least-squares optimizer
that strives to maintain the most stable instantaneous grasp by minimizing
contact sliding and spinning. Then, we evaluated our method against a baseline
using collision detection and a baseline using primitive shapes. The results
show that our method performed the best in accuracy and precision, even for
coarse meshes. We conclude with a future work discussion on incorporating
multiple contacts and contact forces to achieve accurate and robust mesh-based
surface contact modeling.

</details>


### [190] [Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics](https://arxiv.org/abs/2508.12456)
*Hadas C. Kuzmenko,David Ehevich,Oren Gal*

Main category: cs.RO

TL;DR: 本研究提出了一种结合多智能体群机器人系统和液体时间常数神经网络(LTCNs)的集成框架，用于实时预测、动态跟踪和快速响应海洋溢油事件，在Deepwater Horizon溢油数据上实现了96%的空间精度。


<details>
  <summary>Details</summary>
Motivation: 海洋溢油对环境和经济构成严重威胁，但由于风、洋流、温度等物理化学环境因素的复杂相互作用，预测和管理溢油轨迹极具挑战性，需要准确的实时轨迹预测和协调的缓解措施。

Method: 基于MOOS-IvP平台构建多智能体群机器人系统，结合液体时间常数神经网络(LTCNs)，将自适应机器学习与自主海洋机器人技术融合，利用LTCNs建模复杂的时间依赖过程，通过群体智能实现去中心化、可扩展的决策。

Result: 在Deepwater Horizon溢油数据验证中，LTC-RK4模型实现了96%的空间精度，比LSTM方法提高了23%，在预测精度、灵活性和操作可扩展性方面都有显著提升。

Conclusion: 该研究通过将先进神经建模与自主协调机器人技术相结合，推进了可持续、自主溢油管理和环境保护的技术水平，增强了轨迹预测和响应协调能力。

Abstract: Marine oil spills pose grave environmental and economic risks, threatening
marine ecosystems, coastlines, and dependent industries. Predicting and
managing oil spill trajectories is highly complex, due to the interplay of
physical, chemical, and environmental factors such as wind, currents, and
temperature, which makes timely and effective response challenging. Accurate
real-time trajectory forecasting and coordinated mitigation are vital for
minimizing the impact of these disasters. This study introduces an integrated
framework combining a multi-agent swarm robotics system built on the MOOS-IvP
platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system
fuses adaptive machine learning with autonomous marine robotics, enabling
real-time prediction, dynamic tracking, and rapid response to evolving oil
spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent
processes--the framework achieves real-time, high-accuracy forecasts of spill
movement. Swarm intelligence enables decentralized, scalable, and resilient
decision-making among robot agents, enhancing collective monitoring and
containment efforts. Our approach was validated using data from the Deepwater
Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,
surpassing LSTM approaches by 23%. The integration of advanced neural modeling
with autonomous, coordinated robotics demonstrates substantial improvements in
prediction precision, flexibility, and operational scalability. Ultimately,
this research advances the state-of-the-art for sustainable, autonomous oil
spill management and environmental protection by enhancing both trajectory
prediction and response coordination.

</details>


### [191] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 开发了一个基于三台步进电机、微控制器和YOLO检测模型的魔方自动求解系统，通过GUI界面显示状态，使用Kociemba算法求解，平均求解时间约2.2分钟


<details>
  <summary>Details</summary>
Motivation: 构建一个完整的魔方自动求解系统，实现从物理操作到视觉识别再到算法求解的全流程自动化

Method: 采用三台步进电机进行物理操作，微控制器控制硬件，YOLOv8模型实时检测魔方状态，Unity开发GUI界面，Kociemba算法生成求解步骤

Result: YOLOv8模型达到高精度（Precision 0.98443, Recall 0.98419），系统平均求解时间约2.2分钟

Conclusion: 成功开发了一个集成硬件控制、视觉识别和算法求解的完整魔方自动求解系统，实现了较好的性能和用户体验

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [192] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF从力控表面探测中估计软材料的静态和动态响应。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖纯几何或视觉数据，无法有效估计软材料的力学特性。PROD通过整合触觉交互来解决这一问题，为机器人操作、医学成像和触觉反馈系统提供更全面的物体重建能力。

Method: 将物体变形建模为弹性静力学过程，推导出控制泊松方程从稀疏的位姿和力测量中估计SDF。结合稳态弹性动力学假设，从变形观测中恢复未变形SDF，并通过分析位移响应估计材料刚度。

Result: PROD在处理位姿误差、非垂直力施加和曲率误差方面表现出鲁棒性，在模拟软体交互中验证了其有效性。

Conclusion: PROD为可变形物体重建提供了强大工具，特别适用于需要精确力学特性估计的应用场景，具有可证明的收敛性。

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [193] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 提出了一种基于运动的时空标定框架，用于事件相机多传感器系统，无需专用标定目标，通过角速度估计和两步优化实现高精度标定


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级延迟特性，但在多传感器融合中的外参标定研究不足，需要一种无需专用标定目标的灵活标定方法

Method: 使用事件相机和其他传感器的旋转运动估计，通过法向流观测估计角速度，采用两步方法：先通过CCA初始化时空参数，然后用连续时间SO(3)参数化进行非线性优化

Result: 在公开和自采集数据集上的广泛评估显示，该方法达到与基于目标方法相当的标定精度，比纯CCA方法更稳定，具有高精度、鲁棒性和灵活性

Conclusion: 该方法为事件相机多传感器系统提供了一种有效的无目标标定解决方案，代码已开源以促进未来研究

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [194] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: 基于域解耦物理信息神经网络(DD-PINN)的实时非线性模型预测控制框架，对软连续体机器人实现了高精度动态跟踪控制，计算速度提升44000倍


<details>
  <summary>Details</summary>
Motivation: 软连续体机器人的动态控制具有广阔应用前景，但精确动态模型的高计算要求以及现有数据驱动方法缺乏适应性和形状捐描能力限制了其应用

Method: 使用域解耦物理信息神经网络(DD-PINN)作为Cosserat柱动态模型的代理模型，结合无病症卡尔滤波估计模型状态和弯曲度，并在GPU上实现70Hz的非线性进化模型预测控制

Result: 模拟中实现了3mm以下的终端执行器位置误差(2.3%机器人长度)，实际实验中达到类似精度和最高3.55m/s2的加速度

Conclusion: 该框架为软连续体机器人提供了一种高效、准确且实时的动态控制方案，解决了传统模型计算复杂性高的问题

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


### [195] [MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA](https://arxiv.org/abs/2508.12729)
*Junhao Ye,Cheng Hu,Yiqin Wang,Weizhan Huang,Nicolas Baumann,Jie He,Meixun Qu,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: 提出了MCTR算法，通过曲率校正移动平均提高轨迹平滑度，并在CARLA模拟器中实现数字孪生系统来验证3D LiDAR感知下的算法鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有DTR算法使用外接圆生成轨迹导致路径不够平滑，且F1TENTH模拟器缺乏3D LiDAR感知支持，限制了在真实场景中的测试效果

Method: 基于Follow-The-Gap方法改进，采用曲率校正移动平均(CCMA)技术提升轨迹平滑度，在CARLA模拟器中构建数字孪生系统进行3D LiDAR感知验证

Result: 算法在仿真和真实车辆实验中得到了充分验证，证明了其在3D LiDAR感知环境下的鲁棒性和性能提升

Conclusion: MCTR算法有效解决了轨迹平滑度问题，并通过数字孪生系统实现了更真实的3D感知测试环境，为自动驾驶赛车提供了更好的反应式控制解决方案

Abstract: In autonomous racing, reactive controllers eliminate the computational burden
of the full See-Think-Act autonomy stack by directly mapping sensor inputs to
control actions. This bypasses the need for explicit localization and
trajectory planning. A widely adopted baseline in this category is the
Follow-The-Gap method, which performs trajectory planning using LiDAR data.
Building on FTG, the Delaunay Triangulation-based Racing algorithm introduces
further enhancements. However, DTR's use of circumcircles for trajectory
generation often results in insufficiently smooth paths, ultimately degrading
performance. Additionally, the commonly used F1TENTH-simulator for autonomous
racing competitions lacks support for 3D LiDAR perception, limiting its
effectiveness in realistic testing. To address these challenges, this work
proposes the MCTR algorithm. MCTR improves trajectory smoothness through the
use of Curvature Corrected Moving Average and implements a digital twin system
within the CARLA simulator to validate the algorithm's robustness under 3D
LiDAR perception. The proposed algorithm has been thoroughly validated through
both simulation and real-world vehicle experiments.

</details>


### [196] [RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph](https://arxiv.org/abs/2508.12916)
*Hecheng Wang,Jiankun Ren,Jia Yu,Lizhe Qi,Yunquan Sun*

Main category: cs.RO

TL;DR: RoboRetriever是一个仅使用单个腕戴式RGB-D相机和自然语言指令的机器人对象检索框架，通过动态层次场景图和主动感知实现复杂环境中的对象检索


<details>
  <summary>Details</summary>
Motivation: 人类能够轻松在杂乱环境中检索对象，而现有机器人系统依赖多摄像头设置，成本高且适应性差，需要开发更灵活的单摄像头解决方案

Method: 构建动态层次场景图编码对象语义、几何和关系；使用视觉提示方案确定6-DoF相机位姿；结合主动感知、交互感知和操作

Result: 在多样化真实世界对象检索任务中表现出强大的适应性和鲁棒性，包括有人干预的场景

Conclusion: RoboRetriever证明了仅用单个RGB-D相机就能在杂乱环境中有效检索对象，为机器人感知和操作提供了更灵活经济的解决方案

Abstract: Humans effortlessly retrieve objects in cluttered, partially observable
environments by combining visual reasoning, active viewpoint adjustment, and
physical interaction-with only a single pair of eyes. In contrast, most
existing robotic systems rely on carefully positioned fixed or multi-camera
setups with complete scene visibility, which limits adaptability and incurs
high hardware costs. We present \textbf{RoboRetriever}, a novel framework for
real-world object retrieval that operates using only a \textbf{single}
wrist-mounted RGB-D camera and free-form natural language instructions.
RoboRetriever grounds visual observations to build and update a \textbf{dynamic
hierarchical scene graph} that encodes object semantics, geometry, and
inter-object relations over time. The supervisor module reasons over this
memory and task instruction to infer the target object and coordinate an
integrated action module combining \textbf{active perception},
\textbf{interactive perception}, and \textbf{manipulation}. To enable
task-aware scene-grounded active perception, we introduce a novel visual
prompting scheme that leverages large reasoning vision-language models to
determine 6-DoF camera poses aligned with the semantic task goal and geometry
scene context. We evaluate RoboRetriever on diverse real-world object retrieval
tasks, including scenarios with human intervention, demonstrating strong
adaptability and robustness in cluttered scenes with only one RGB-D camera.

</details>


### [197] [Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users](https://arxiv.org/abs/2508.12925)
*Eetu Laukka,Evan G. Center,Timo Ojala,Steven M. LaValle,Matti Pouke*

Main category: cs.RO

TL;DR: 该研究探索了在移动远程临场机器人系统中使用光流技术来缓解500毫秒延迟对用户体验的影响，但实验结果显示该方法未能显著提升控制性能，反而可能增加VR眩晕感。


<details>
  <summary>Details</summary>
Motivation: 传统移动远程临场机器人系统使用360度相机时会产生显著延迟（可达数秒），这给实时控制带来困难。研究旨在通过光学流动技术创造自运动错觉来改善高延迟期间的用户体验。

Method: 利用光流技术在用户发送运动指令到实际看到360度相机流之间的延迟期间，为用户创造自运动错觉，以缓解延迟带来的控制困难。

Result: 在500毫秒延迟条件下，使用自运动错觉方法对控制性能（任务完成时间和碰撞次数）没有显著改善，且有证据表明该方法可能增加VR眩晕感（通过模拟器眩晕问卷测量）。

Conclusion: 需要进一步调整该方法才能使其可行，当前形式在500毫秒延迟条件下未能有效改善用户体验。

Abstract: Mobile telepresence robots allow users to feel present and explore remote
environments using technology. Traditionally, these systems are implemented
using a camera onboard a mobile robot that can be controlled. Although
high-immersion technologies, such as 360-degree cameras, can increase
situational awareness and presence, they also introduce significant challenges.
Additional processing and bandwidth requirements often result in latencies of
up to seconds. The current delay with a 360-degree camera streaming over the
internet makes real-time control of these systems difficult. Working with
high-latency systems requires some form of assistance to the users.
  This study presents a novel way to utilize optical flow to create an illusion
of self-motion to the user during the latency period between user sending
motion commands to the robot and seeing the actual motion through the
360-camera stream. We find no significant benefit of using the self-motion
illusion to performance or accuracy of controlling a telepresence robot with a
latency of 500 ms, as measured by the task completion time and collisions into
objects. Some evidence is shown that the method might increase virtual reality
(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We
conclude that further adjustments are necessary in order to render the method
viable.

</details>


### [198] [Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion](https://arxiv.org/abs/2508.12928)
*Victor Dhédin,Haizhou Zhao,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出基于MCTS和全身轨迹优化的完整框架，用于同时规划接触序列和接触点选择，解决腿式机器人在高度受限环境中的运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人需要在高度受限环境中进行敏捷机动，但规划此类运动需要解决混合连续和离散决策变量的复杂优化问题。

Method: 使用蒙特卡洛树搜索（MCTS）和全身轨迹优化（TO）相结合的方法，实现同时进行接触序列和接触点选择。

Result: 仿真实验表明该框架能快速找到多种动态一致的规划方案，并可成功转移到真实四足机器人上执行，还能规划复杂的人形机器人非周期性机动。

Conclusion: 这是首个使用四足机器人全身动力学进行非周期性多接触运动的同时接触序列和接触点选择框架的演示。

Abstract: Legged robots have the potential to traverse highly constrained environments
with agile maneuvers. However, planning such motions requires solving a highly
challenging optimization problem with a mixture of continuous and discrete
decision variables. In this paper, we present a full pipeline based on
Monte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to
perform simultaneous contact sequence and patch selection on highly challenging
environments. Through extensive simulation experiments, we show that our
framework can quickly find a diverse set of dynamically consistent plans. We
experimentally show that these plans are transferable to a real quadruped
robot. We further show that the same framework can find highly complex acyclic
humanoid maneuvers. To the best of our knowledge, this is the first
demonstration of simultaneous contact sequence and patch selection for acyclic
multi-contact locomotion using the whole-body dynamics of a quadruped.

</details>


### [199] [Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade](https://arxiv.org/abs/2508.12946)
*Ann-Sophie Schenk,Stefan Schiffer,Heqiu Song*

Main category: cs.RO

TL;DR: 本研究通过访谈探讨了六年级计算机科学课堂中使用社交机器人的教师和学生需求，发现双方都持开放态度但需求存在差异，带来了复杂的设计挑战。


<details>
  <summary>Details</summary>
Motivation: 了解教师和学生对在计算机科学课堂中使用社交机器人的需求和潜在应用，特别关注两个群体对机器人使用方式和功能要求的看法。

Method: 通过对教师和学生进行访谈，收集关于社交机器人在六年级计算机科学课堂中应用的看法和需求。

Result: 教师和学生都对在课堂中使用机器人持开放态度，但两个群体的需求存在显著差异，部分需求相当异质化。

Conclusion: 研究揭示了社交机器人在教育环境中应用的复杂性，不同用户群体的需求差异导致了复杂的设计挑战，需要在机器人设计中平衡这些不同的需求。

Abstract: In this paper we report on first insights from interviews with teachers and
students on using social robots in computer science class in sixth grade. Our
focus is on learning about requirements and potential applications. We are
particularly interested in getting both perspectives, the teachers' and the
learners' view on how robots could be used and what features they should or
should not have. Results show that teachers as well as students are very open
to robots in the classroom. However, requirements are partially quite
heterogeneous among the groups. This leads to complex design challenges which
we discuss at the end of this paper.

</details>


### [200] [Scaling Whole-body Multi-contact Manipulation with Contact Optimization](https://arxiv.org/abs/2508.12980)
*Victor Levé,João Moura,Sachiya Fujita,Tamon Miyake,Steve Tonneau,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 提出了一种用于人形机器人全身操纵的连续接触表面表示方法和优化框架，显著提高了规划效率


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人全身操纵任务中接触表面无限可能导致的规划可扩展性问题，现有离散采样方法难以处理连续接触表面

Method: 提出机器人及物体表面的闭合形式邻近点计算表示方法，设计有效的成本函数来指导全身操纵规划

Result: 解决了现有方法无法处理的问题，规划时间比现有最优方法提高77%，并在真实人形机器人上成功验证了箱子操纵任务

Conclusion: 该框架为人形机器人连续接触表面的全身操纵规划提供了有效的梯度优化解决方案，显著提升了规划效率和实用性

Abstract: Daily tasks require us to use our whole body to manipulate objects, for
instance when our hands are unavailable. We consider the issue of providing
humanoid robots with the ability to autonomously perform similar whole-body
manipulation tasks. In this context, the infinite possibilities for where and
how contact can occur on the robot and object surfaces hinder the scalability
of existing planning methods, which predominantly rely on discrete sampling.
Given the continuous nature of contact surfaces, gradient-based optimization
offers a more suitable approach for finding solutions. However, a key remaining
challenge is the lack of an efficient representation of robot surfaces. In this
work, we propose (i) a representation of robot and object surfaces that enables
closed-form computation of proximity points, and (ii) a cost design that
effectively guides whole-body manipulation planning. Our experiments
demonstrate that the proposed framework can solve problems unaddressed by
existing methods, and achieves a 77% improvement in planning time over the
state of the art. We also validate the suitability of our approach on real
hardware through the whole-body manipulation of boxes by a humanoid robot.

</details>


### [201] [BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments](https://arxiv.org/abs/2508.13052)
*Sourav Raxit,Abdullah Al Redwan Newaz,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: BOW Planner是一个基于约束贝叶斯优化的可扩展运动规划算法，通过集中处理可达速度规划窗口和高效采样控制输入，有效处理运动学约束和高维目标函数，在复杂环境中实现快速安全的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在处理速度、加速度等运动学约束时存在困难，特别是在复杂环境中难以高效生成安全轨迹。需要一种能够处理高维目标函数和严格安全约束的规划算法。

Method: 采用约束贝叶斯优化(CBO)方法，专注于可达速度规划窗口，通过最小化采样来高效生成控制输入，管理高维目标函数和严格安全约束。

Result: 理论分析证明算法渐近收敛到接近最优解，在杂乱和受限环境中的广泛评估显示计算时间、轨迹长度和求解时间相比现有技术有显著改进。在实际机器人系统中成功部署，表现出卓越的采样效率、安全感知优化和快速规划能力。

Conclusion: BOW Planner通过约束贝叶斯优化方法有效解决了复杂环境中的运动规划问题，具有优异的性能和实用性，已作为开源包发布，对机器人应用发展具有重要价值。

Abstract: This paper introduces the BOW Planner, a scalable motion planning algorithm
designed to navigate robots through complex environments using constrained
Bayesian optimization (CBO). Unlike traditional methods, which often struggle
with kinodynamic constraints such as velocity and acceleration limits, the BOW
Planner excels by concentrating on a planning window of reachable velocities
and employing CBO to sample control inputs efficiently. This approach enables
the planner to manage high-dimensional objective functions and stringent safety
constraints with minimal sampling, ensuring rapid and secure trajectory
generation. Theoretical analysis confirms the algorithm's asymptotic
convergence to near-optimal solutions, while extensive evaluations in cluttered
and constrained settings reveal substantial improvements in computation times,
trajectory lengths, and solution times compared to existing techniques.
Successfully deployed across various real-world robotic systems, the BOW
Planner demonstrates its practical significance through exceptional sample
efficiency, safety-aware optimization, and rapid planning capabilities, making
it a valuable tool for advancing robotic applications. The BOW Planner is
released as an open-source package and videos of real-world and simulated
experiments are available at https://bow-web.github.io.

</details>


### [202] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 这篇论文是关于基于大型视觉语言模型(VLM)的视觉-语言-动作(VLA)模型在机器人操作领域的系统性综述，提出了分类体系并分析了两种主要架构范式：单体模型和分层模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法在非结构化、新颖环境中无法扩展和泛化，而基于大型VLM的VLA模型为机器人操作提供了变革性的解决方案，需要系统性的综述来整合现有研究。

Method: 采用分类导向的综述方法，定义了基于大型VLM的VLA模型，分析了两种主要架构范式（单体模型和分层模型），并深入研究了与先进领域的集成、特性综合以及未来方向。

Result: 建立了系统化的分类体系，整合了架构特征、操作优势以及支持发展的数据集和基准测试，识别了包括记忆机制、4D感知、高效适应等有前景的研究方向。

Conclusion: 该综述通过系统整合大型VLM与机器人操作交叉领域的研究，解决了现有分类不一致性问题，减轻了研究碎片化，填补了关键空白，并提供了持续更新的项目页面。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.

</details>


### [203] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA框架通过将动作预测直接建立在相机观测空间中，解决了VLA模型在不同视角下的空间不一致性问题，显著提升了模型的泛化能力和任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在真实环境中泛化能力差，主要原因是观测空间和动作空间之间存在差异。虽然训练数据来自不同相机视角，但模型通常在机器人基座坐标系中预测末端执行器位姿，导致空间不一致。

Method: 提出OC-VLA框架，利用相机外参标定矩阵将末端执行器位姿从机器人基座坐标系转换到相机坐标系，统一不同视角下的预测目标。这是一个轻量级即插即用策略。

Result: 在仿真和真实机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。

Conclusion: OC-VLA框架通过统一观测和动作空间，有效解决了VLA模型的视角变化问题，具有很好的兼容性和实用性。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


### [204] [Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors](https://arxiv.org/abs/2508.13151)
*Yuying Zhang,Joni Pajarinen*

Main category: cs.RO

TL;DR: 提出基于强化学习的方法解决移动机器人在动态环境中需要先操纵障碍物再导航的问题，结合可操纵性先验和功能映射来选择高质量操纵动作，在仿真和真实Spot机器人上验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统方法将导航和操纵作为独立任务处理，在需要先清除障碍物才能导航的"操纵以导航"场景中经常失败，需要主动与环境交互来清理路径

Method: 结合可操纵性先验（关注高可操纵性体位）和功能映射（选择高质量操纵动作）的强化学习方法，减少不必要的探索，有效学习操纵策略

Result: 在两个新的操纵导航仿真任务（Reach和Door）中，方法能让机器人有效与环境交互并穿越动态环境，学习到的策略成功迁移到真实Spot机器人完成Reach任务

Conclusion: 该方法能有效解决操纵以导航问题，使机器人学会通过操纵动作清理路径来实现后续导航，在动态环境中表现出良好的性能

Abstract: Mobile manipulation in dynamic environments is challenging due to movable
obstacles blocking the robot's path. Traditional methods, which treat
navigation and manipulation as separate tasks, often fail in such
'manipulate-to-navigate' scenarios, as obstacles must be removed before
navigation. In these cases, active interaction with the environment is required
to clear obstacles while ensuring sufficient space for movement. To address the
manipulate-to-navigate problem, we propose a reinforcement learning-based
approach for learning manipulation actions that facilitate subsequent
navigation. Our method combines manipulability priors to focus the robot on
high manipulability body positions with affordance maps for selecting
high-quality manipulation actions. By focusing on feasible and meaningful
actions, our approach reduces unnecessary exploration and allows the robot to
learn manipulation strategies more effectively. We present two new
manipulate-to-navigate simulation tasks called Reach and Door with the Boston
Dynamics Spot robot. The first task tests whether the robot can select a good
hand position in the target area such that the robot base can move effectively
forward while keeping the end effector position fixed. The second task requires
the robot to move a door aside in order to clear the navigation path. Both of
these tasks need first manipulation and then navigating the base forward.
Results show that our method allows a robot to effectively interact with and
traverse dynamic environments. Finally, we transfer the learned policy to a
real Boston Dynamics Spot robot, which successfully performs the Reach task.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [205] [RefAdGen: High-Fidelity Advertising Image Generation](https://arxiv.org/abs/2508.11695)
*Yiyun Chen,Weikai Yang*

Main category: cs.GR

TL;DR: 提出了RefAdGen框架，通过解耦设计和注意力融合模块解决AIGC广告图像生成中的保真度-效率困境，在大型数据集AdProd-100K上实现高保真度和卓越视觉效果


<details>
  <summary>Details</summary>
Motivation: 现有AIGC技术要么需要大量微调才能实现高保真度，要么难以在不同产品间保持保真度，无法满足电商和营销行业的实际需求

Method: 构建AdProd-100K大规模广告图像生成数据集，采用双重数据增强策略；提出RefAdGen生成框架，通过U-Net输入注入产品掩码实现精确空间控制，使用注意力融合模块集成产品特征

Result: RefAdGen实现了最先进的性能，对未见过的产品和具有挑战性的真实世界图像都能保持高保真度和卓越的视觉效果

Conclusion: 该框架为传统工作流程提供了可扩展且经济高效的替代方案，解决了AIGC在广告图像生成中的保真度-效率困境

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
techniques has unlocked opportunities in generating diverse and compelling
advertising images based on referenced product images and textual scene
descriptions. This capability substantially reduces human labor and production
costs in traditional marketing workflows. However, existing AIGC techniques
either demand extensive fine-tuning for each referenced image to achieve high
fidelity, or they struggle to maintain fidelity across diverse products, making
them impractical for e-commerce and marketing industries. To tackle this
limitation, we first construct AdProd-100K, a large-scale advertising image
generation dataset. A key innovation in its construction is our dual data
augmentation strategy, which fosters robust, 3D-aware representations crucial
for realistic and high-fidelity image synthesis. Leveraging this dataset, we
propose RefAdGen, a generation framework that achieves high fidelity through a
decoupled design. The framework enforces precise spatial control by injecting a
product mask at the U-Net input, and employs an efficient Attention Fusion
Module (AFM) to integrate product features. This design effectively resolves
the fidelity-efficiency dilemma present in existing methods. Extensive
experiments demonstrate that RefAdGen achieves state-of-the-art performance,
showcasing robust generalization by maintaining high fidelity and remarkable
visual results for both unseen products and challenging real-world, in-the-wild
images. This offers a scalable and cost-effective alternative to traditional
workflows. Code and datasets are publicly available at
https://github.com/Anonymous-Name-139/RefAdgen.

</details>


### [206] [Substepping the Material Point Method](https://arxiv.org/abs/2508.11722)
*Chenfanfu Jiang*

Main category: cs.GR

TL;DR: 提出了一种将显式MPM积分器包装成伪隐式积分器的简单算法，通过子步长实现大时间步长推进


<details>
  <summary>Details</summary>
Motivation: 许多MPM实现采用显式时间积分，但有时需要大时间步长（如与其他大步长求解器耦合、施加约束或投影、多物理场求解等）

Method: 通过子步长(substeps)方式包装显式MPM积分器，使其能够以大时间步长推进，形成伪隐式积分器

Result: 开发了一个即插即用的算法，能够有效实现大时间步长的MPM计算

Conclusion: 该方法为MPM提供了一种简单有效的大时间步长积分方案，适用于多种特殊应用场景

Abstract: Many Material Point Method implementations favor explicit time integration.
However large time steps are often desirable for special reasons - for example,
for partitioned coupling with another large-step solver, or for imposing
constraints, projections, or multiphysics solves. We present a simple,
plug-and-play algorithm that advances MPM with a large time step using
substeps, effectively wrapping an explicit MPM integrator into a
pseudo-implicit one.

</details>


### [207] [Mesh Processing Non-Meshes via Neural Displacement Fields](https://arxiv.org/abs/2508.12179)
*Yuta Noma,Zhecheng Wang,Chenxi Liu,Karan Singh,Alec Jacobson*

Main category: cs.GR

TL;DR: 一种紧凑的神经场表示方法，能够在不同表面表示之间进行常见的几何处理任务，解决传统网格处理流水线在新表示方式上的问题。


<details>
  <summary>Details</summary>
Motivation: 传统网格处理流水线已成熟，但适配新的非网格表面表示需要耗费的网格化或传输巨大的网格文件，这会失去这些新表示在流媒体应用中的核心优势。

Method: 通过学习一个神经地图，将粗糕的网格近似映射到表面。这种表示仅需几百KB，适合轻量传输。方法支持快速提取流形和Delaunay网格进行本质形状分析，并压缩标量场以高效传递预计算结果。

Result: 实验和应用表明，该方法具有快速、紧凑和高精度的特点，为交互式几何处理开启了新可能性。

Conclusion: 该神经场表示方法有效解决了新型表面表示在几何处理中的问题，提供了一种紧凑、高效的解决方案，适合流媒体和交互式应用。

Abstract: Mesh processing pipelines are mature, but adapting them to newer non-mesh
surface representations -- which enable fast rendering with compact file size
-- requires costly meshing or transmitting bulky meshes, negating their core
benefits for streaming applications.
  We present a compact neural field that enables common geometry processing
tasks across diverse surface representations. Given an input surface, our
method learns a neural map from its coarse mesh approximation to the surface.
The full representation totals only a few hundred kilobytes, making it ideal
for lightweight transmission. Our method enables fast extraction of manifold
and Delaunay meshes for intrinsic shape analysis, and compresses scalar fields
for efficient delivery of costly precomputed results. Experiments and
applications show that our fast, compact, and accurate approach opens up new
possibilities for interactive geometry processing.

</details>


### [208] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: 提出了Express4D数据集，使用消费级设备和LLM生成的自然语言指令，提供细腻的面部表情动作和语义标注，训练了两个文本到表情生成的基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型面临数据集要么是语音驱动，要么仅限于粗略情感标签的问题，缺乏细腻的表达描述和细粒度控制，且数据采集设备昂贵复杂。

Method: 使用商品级设备采集面部动作序列，采用LLM生成自然语言指令进行语义标注，数据格式为ARKit blendshape，训练了两个文本到表情生成的基线模型。

Result: 训练模型能够学习有意义的文本到表情动作生成，捕捉两种模态之间的多对多映射关系。

Conclusion: Express4D数据集提供了可绑定的动作数据，包含丰富的表达表演和标签，为未来基准测试提供了评估基础。

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [209] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: MixCache是一个训练无关的缓存框架，通过多粒度缓存策略和自适应触发机制，显著提升视频DiT模型的推理速度，在保持生成质量的同时实现近2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有的视频DiT模型存在计算成本高、推理延迟大的问题，而现有的缓存方法仅限于单粒度策略，无法灵活平衡生成质量和推理速度。

Method: 提出MixCache框架，包含上下文感知缓存触发策略和自适应混合缓存决策策略，能够动态选择最优缓存粒度，无需额外训练。

Result: 在多个模型上实验表明，MixCache能实现1.94-1.97倍的加速，同时在生成质量和推理效率方面优于基线方法。

Conclusion: MixCache通过创新的多粒度缓存策略有效解决了视频DiT模型推理效率问题，为高质量视频生成提供了高效的解决方案。

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>
