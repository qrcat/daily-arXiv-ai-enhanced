<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CG](#cs.CG) [Total: 6]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 一个实时生成式绘画系统，同时解释结构意图和语义意图，通过多阶段生成流程实现低延迟的协作美术创作。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本提示生成系统主要关注高级语义描述，缺乏对基础级几何特征的分析和结合。

Method: 同时分析直观几何特征（线条轨迹、比例、空间排列）和高级语义索引，通过多阶段生成流水线结合边缘保持结构控制与风格内容识别图像合成。

Result: 实现了低延迟的两阶段转换，支持多用户在共享画布上的协作创作，无需艺术专业知识。

Conclusion: 该系统重新定义人工智能互动为共同创作和相互增强的过程，促进同步共作视觉创作。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 通过3D高斯散射流水线集成双鱼眼相机模型，从不完美的360度图像生成无缝渲染效果


<details>
  <summary>Details</summary>
Motivation: 消费级双鱼眼系统产生的360度内容存在镜头间隔和角度异变等缺陷，影响全景图质量

Method: 将双鱼眼相机模型集成到3D高斯散射流水线，聚合优化3D高斯参数和标定变量，模拟镜头间隔与异变

Result: 在真实数据集上评估显示，方法能从不完美图像生成无缝渲染，性能超过现有全景渲染模型

Conclusion: 该框架能够有效处理双鱼眼相机的本质异常，为不完美全景输入提供了优质的新视角合成解决方案

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [3] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 通过时间切片融合技术，在不需训练的情况下提升VLA模型的推理质量，解决了当前模型忽视视觉序列时间相关性的问题


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在每个时间步独立处理视觉输入，弃用了机器人操作任务中的时间信息，导致模型容易受到视觉噪声影响且忽视连续帧之间的一致性

Method: 提出时间切片融合(TTF)方法，通过双维度检测（灰度像素差异分析+关注机制语义相关性评估）进行选择性融合，采用硬融合策略和关键帧锚定防止错误累积

Result: 在LIBERO平均提升4.0个百分点（72.4% vs 68.4%），SimplerEnv环境中相对提升4.8%，真实机器任务中相对提升8.7%，方法模型无关性

Conclusion: TTF方法有效利用时间信息提升VLA模型性能，同时发现选择性重用Query矩阵可以同时提升性能和计算效率，为直接KQV矩阵重用策略指明了新方向

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [4] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 无监督幻灯片质量评估系统，结合专家视觉指标和CLIP-ViT嵌入，达到了与人类评分0.83的相关系数，性能超过现有视觉-语言模型


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展、对象的幻灯片质量评估，提供实时反馈，避免主观人工评分的差异

Method: 结合7个专家视觉设计指标(白艺、色彩、边缘密度等)和CLIP-ViT嵌入，使用Isolation Forest异常分数进行评估，基于12k个专业讲座幻灯片训练

Result: 在6个学术讲座(115张幻灯片)上达到与人类视觉质量评分Pearson相关系数0.83，性能是现有视觉-语言模型的1.79-3.23倍

Conclusion: 通过结合低级设计线索和多模态嵌入，可以精确近似观众对幻灯片质量的感知，实现可扩展的实时对象评估

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [5] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 本文提出了一种针对2D范围视图LiDAR分割的高效对抗防御方法，通过可解释的净化网络实现强对抗鲁棒性，计算开销小，在实际自动驾驶场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR分割网络容易受到对抗攻击威胁安全，而大多数防御方法针对原始3D点云设计，计算量大。当前先进的LiDAR分割管道多采用更高效的2D范围视图表示，但该领域的轻量级对抗防御研究不足。

Method: 提出了针对2D范围视图LiDAR分割的高效模型净化框架，包括在范围视图域的直接攻击公式化，以及基于数学优化问题的可解释净化网络设计。

Result: 在开放基准测试中取得竞争性性能，一致优于生成式方法和对抗训练基线方法。在实际演示车辆上的部署验证了框架在真实自动驾驶场景中的准确运行能力。

Conclusion: 该方法为2D范围视图LiDAR分割提供了一种计算高效的对抗防御解决方案，具有实际部署价值，能够有效提升自动驾驶系统的安全性和可靠性。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [6] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 这篇综述论文系统分析了大型视觉语言模型(LVLMs)在目标检测领域的应用，探讨了其架构创新、训练范式、性能表现，并指出了当前局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言和视觉融合技术的发展，大型视觉语言模型在目标检测领域展现出超越传统架构的适应性、上下文推理和泛化能力，需要系统梳理该领域的最新进展。

Method: 采用三步研究综述方法：首先分析VLMs在目标检测中的工作原理，然后阐述LVLMs的架构创新和训练范式，最后通过可视化对比评估其性能表现。

Result: LVLMs在多样化场景(包括定位和分割)中表现出色，预计很快将达到或超越传统深度学习方法在目标检测方面的性能，但仍存在一些主要局限性。

Conclusion: LVLMs的最新进展已经并将继续对目标检测和机器人应用产生变革性影响，需要解决当前挑战并沿着清晰的路线图推进未来发展。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [7] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 这篇论文提出了一个两级精调的大视觉语言模型管线，专门用于生产级别的体育演出图片描述，在Super Bowl实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能解释通用体育活动，但缺乏专业的体育术语，无法生成自然的人类化描述。需要解决生产级体育图片标题生成的问题。

Method: 设计了一个两级精调的大视觉语言模型管线，通过分层精调来提升体育领域的专业性和语言生成能力。

Result: 在F1指标上提升8-10%，BERT分数提升2-10%，具有小的运行时内存占用和快速执行时间。在Super Bowl LIX中实时处理超1000张图片，每3-5秒处理6张图片。

Conclusion: 该方法有效解决了现有模型在体育领域的限制，能够生成高精度、风格化的体育图片描述，具有实际应用价值。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [8] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: DemoBias研究评估了大型视觉语言模型在生物特征人脸识别任务中的 demographic biases，发现LLaVA和PaliGemma在Hispanic/Latino、Caucasian和South Asian群体中存在较高偏差，而BLIP-2表现相对一致。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在各种下游任务中表现出色，但在生物特征人脸识别中存在 demographic biases 问题，需要评估这些模型在不同人口群体中的公平性表现。

Method: 在自建的人口平衡数据集上微调和评估了三种预训练LVLM模型（LLaVA、BLIP-2、PaliGemma），使用BERTScores和Fairness Discrepancy Rate等指标量化性能差异。

Result: 实验发现LVLMs存在人口统计偏差，PaliGemma和LLaVA在Hispanic/Latino、Caucasian和South Asian群体中表现出较高差异，而BLIP-2表现相对一致。

Conclusion: 该研究揭示了LVLMs在生物特征人脸识别任务中的人口统计偏差问题，强调了模型公平性评估的重要性，并为改进模型公平性提供了实证基础。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [9] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec是一种新颖的空间表示学习方法，直接在原始空间中操作，通过自适应采样和符号距离场编码来统一表示各种地理实体，避免了现有方法的分解计算成本高和几何对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有空间表示学习方法要么只针对单一地理实体类型，要么需要将实体分解为简单组件进行傅里叶变换，计算成本高且缺乏几何对齐，导致细粒度特征模糊。

Method: 基于符号距离场(SDF)思想，直接在原始空间中自适应采样点并编码其符号距离(外部为正，内部为负)，使用神经网络近似SDF来生成紧凑、几何感知的统一表示，并提出旋转不变位置编码来建模高频空间变化。

Result: 实验结果表明，Geo2Vec在表示形状和位置、捕捉拓扑和距离关系方面始终优于现有方法，并在实际GeoAI应用中实现了更高的效率。

Conclusion: Geo2Vec提供了一种计算高效、几何感知的统一空间表示学习方法，能够更好地捕捉地理实体的细粒度特征，为下游GeoAI模型构建了结构化和鲁棒的嵌入空间。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [10] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 本研究提出基于卷积神经网络的自动化方法，成功分类5种水稻品种并诊断4种水稻叶部病害，结合可解释AI技术提高模型透明度和可靠性


<details>
  <summary>Details</summary>
Motivation: 水稻作为全球重要主食，传统人工检测方法劳动密集、耗时且易出错，需要自动化解决方案来保障粮食质量和提高产量

Method: 使用包含75000张图像的公开数据集，采用CNN、VGG16、ResNet50和MobileNetV2等深度学习模型，结合SHAP和LIME等可解释AI技术

Result: 模型实现了高分类准确率，误分类极少，能够有效区分水稻品种并准确诊断叶部病害

Conclusion: 深度学习在农业应用中具有巨大潜力，可开发出稳健、可解释的系统，支持自动化作物质量检测和病害诊断，惠及农民、消费者和农业经济

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [11] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于联邦学习和OpenMax算法的面部识别系统，专门处理开收集场景下的已知和未知主体识别问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI面部识别技术虽然在特定场景下得到高精度，但在隐私保护和识别管理方面面临挑战，尤其是当操作环境中出现未知个体时。

Method: 将OpenMax算法集成到联邦学习框架中，利用平均激活向量和本地距离测量的交换来区分已知和未知主体。

Result: 实验结果验证了该方案的有效性，能够在分布式环境中实现更加注重隐私和稳健的面部识别。

Conclusion: 该研究为开收集面部识别提供了一种联邦学习方案，通过结合OpenMax算法有效解决了隐私保护和未知个体识别的挑战。

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [12] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 基于地面照片的深度学习生境分类方法，可在大规模生态监测中应用


<details>
  <summary>Details</summary>
Motivation: 现有生境分类方法主要基于卫星影像需野外验证，本研究提出仅使用地面照片的分类方法，提高验证效率和大规模分类能力

Method: 使用DeepLabV3-ResNet101深度学习模型，对地面照片进行预处理（调整大小、标准化、增强），重采样平衡训练数据集，采用五折交叉验证

Result: 模型在18个生境类别中平均F1分数为0.61，视觉区别明显的生境（如免土、沙地）达到0.90以上，混合或模糊类别分数较低

Conclusion: 证明了基于地面照片的生境分类方法的潜力，为生态监测提供了可扩展的解决方案，并提供了实用的网页应用工具

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [13] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出了一种基于自回归框架的交互式数字人视频生成系统，支持多模态输入控制和低延迟流式生成


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字人视频生成方法存在高延迟、高计算成本和有限可控性问题，需要构建实时交互的实用系统

Method: 基于大型语言模型(LLM)的自回归视频生成框架，接受音频、姿态和文本等多模态条件编码，通过扩散头去噪过程生成空间和语义一致的表示；构建20,000小时大规模对话数据集；引入压缩比达64倍的深度压缩自编码器

Result: 在双工对话、多语言人像合成和交互式世界模型等实验中表现出低延迟、高效率和细粒度多模态可控性优势

Conclusion: 该框架为实时交互式数字人生成提供了有效的解决方案，在保持高质量生成的同时显著降低了计算负担和延迟

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [14] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本文调查数字水印和隐写术作为ICAO合规面部图像的防篡改解决方案，分析现有技术在身份验证系统中的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像在身份验证中广泛应用，但标准化也带来了篡改和深度伪造的风险，传统实时检测方法无法提供捕获后保护。

Method: 对最先进的数字水印和隐写术技术进行全面分析，评估这些方法在ICAO标准约束下的适用性和效果。

Result: 研究发现数字水印和隐写术能够在不影响ICAO合规性的前提下，为图像提供持久的防篡改验证能力。

Conclusion: 数字水印和隐写术是ICAO合规面部图像身份验证系统的有效补充方案，但需要在安全性和实用性之间找到平衡点。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [15] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一个自监督框架，通过整合心脏磁共振成像特征和电子健康记录，使用医学提示引导进行生存分析，在四个独立临床队列中超越了传统和深度学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件(MACE)是心血管预后的核心挑战，需要整合多模态医疗数据来提升预测精度。

Method: PRISM通过运动感知多视图蒸馏提取时间同步的成像特征，并使用医学知识文本提示进行调制，结合结构化电子健康记录进行生存分析。

Result: 在四个独立临床队列中，PRISM在内部和外部验证中均超越了经典生存预测模型和最先进的深度学习基线，发现了三个与MACE风险相关的成像特征。

Conclusion: PRISM提供的成像和EHR组合表征为不同人群的心脏风险评估提供了有价值的见解，提示引导归因识别出高血压、糖尿病和吸烟是主要的临床风险因素。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [16] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: 基于CNN和ViT的EffNetViTLoRA模型，通过整合局部和全局特征及LoRA调整技术，在全量ADNI MRI数据集上实现阶段性阻塞症准确诊断，达到92.52%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 阻塞症早期诊断至关重要，轻度认知障碍(MCI)诊断面临边界区分困难。以往研究依赖有限数据子集，导致模型偏差和不可靠。需要一种基于全量数据的综合方法来提高临床诊断的准确性和可靠性。

Method: 提出EffNetViTLoRA模型，结合卷积神经网络(CNN)和视觉Transformer(ViT)，从MRI图像中提取局部和全局特征。使用全量ADNI T1加权MRI数据进行训练。采用低秩适配(LoRA)技术对预训练ViT模型进行基于目标领域的高效微调，减少过拟合风险。

Result: 模型在阻塞症(AD)、轻度认知障碍(MCI)和正常认知(CN)三个诊断类别上达到了92.52%的分类准确率和92.76%的F1分数，显示出在全量ADNI数据集上的稳健性能。

Conclusion: EffNetViTLoRA通过整合CNN和ViT的优势，结合全量数据训练和LoRA微调技术，为阻塞症的阶段性诊断提供了一种准确、可靠且高效的解决方案，在临床应用中具有重要价值。

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [17] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 这篇论文通过对比三种商业计算机视觉和AI跟踪软件的性能，评估了它们在使用播出镜头跟踪足球运动员位置、速度和距离的准确性，并分析了摄像头视频和分辨率对精度的影响。


<details>
  <summary>Details</summary>
Motivation: 评估商业计算机视觉和AI运动员跟踪软件的准确性，确定播出镜头跟踪技术在足球比赛中的应用潜力，以及摄像头类型和分辨率对跟踪精度的影响。

Method: 采用2022卡塔尔FIFA世界杯的比赛数据，使用战术、程序和摄像头1三种视频源。三家商业跟踪提供商使用计算机视觉和AI技术分析运动员位置和速度，并与高精度多摄像头跟踪系统(TRACAB Gen 5)进行对比。计算根均方误差(RMSE)和平均偏差。

Result: 位置根均方误差范围1.68-16.39米，速度根均方误差范围0.34-2.38米/秒。总跑动距离平均偏差范围-1745米(-21.8%)到+1945米(+24.3%)。计算机视觉和AI跟踪软件在检测到运动员时具有较好的精度。

Conclusion: 建议使用战术视频来最大化运动员检测率和提高准确性，720p和1080p分辨率在适当的计算机视觉和AI模型下均可以满足要求。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [18] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 提出JVLGS框架，结合视觉和文本模态的优势来增强气体泄漏分割，通过后处理减少误报，在监督学习和少样本学习设置下均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 气体泄漏对人类健康和环境造成严重威胁，但现有检测方法效果有限，特别是红外视频中气体云模糊和非刚性的特性限制了传统视觉方法的有效性

Method: 提出联合视觉-语言气体泄漏分割框架(JVLGS)，整合视觉和文本模态的互补优势，包含后处理步骤来减少噪声和非目标物体引起的误报

Result: 在多样化场景下的广泛实验表明，JVLGS显著优于最先进的气体泄漏分割方法，在监督学习和少样本学习设置下均表现优异

Conclusion: JVLGS框架通过多模态融合和后处理机制，有效解决了气体泄漏检测中的挑战，为准确可靠的气体泄漏识别提供了新方案

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [19] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM是一个新颖的知识整合框架，可以从异构的预训练模型中提取共识知识，无需依赖数据分布或网络架构假设，显著提升了无监督目标识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识整合方法通常对训练数据分布和网络架构有强假设限制，只能从特定类型的模型中学习，存在数据和归纳偏差。预训练模型的集体共识具有通用性和泛化性，但如何有效利用这些异构模型的集体知识是一个根本性挑战。

Method: 提出了UNIFORM框架，通过专门的投票机制在logit层面（整合能够预测目标类别的教师模型）和特征层面（利用在任意标签空间学习的视觉表示）捕获知识共识。

Result: 大量实验表明，UNIFORM相比强基线方法有效提升了无监督目标识别性能，具有显著的可扩展性，能够从100多个教师模型中受益，而现有方法在更小规模时就达到饱和。

Conclusion: UNIFORM框架成功解决了从异构预训练模型中整合知识的挑战，无需强假设约束，在可扩展性和性能方面都表现出色，为知识迁移提供了新的有效途径。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [20] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow是一个基于扩散模型的框架，仅使用卫星图像生成结构一致的OD流量矩阵，解决了现有方法对辅助数据和空间拓扑敏感性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有OD流量生成方法依赖昂贵的辅助特征（如POI、社会经济统计），且对空间拓扑结构敏感，区域索引重排序会破坏生成流量的结构一致性。

Method: 提出多核编码器捕捉区域交互，采用排列感知扩散过程确保不同区域排序下的潜在表示对齐，通过联合对比训练目标和等变扩散训练实现结构一致性。

Result: 在真实城市数据集上，Sat2Flow在数值精度上优于物理和数据驱动基线方法，同时在索引排列下保持经验分布和空间结构。

Conclusion: Sat2Flow为数据稀缺城市环境提供了可扩展的OD流量生成解决方案，消除了区域特定辅助数据依赖，同时保持结构不变性以实现稳健的移动性建模。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [21] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 该研究提出了一种诊断驱动的半监督框架，用于解决农业杂草自动管理中的环境挑战和数据标注成本问题，通过伪标签技术利用未标注数据提升模型鲁棒性，有效缓解了阴影偏差问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在真实农田环境中性能下降的两个主要问题：具有挑战性的环境条件和高昂的数据标注成本。研究发现模型存在普遍的"阴影偏差"问题，即模型错误地将阴影识别为植被。

Method: 使用包含975张标注图像和10,000张未标注图像的独特数据集，首先建立强监督基线（ResNet分类和YOLO、RF-DETR检测），然后通过诊断分析发现阴影偏差问题，最后开发半监督管道利用未标注数据通过伪标签技术增强模型鲁棒性。

Result: 监督基线模型达到F1分数0.90和mAP50分数超过0.82。半监督框架不仅有效缓解了阴影偏差，还显著提高了召回率这一自动化喷洒系统中的关键指标。在低数据机制下的公开作物-杂草基准测试中验证了方法的有效性。

Conclusion: 该研究提供了一个清晰且经过实地测试的框架，用于开发、诊断和改进适用于精准农业复杂现实环境的鲁棒计算机视觉系统，为解决农业杂草管理中的实际挑战提供了有效解决方案。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [22] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了TAPO和MotionFLUX两个框架，用于解决文本驱动动作生成中的语义对齐和生成效率问题，实现了高质量的实时动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动动作生成方法存在语言描述与动作语义对齐不准确、多步推理效率低的问题，需要提高生成质量和速度。

Method: 提出TAPO框架通过对齐偏好优化来实现细微动作变化与文本修饰词的对齐；提出MotionFLUX框架采用确定性修正流匹配技术，通过构建器器分布与动作空间之间的最优运输路径来实现实时生成。

Result: 实验结果显示，TAPO和MotionFLUX统一系统在语义一致性和动作质量方面都超过了现有最先进方法，同时显著提高了生成速度。

Conclusion: 该研究提供了一个统一的解决方案，同时解决了动作生成中的语义对齐和生成效率问题，为虚拟角色动画和体现式代理提供了高效的实时动作生成能力。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [23] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 提出Discrete Diffusion VLA方法，使用离散扩散模型处理动作序列，统一视觉语言模型的训练目标和解码过程，在多个机器人任务上超越自回归和连续扩散基线


<details>
  <summary>Details</summary>
Motivation: 现有的VLA解码器要么采用固定的自回归顺序生成动作，要么在主干网络外附加连续扩散头，需要专门训练和迭代采样，阻碍了统一可扩展架构的发展

Method: 使用离散扩散模型处理离散化的动作块，与VLM主干网络采用相同的交叉熵目标进行训练，支持自适应解码顺序和二次重掩码机制

Result: 在LIBERO上达到96.3%平均成功率，SimplerEnv Fractal上71.2%视觉匹配率，SimplerEnv Bridge上49.3%总体表现，优于自回归和连续扩散基线

Conclusion: 离散扩散动作解码器支持精确的动作建模和一致的训练，为将VLA扩展到更大模型和数据集奠定了基础

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [24] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench是首个专门评估多视频关系推理能力的综合基准，包含1000个QA对，涵盖三个层次：跨视频对象关联、事件关联和复杂推理。测试发现当前MLLMs在多视频推理方面存在显著性能差距，顶级模型如GPT-4o在因果推理任务上仅达60%准确率，远低于人类的91%。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在单视频任务上表现良好，但其在多视频推理方面的能力尚未得到充分探索，而这种能力对实际应用（如多摄像头监控、跨视频程序学习）至关重要。

Method: 构建CVBench基准，包含来自五个不同领域视频集群的1000个QA对，分为三个层次：对象关联、事件关联和复杂推理。评估了10多个领先MLLMs在零样本和思维链提示下的表现。

Result: 评估结果显示显著性能差距：最佳模型GPT-4o在因果推理任务上仅达到60%准确率，而人类表现达到91%。分析揭示了当前MLLM架构的根本瓶颈，包括跨视频上下文保持能力不足和重叠实体消歧能力差。

Conclusion: CVBench为诊断和推进多视频推理建立了严格框架，为下一代MLLMs提供了架构设计洞见。该基准揭示了当前模型在多视频关系推理方面的局限性，并指出了未来改进的方向。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [25] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack是一个在浏览器中运行的轻量级视线追踪框架，通过集成头部姿态估计和少量样本校准，实现了接近商业级眼动仪的精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI视线估计方法虽然在基准测试中表现优异，但在实际应用中与商业眼动仪存在差距，且存在模型大小、推理时间和隐私等问题。基于摄像头的眼动追踪方法因头部运动导致精度不足。

Method: 提出WebEyeTrack框架，在浏览器中直接集成轻量级SOTA视线估计模型，结合基于模型的头部姿态估计和仅需9个校准样本的少样本学习，实现设备端自适应学习。

Result: 在GazeCapture数据集上达到2.32厘米的误差，在iPhone 14上实现2.4毫秒的实时推理速度，性能达到SOTA水平。

Conclusion: WebEyeTrack成功解决了现有方法的局限性，提供了高精度、实时且保护隐私的浏览器端视线追踪解决方案，代码已开源。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [26] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2是一个端到端模型，能够从单张图像中直接恢复2.5D浮雕，在复杂材质和光照变化下表现出色。相比仅使用合成数据训练的V1版本，V2通过整合真实数据显著提升了鲁棒性、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像恢复2.5D浮雕的挑战，特别是在复杂材质和光照变化条件下。克服大规模真实数据集获取困难的问题，提升模型在真实场景中的性能。

Method: 使用文本到图像生成模型生成约15,000张伪真实图像，并通过深度和法线预测融合获得深度伪标签。构建包含800个样本的小规模真实数据集，采用多视角重建和细节细化。在伪真实和真实数据集上进行渐进式训练。

Result: 综合实验表明，MonoRelief V2在深度和法线预测方面达到了最先进的性能，展现出在下游应用中的强大潜力。

Conclusion: MonoRelief V2通过整合真实数据和创新的训练策略，成功提升了从单图像恢复2.5D浮雕的性能，为相关应用提供了有效的解决方案。

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [27] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet是一个基于DETR架构的高速目标检测器，通过解耦编码器优化策略、几何可变形单元和尺度感知注意力模块，在保持高精度的同时显著降低计算成本，在Intersection-Flow-5k数据集上达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决端到端目标检测器在复杂场景（如交叉路口交通监控）中计算成本过高的问题，实现NMS-free的实时应用。

Method: 提出FlowDet检测器，采用解耦编码器优化策略，包含几何可变形单元(GDU)进行交通感知几何建模，以及尺度感知注意力(SAA)模块处理极端尺度变化。

Result: 在Intersection-Flow-5k数据集上，相比RT-DETR基线，AP提升1.5%，AP50提升1.6%，同时GFLOPs减少63.2%，推理速度提升16.2%。

Conclusion: 为构建高效准确的现实世界感知系统提供了新路径，证明了在保持高精度的同时显著降低计算成本的可行性。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [28] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 提出结合可训练编码器与原型引导重建的统一框架，通过多样性感知对齐损失防止原型坍塌，在医学图像异常检测中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临标注稀缺和领域差距问题，现有重建方法依赖冻结预训练编码器限制了领域适应能力，原型学习方法存在原型坍塌问题

Method: 使用可训练编码器（含动量分支）进行领域自适应特征学习，轻量级原型提取器挖掘正常原型，通过注意力机制指导解码器重建，采用多样性感知对齐损失防止原型坍塌

Result: 在多个医学成像基准测试中显著提升了表示质量和异常定位精度，超越了现有方法

Conclusion: 该框架有效解决了原型坍塌问题，提高了医学图像异常检测的性能和可解释性

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [29] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch是一个新颖的多模态病理图像分割框架，通过图像和文本原型与像素标签的双重对比学习，在结构和语义层面提供监督，显著改善了语义边界建模。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像分割中语义边界模糊和像素级标注成本高的问题，现有方法主要依赖图像模态内的扰动一致性，难以捕捉高层次语义先验。

Method: 提出MPAMatch框架，采用多模态原型引导的监督范式，进行图像原型-像素标签和文本原型-像素标签的双重对比学习，并使用病理预训练基础模型重构分割架构。

Result: 在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI数据集上的广泛实验表明，MPAMatch优于最先进方法，验证了其在结构和语义建模方面的双重优势。

Conclusion: MPAMatch通过引入文本原型监督和双重对比学习机制，有效提升了病理图像分割的性能，特别是在语义边界建模方面表现出色。

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [30] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种定制化人机交互图像生成方法Interact-Custom，通过两阶段模型解决同时保持目标身份特征和控制交互语义的挑战


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注目标实体的外观保持，而忽略了目标实体之间的细粒度交互控制，特别是在人机交互场景中需要同时保持身份特征和交互语义控制

Method: 首先处理大规模数据集，设计两阶段模型Interact-Custom：第一阶段通过生成描述交互行为的前景掩码显式建模空间配置，第二阶段在掩码指导下生成保持身份特征的目标人机交互图像

Result: 在专门为CHOI任务设计的评估指标上进行了广泛实验，证明了方法的有效性

Conclusion: 提出的Interact-Custom方法能够有效解决定制化人机交互图像生成中的身份保持和交互控制问题，并提供了高内容可控性

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [31] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出SGDDM和HoloMamba两种方法解决全彩全息视频生成中的帧率-色彩保真度权衡和计算效率问题，实现高帧率高保真度显示


<details>
  <summary>Details</summary>
Motivation: 现有学习方法产生过度平滑相位导致色彩串扰，帧间优化方法忽略时空相关性导致计算效率低下

Method: SGDDM通过频率调制优化相位分布，HoloMamba采用轻量级Mamba-Unet架构显式建模视频序列的时空相关性

Result: SGDDM实现高帧率高保真度全彩显示，HoloMamba以260+FPS生成1080p全彩全息视频，比现有技术快2.6倍

Conclusion: 提出的方法成功解决了全彩全息视频生成中的关键限制，实现了高帧率和高保真度的统一

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [32] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: 提出Score-based Discriminator Correction (SBDC)方法，通过判别器训练和对抗损失来校正预训练条件扩散模型中的标签噪声问题，仅在生成早期阶段使用指导，计算高效且不增加额外训练成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在大规模数据集上表现出色，但数据集中的手动标注错误会影响生成质量和可控性，现有研究对此问题关注不足。

Method: 基于判别器训练和对抗损失构建指导技术，利用先验噪声检测技术评估样本真实性，限制在生成过程早期阶段使用指导。

Result: 在不同噪声设置下的实验表明，该方法优于现有最先进方法，计算效率高，仅略微增加推理时间，无需重新训练扩散模型。

Conclusion: SBDC方法有效解决了扩散模型中标签噪声导致的生成质量问题，提供了一种高效且实用的校正方案。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [33] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: 该论文针对单目3D目标检测的泛化性问题，提出了多个创新方法：GrooMeD-NMS提升遮挡鲁棒性，DEVIANT主干网络改善数据集泛化，SeaBird方法解决大目标检测问题，并分析了相机高度外推的数学基础。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测在自动驾驶、增强现实等应用中至关重要，但现有模型在遮挡、不同数据集、大目标检测和相机参数变化等多样化场景中的泛化能力不足，需要系统性的解决方案。

Method: 1) 提出数学可微的GrooMeD-NMS处理遮挡问题；2) 探索深度等变(DEVIANT)主干网络提升数据集泛化；3) 引入基于分割的SeaBird方法配合dice loss解决大目标检测；4) 数学分析相机高度外推问题。

Result: 系统性地解决了单目3D检测在遮挡鲁棒性、跨数据集泛化、大目标检测和相机参数外推等多个维度的泛化挑战，提出了相应的有效解决方案。

Conclusion: 该研究为单目3D目标检测的泛化性问题提供了全面的解决方案框架，通过数学分析和创新方法设计，显著提升了模型在多样化实际场景中的适应能力和性能表现。

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [34] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 这篇论文通过实验研究评估了YOLO目标检测模型在各种精度格式下的粗鲁性表现，并提出了一种基于透尽性图像的量化校准策略，但在大多数情况下并未实现一致性的粗鲁性提升。


<details>
  <summary>Details</summary>
Motivation: 评估后训练量化(PTQ)在实际部署中的粗鲁性问题，因为粗鲁性对于对象检测模型在真实世界中面对噪声、模糊、压缩恶化等输入透尽时的性能至关重要。

Method: 使用多种精度格式(FP32、FP16、Dynamic UINT8、Static INT8)对YOLO模型(nano到extra-large)进行实验评估，并提出了一种透尽性图像校准策略，在TensorRT校准过程中混入清洁和合成透尽图像。在COCO数据集上测试七种不同透尽条件和混合透尽场景。

Result: Static INT8 TensorRT引擎在清洁数据上实现了1.5-3.3倍速度提升，mAP50-95下降3-7%。但透尽性校准策略在大多数模型和透尽条件下并未实现一致性的粗鲁性提升，仅在某些噪声条件下的较大模型中观察到显著改善。

Conclusion: 研究结果显示了提高PTQ粗鲁性的挑战性，为在非受控环境中部署量化检测器提供了有价值的见解，并指出模型容量可能影响校准策略的效果。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [35] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 提出了IELDM和IELFormer框架，通过逆演化层抑制扩散模型生成缺陷，并利用多尺度频率融合提升跨域语义分割的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型生成数据中的结构和语义缺陷问题，这些缺陷会导致分割模型性能下降和错误累积

Method: 引入逆演化层(IELs)来突出空间不连续性和语义不一致性，提出IELDM数据增强框架和IELFormer分割网络，包含多尺度频率融合模块

Result: 在基准数据集上的大量实验表明，该方法相比现有方法实现了更优越的泛化性能

Conclusion: 逆演化层能有效抑制生成缺陷和伪影传播，多尺度频率融合增强了跨尺度语义一致性，显著提升了跨域语义分割的泛化能力

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [36] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR是一个可控皮肤图像合成模型，利用病变测量分数和类型标签，通过语言提示生成高质量、临床相关的皮肤图像，在FID指标上比现有最佳方法提升6.3%。


<details>
  <summary>Details</summary>
Motivation: 解决真实临床皮肤图像数据稀缺的问题，现有合成方法生成的图像质量低且无法控制病变位置和类型。

Method: 使用多尺度病变聚焦的VQVAE编码图像为离散潜在表示，然后训练视觉自回归变换器进行图像合成，整合病变测量和类型作为条件嵌入。

Result: 在七种病变类型上获得最佳FID分数（平均0.74），比之前SOTA方法提升6.3%。

Conclusion: LF-VAR能够有效生成高保真度、临床相关的合成皮肤图像，解决了可控皮肤合成的关键问题。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [37] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute是一个针对长尾视觉识别问题的模块化框架，通过难度感知优化和动态专家协作来提升分类性能，特别是在稀有和困难类别上表现显著提升


<details>
  <summary>Details</summary>
Motivation: 长尾视觉识别不仅面临类别不平衡问题，还存在不同类别学习难度差异大的挑战。简单的基于频率的类别重加权方法往往忽略了那些本质上难以学习的类别

Method: DQRoute首先基于预测不确定性和历史性能估计类别难度，用这个信号指导自适应损失加权训练。在架构上采用混合专家设计，每个专家专注于类别分布的不同区域。推理时通过专家特定的OOD检测器生成的置信度分数加权专家预测，实现无需集中路由器的输入自适应路由

Result: 在标准长尾基准测试上的实验表明，DQRoute显著提高了性能，特别是在稀有和困难类别上

Conclusion: 将难度建模与去中心化专家路由相结合具有明显优势，为长尾视觉识别提供了有效的解决方案

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [38] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT是一个新颖的协作感知框架，使用点级优化令牌来保留3D结构信息，通过语义感知令牌重排序、频率增强状态空间模型和邻居到自车对齐模块，在降低通信和计算开销的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法通常使用2D鸟瞰图表示，丢弃了关键的细粒度3D结构线索，这些线索对于精确的目标识别和定位至关重要。

Method: 1) 引入点级令牌作为中间表示；2) 语义感知令牌重排序模块生成自适应1D重排序；3) 频率增强状态空间模型捕获长距离序列依赖；4) 邻居到自车对齐模块进行全局和局部对齐。

Result: 在模拟和真实数据集上的广泛实验表明，CoPLOT优于最先进的模型，同时通信和计算开销更低。

Conclusion: CoPLOT通过点原生处理流程有效解决了点云数据的无序性、海量性和位置敏感性挑战，为协作感知提供了更精确的3D结构信息保留方案。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [39] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级的无监督骨架基动作定位方法，通过空间-时间图神经网络和动作动力学指标，在无需手动标注的情况下实现了与监督方法相当的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度动作定位方法通常需要大量标注数据和高容量模型，计算成本高且适应性差，难以应用于实际场景。需要一种轻量级、无监督的方案来解决这些问题。

Method: 使用关注机制空间-时间图卷积网络(ASTGCN)在姿势序列去噪任务上进行预训练，通过块分区域学习内在运动动力学。推理时使用新的动作动力学指标(ADM)，直接从低维ASTGCN嵌入中计算，通过梯度曲线的折点检测运动边界。

Result: 在DSV跳水数据集上实现了82.66%的平均精度(mAP)和29.09毫秒的平均定位延迟，达到了与最先进监督方法相当的性能。同时在未见过的野外跳水视频中也显示了良好的演续性能。

Conclusion: 该方法提供了一种高效、轻量级的无监督动作定位解决方案，在保持计算效率的同时实现了与监督方法相当的性能，适合嵌入式或动态环境中的实时动作分析系统。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [40] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出一种基于动态生成核的迭代图像去噪方法，通过特征提取、全局统计和局部相关性模块来预测像素级变化核，在单一高斯噪声训练下实现多种噪声类型和级别的优秀泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习去噪方法依赖特定噪声分布，泛化能力有限且容易过拟合，需要大量训练数据和计算资源。

Method: 使用特征提取模块获取噪声不变特征，通过全局统计和局部相关性模块捕捉噪声特性和结构关联，核预测模块生成像素级变化核进行迭代去噪。

Result: 紧凑模型（约0.04M参数）在单一高斯噪声训练下，对多种噪声类型和级别均表现出色，展示了迭代动态滤波的实用价值。

Conclusion: 动态核生成方法有效防止过拟合，提升对未知噪声的鲁棒性，为实际图像去噪应用提供了高效且高质量的解决方案。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [41] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge是一个专门评估大型视频语言模型位置偏见的基准测试，通过标准化探针和定制化上下文设置，系统性地分析模型在不同视频位置的表现偏差。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准测试主要评估整体性能，忽略了位置偏见这一关键但未被充分探索的方面，需要专门的评估工具来系统分析LVLMs的位置敏感性。

Method: 采用标准化探针和定制化上下文设置，灵活控制上下文长度、探针位置和上下文类型；结合统计测量和形态模式识别方法进行综合分析；包含438个手动策划视频，生成1,177个多选题和120个开放式问题。

Result: 评估27个最先进的LVLMs发现，许多领先开源模型存在显著位置偏见（通常表现为头部或邻近内容偏好），而商业模型如Gemini2.5-Pro在整个视频序列中表现一致且出色。

Conclusion: 该基准测试有效暴露了LVLMs的位置偏见问题，为缓解偏见和指导模型改进提供了可行见解，特别是在上下文长度、上下文变化和模型规模方面的分析具有重要指导意义。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [42] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: 这篇论文提出了ODAL框架，通过分布式设计在车载系统与云端之间分配计算任务，解决车辆内部物体检测和定位的计算资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 车辆内部的AI任务如物体识别和定位对个人助手的响应质量至关重要，但车载系统的计算资源受限制，无法直接部署大型视觉基础模型。

Method: 提出ODAL框架，采用分布式架构，将计算任务在车载系统和云端之间分配，并介绍了ODALbench评估指标。比较了GPT-4o和LLaVA 1.5 7B模型，并通过微调提升轻量模型性能。

Result: 经过微调的ODAL-LLaVA模型达到了89%的ODAL得分，比基线性能提升71%，超过GPT-4o近20%。同时显著减少了幻觉现象，ODAL_SNR指标是GPT-4o的3倍。

Conclusion: ODAL框架能够有效解决车载系统计算资源限制问题，通过微调轻量模型可以达到超过大型基础模型的性能，为车辆内部场景理解领域建立了新标准。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [43] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1是一种自奖励方法，通过强化学习改进视觉语言模型的视觉推理能力，无需外部视觉监督，有效减少视觉幻觉和语言捷径问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型存在视觉幻觉和语言捷径问题，现有方法依赖人工标注或外部模型监督，成本高且容易导致分布偏移。需要一种无需外部监督的自我改进方法。

Method: 将VLM推理分解为视觉感知和语言推理两阶段，模型首先生成自包含的视觉感知，然后使用相同模型仅基于生成的感知进行语言推理来计算奖励，结合最终输出监督进行训练。

Result: 实验证明Vision-SR1能改进视觉推理能力，减轻视觉幻觉，减少对语言捷径的依赖，在多种视觉语言任务中表现优异。

Conclusion: Vision-SR1提供了一种有效的自监督方法，通过内部自我奖励机制平衡视觉感知和语言推理的训练信号，解决了VLM的视觉幻觉和语言捷径问题。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [44] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: SNNs在数字实现中的能效优势受到质疑，本研究通过卫星位置估计任务比较硬件感知和硬件无关的能耗评估方法，发现SNNs仅在神经形态硬件和高输入稀疏性下才具有显著能效优势


<details>
  <summary>Details</summary>
Motivation: 近期研究表明SNNs在数字实现中的能效优势可能被高估，需要更透明的评估方法来公平比较神经网络能效

Method: 使用LIF神经元膜电位进行多输出回归训练，在逼真卫星数据集上比较SNN和CNN性能，采用硬件感知和硬件无关两种能耗评估方法

Result: SNN达到与CNN相当的MSE性能，硬件无关方法预测SNN有50-60%能效优势，但硬件感知分析显示仅在神经形态硬件和高输入稀疏性下才有显著节能

Conclusion: 数据特性和硬件假设对能效评估影响重大，需要透明评估方法和明确假设披露来确保神经网络能效比较的公平性

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [45] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出了一种新颖的频率感知自监督学习方法，用于超宽场视网膜图像增强，通过频率解耦去模糊和Retinex引导的照明补偿模块，有效解决UWF图像质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 超宽场视网膜成像虽然提供了全面的视网膜视图，但经常受到模糊和光照不均等质量退化因素的影响，这些因素会掩盖精细细节和病理信息。现有的视网膜图像增强方法往往无法满足UWF的特殊需求，特别是需要保留病理细节的要求。

Method: 采用频率感知自监督学习方法，包含频率解耦图像去模糊模块（引入非对称通道集成操作结合全局和局部视图）和Retinex引导的照明补偿模块（提出颜色保护单元提供多尺度空间和频率信息）。

Result: 实验结果表明，该方法不仅提高了可视化质量，还通过恢复和校正精细局部细节和不均匀强度来改善疾病诊断性能。

Conclusion: 这是UWF图像增强的首次尝试，为改善视网膜疾病管理提供了一个强大且具有临床价值的工具。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [46] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT是一个单视图纹理3D人体重建框架，通过统一学习多种几何先验和使用监督特征正则化模块，有效解决几何模糊性和数据稀缺问题，实现了高质量3D虚拟形象重建。


<details>
  <summary>Details</summary>
Motivation: 单视图3D人体重建面临几何模糊性和3D训练数据稀缺的挑战，现有方法难以有效整合不同几何模态，导致视角不一致和面部扭曲等问题。

Method: 提出两阶段SAT框架：1）统一学习SMPL模型和法线图等几何先验；2）引入监督特征正则化模块，通过多视图网络提供中间特征监督；3）在线动画增强模块通过前馈动画网络在线生成大量训练样本。

Result: 在两个基准测试上的广泛实验表明，该方法在重建质量上优于现有最先进方法。

Conclusion: SAT框架通过统一几何学习、特征监督正则化和在线数据增强，有效解决了单视图3D人体重建的关键挑战，实现了高质量的纹理3D虚拟形象生成。

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [47] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: 基于物理发生图论的无监督假图检测方法，通过图象的Bethe-Hessian谱间隔识别真实与生成图像，达到94%准确率


<details>
  <summary>Details</summary>
Motivation: 当前GAN和扩散模型生成的图像已难以与真实照片区分，伤害媒体证据学和生物识别安全。现有监督方法对新生成器敏感，而无监督方法依赖低级统计特征容易被攻击

Method: 将假图检测模型化为稀疏权重图上的社区发现问题。使用预训练CNN提取图像特征并降维到32维，构建Multi-Edge Type QC-LDPC图。通过Nishimori温度梯度调节边耦合，形成随机铁磁Ising模型，分析其Bethe-Hessian谱的特征间隔

Result: 在二元分类任务（猫vs狗，男vs女）上达到超过94%的准确率。真实图像集显示多个明显分离的谱间隔，而生成图像谱线呈现凸陷状态

Conclusion: 该方法提供了一种新题的无监督假图检测方案，具有模型无关性、高准确率和强镇健性。主要贡献包括新的LDPC图构建方法、Nishimori温度RBIM与Bethe-Hessian谱的分析联系，以及实用的假图检测器

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [48] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS是一种增强3D高斯泼溅表示的方法，通过引入对象标签和跨视图一致的语义掩码，解决了3DGS缺乏3D分割能力的问题，实现了22倍训练加速。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)虽然提供了高保真重建和高效渲染，但缺乏3D分割能力，限制了其在需要场景理解任务中的应用。识别和分离特定对象组件至关重要。

Method: 提出LabelGS方法，为高斯表示添加对象标签，引入跨视图一致的语义掩码，使用遮挡分析模型避免优化过程中的过拟合，主高斯标记模型将2D语义先验提升到3D高斯，以及高斯投影滤波器避免标签冲突。采用随机区域采样策略优化3DGS过程。

Result: LabelGS在3D场景分割任务中优于包括Feature-3DGS在内的先前最先进方法，在1440X1080分辨率下实现了22倍的训练加速。

Conclusion: LabelGS通过增强高斯表示的分割能力，显著提高了3D场景理解和分割的效率与性能，为3D场景分析任务提供了有效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [49] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种免训练的视频恢肌分割方法FreeVPS，通过结合SAM2模型和两个关联模块来解决长期跟踪中的错误积累问题，实现了更好的时空建模和领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频恢肌分割方法在时空建模和领域泛化之间彘在平衡困难，而SAM2在长期跟踪中又存在错误积累问题，影响分割稳定性。

Method: 重构VPS任务为跟踪-检测范式，利用IPS模型的空间上下文和SAM2的时间建模能力。设计了两个免训练模块：内部关联筛选模块消除空间不准确性，部间关联精炼模块防止错误传播。

Result: 在领域内外场景中都达到了最先进性能，并在长时间未剪辑的细肠镜视频中展现了稳健的跟踪能力。

Conclusion: FreeVPS通过结合SAM2和免训练模块，有效解决了长期跟踪中的错误积累问题，具有强大的时空建模和领域泛化能力，具备可靠临床分析潜力。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [50] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 基于人脸基础模型的鲁棒视频深度伪造检测框架，通过自监督学习和多数据集集成训练，结合三元组损失和属性监督，在真实场景中表现出强大的泛化能力


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术日益逼真和普及，对媒体真实性和信息完整性构成严重威胁。现有的深度伪造检测模型往往难以泛化到训练分布之外的真实媒体内容，特别是在野外场景中表现不佳

Method: 利用人脸基础模型FSFM（自监督训练在真实人脸数据上）作为基础，通过集成多种深度伪造数据集（包括人脸交换和人脸重演）进行微调。采用三元组损失变体增强判别能力，使真实和伪造样本的嵌入更加可分。同时探索基于属性的监督方案，按操纵类型或源数据集对深度伪造进行分类

Result: 在多样化评估基准上的广泛实验证明了该方法的有效性，特别是在具有挑战性的真实世界场景中表现出色

Conclusion: 该研究提出了一种具有强大泛化能力的鲁棒视频深度伪造检测框架，通过利用人脸基础模型的丰富表征和多策略训练方法，显著提升了在真实场景中的检测性能

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [51] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了POEv2框架，可同时用于通用线段检测和线框线段检测，是POE方法的改进版本，结合边缘检测器在多个数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有线段检测器分为通用线段检测器和线框线段检测器两类，由于设计目标不同，彼此在对方任务上表现不佳，需要一种能同时处理两种任务的鲁棒框架

Method: 基于像素方向估计(POE)方法的改进版本POEv2，从边缘强度图检测线段，可与任何边缘检测器结合使用

Result: 通过与高效边缘检测器结合，在三个公开数据集上实现了最先进的性能

Conclusion: POEv2提供了一个统一的框架，能够有效处理通用线段检测和线框线段检测两种任务，具有很好的实用性和性能表现

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [52] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM是一个自提示光场分割模型，通过统一多尺度特征嵌入块和多尺度自适应滤波适配器，解决了现有方法忽略提示信息提取和频域信息分析的问题，在光场显著目标检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有Segment Anything Model在光场显著目标检测中表现优秀，但大多数模型忽略了提示信息的提取，同时传统方法忽视了频域信息分析，导致小目标容易被噪声淹没。

Method: 提出了SPLF-SAM模型，包含统一多尺度特征嵌入块(UMFEB)和多尺度自适应滤波适配器(MAFA)。UMFEB能够识别不同大小的多个目标，MAFA通过学习频域特征有效防止小目标被噪声淹没。

Result: 在十个最先进的光场显著目标检测方法上进行了广泛实验，证明了该方法的优越性。

Conclusion: SPLF-SAM模型通过创新的多尺度特征嵌入和频域滤波技术，显著提升了光场显著目标检测的性能，特别是在小目标检测方面表现突出。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [53] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar是一个快速3D头像重建框架，使用单一统一模型在几秒内从各种日常记录（单张图像、多视角观察或单目视频）重建高质量的3D高斯溅射模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像重建方法存在时间复杂度过高、对数据质量敏感以及数据利用率低等问题，需要一种能够灵活利用多样化输入数据并快速重建高质量3D模型的方法。

Method: 采用大型高斯重建变换器，包含三个关键设计：变体VGGT式变换器架构聚合多帧线索并注入初始3D提示；多粒度引导编码缓解动画引起的错位；通过地标跟踪和切片融合损失进行增量高斯聚合。

Result: 实验表明FastAvatar在质量和速度方面都优于现有方法，支持增量重建，即随着更多观察数据输入而提升质量。

Conclusion: FastAvatar提供了一个质量-速度可调的高可用头像建模范式，能够高效利用输入数据，在几秒内完成高质量3D头像重建。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [54] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet是一个用于传粉昆虫监测的大规模数据集，包含7856张高分辨率图像和8000多个标注实例，使用YOLOv12和RF-DETR模型实现了高效的蜜蜂识别。


<details>
  <summary>Details</summary>
Motivation: 传粉昆虫对全球粮食生产和生态系统稳定至关重要，但其种群数量正在下降。需要可扩展的自动化监测方法来应对这一挑战。

Method: 收集真实农业田间条件下的高分辨率传粉昆虫图像，使用YOLOv12模型生成初始标注并通过人工验证完善。将所有图像预处理为256×256图块以改善小昆虫检测。采用基于transformer的RF-DETR目标检测器建立基准。

Result: 模型在蜜蜂和大黄蜂类别上分别达到0.94和0.92的高F1分数，混淆矩阵显示类别间误分类极少。最佳mAP@0.50为0.559，未识别类别由于标签模糊和样本频率较低而更具挑战性。

Conclusion: BuzzSet为小目标检测、标签噪声下的类别分离以及生态计算机视觉提供了有价值的基准数据集，支持可扩展的自动化传粉昆虫监测。

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [55] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出AIM方法解决多模态学习中的优化偏差问题，通过自适应网络内调制实现平衡的多模态学习，无需抑制主导或弱势模态


<details>
  <summary>Details</summary>
Motivation: 现有不平衡多模态学习方法通常通过抑制主导模态来促进弱势模态，这影响了整体多模态性能。研究发现根本原因是网络内部的优化偏差问题

Method: AIM方法将主导模态的未优化参数解耦到辅助块中，鼓励在联合训练中依赖这些性能下降的块，同时根据网络深度自适应调整调制强度

Result: AIM在多个基准测试中优于最先进的不平衡模态学习方法，并在不同骨干网络、融合策略和优化器上表现出强大的泛化能力

Conclusion: AIM首次实现了不抑制任何模态的平衡多模态学习，有效解决了网络内部优化偏差问题，为多模态学习提供了新的解决方案

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [56] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: 该论文提出了一种结构化手写数学表达式识别方法，通过自动标注系统和模块化结构识别系统，实现了符号到轨迹的显式对齐，提高了错误分析和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基于编码器-解码器架构的大语言模型在LaTeX生成方面表现优异，但缺乏符号到轨迹的显式对齐，这限制了错误分析、可解释性以及需要选择性内容更新的空间感知交互应用。

Method: 1) 使用神经网络自动标注系统，将LaTeX方程映射到原始轨迹，自动生成符号分割、分类和空间关系标注；2) 模块化结构识别系统，独立优化分割、分类和关系预测，结合基于图的轨迹排序、混合卷积-循环网络和基于transformer的校正。

Result: 在CROHME-2023基准测试中取得了有竞争力的性能，生成了完整的图结构，直接将手写轨迹与预测符号链接。

Conclusion: 该方法通过结构化识别实现了符号到轨迹的显式对齐，为错误分析和可解释输出提供了透明性，对教育技术应用具有重要意义。

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [57] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo框架通过动态评分分区策略，将3D高斯分为高动态和低动态区域，对高动态区域进行时间分区和网络复制以捕捉精细运动细节，同时使用跨帧一致性损失确保视觉连续性，在保持计算效率的同时显著提升动态场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于变形的3D高斯泼溅方法在处理动态场景时，由于使用单一统一模型来表征多样化的运动模式，往往会产生模糊渲染结果并丢失高度动态区域的精细运动细节。

Method: 提出动态评分分区策略，区分高动态和低动态3D高斯；对高动态高斯进行时间递归分区并为每个时间段复制变形网络；对低动态高斯保持静态以减少计算成本；引入跨帧一致性损失解决分区边界视觉不连续问题。

Result: 大量实验表明，MAPo在保持可比计算成本的同时，相比基线方法实现了更优越的渲染质量，特别是在具有复杂或快速运动的区域表现突出。

Conclusion: MAPo框架通过创新的动态分区策略和一致性约束，有效解决了动态3D高斯泼溅中的运动细节丢失和视觉不连续问题，为高保真动态场景重建提供了有效解决方案。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [58] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic是一个单步扩散模型，用于多视角材质估计，能够高质量、低方差地预测材质参数，在PSNR和MSE指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的材质估计方法采用多步去噪策略，耗时且存在随机性，与确定性材质估计任务冲突，导致结果方差较高。

Method: 提出单步扩散模型StableIntrinsic，在像素空间设计基于材质特性的损失函数，并引入细节注入网络(DIN)来消除VAE编码导致的细节损失。

Result: 实验结果显示，该方法在albedo的PSNR上提升9.9%，金属和粗糙度的MSE分别降低44.4%和60.0%，超越当前最优技术。

Conclusion: StableIntrinsic通过单步扩散和细节注入网络，成功解决了多步扩散模型的耗时和方差问题，实现了高质量、低方差的材质估计。

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [59] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成中多对象颜色属性的语义对齐问题，提出了专门的图像编辑技术来解决多颜色提示的语义对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在处理复杂多对象提示时难以准确捕捉精确语义，特别是在颜色属性方面存在显著问题。现有方法主要使用粗粒度指标或人工评估，难以进行大规模评估。

Method: 作者进行了颜色属性的案例研究，发现预训练模型在处理多颜色提示时存在困难。为此，他们引入了一种专门的图像编辑技术来解决多对象语义对齐问题。

Result: 该方法在各种基于扩散的文本到图像技术生成的图像上显著提升了性能，在广泛的评估指标上都表现出色。

Conclusion: 研究表明多颜色提示的语义对齐是一个重要挑战，提出的专用编辑技术能有效解决这一问题，为文本到图像生成中的精确语义控制提供了新的解决方案。

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [60] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 提出了一种基于编码器-解码器结构的增强神经网络架构，通过综合注意力块、Mamba架构和数据融合块等技术，显著提高了非生物降解废物自动分拣的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 废物管理领域中，非生物降解材料的自动分拣面临复杂多变的废物流带来的挑战，需要更准确高效的分类方法。

Method: 在编码器-解码器结构基础上集成综合注意力块（结合卷积和上采样操作）、Mamba架构的注意力机制，以及数据融合块（使用PCA降维处理多通道图像数据）。

Result: 在RGB、高光谱、多光谱以及RGB与高光谱组合数据上的评估表明，该方法显著优于现有方法。

Conclusion: 该增强神经网络架构通过创新的注意力机制和数据融合技术，有效解决了复杂废物流的自动分拣问题，性能表现优异。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [61] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 通过多域训练、均衡采样、细心数据增帽和难例挖掘等技巧，基于RTMDet单阶段检测器实现了高效准确的有丝切裂图识别，在多个数据集上获得0.78-0.84的F1分数，适合临床部署。


<details>
  <summary>Details</summary>
Motivation: 解决组织学分析图像中有丝切裂图检测面临的挖发变性、染色协议、组织类型和人工物影响等挖战，需要开发能够在多样化领域中稳健运行的实时检测方案。

Method: 基于RTMDet单阶段物体检测器，采用多域训练数据、均衡采样策略、细心设计的数据增帽，并针对坏死组织和残留物进行难例挖掘来降低假阻性。

Result: 在多个MF数据集上进行分组5折交叉验证，F1分数范围0.78-0.84。在MIDOG 2025挑战赛预测试集上获得0.81的F1分数，超过更大模型并显示出对新领域的适应能力。

Conclusion: 该方案提供了准确性和速度之间的实用平衡，具有强大的领域适应性，适合实际临床应用部署。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [62] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出CSSL框架，通过上下文感知阈值动态调节神经元激活，在事件相机视觉任务中实现高稀疏性和优异性能


<details>
  <summary>Details</summary>
Motivation: 现有事件数据处理方法未能充分利用事件数据的稀疏性，而脉冲神经网络在复杂视觉任务中性能不足，且实现高激活稀疏性需要复杂的手动调参

Method: 提出上下文感知稀疏时空学习(CSSL)框架，采用上下文感知阈值技术，根据输入分布动态调节神经元激活，无需显式稀疏约束即可自然降低激活密度

Result: 在事件目标检测和光流估计任务中，CSSL达到或超越最先进方法性能，同时保持极高的神经元稀疏性

Conclusion: CSSL为神经形态处理实现高效事件视觉提供了关键解决方案，在性能和效率之间取得了良好平衡

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [63] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS是一个无监督视频实例分割框架，通过质量引导的自训练方法，在不需要人工标注的情况下实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割需要像素级掩码和时间一致性标注，标注成本高昂。现有无监督方法依赖合成数据但存在合成到真实域的差距问题。

Method: 建立伪标签生成和自动质量评估的闭环系统，通过质量引导的自训练实现从合成视频到真实视频的渐进式适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA方法VideoCutLER提升4.4%，且无需人工标注。

Conclusion: 质量感知的自训练方法对于无监督视频实例分割是可行的有效方案。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [64] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: 提出ERSR半监督框架用于胎儿头部超声分割，通过双评分过滤、椭圆约束伪标签精炼和对称一致性正则化，在两个基准数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 胎儿头部超声自动分割对产前监测至关重要，但超声图像质量差和标注数据缺乏使鲁棒分割具有挑战性。现有半监督方法难以处理胎儿头部超声图像的特殊性，无法生成可靠伪标签和实施有效一致性约束

Method: ERSR框架包含三个核心组件：1)双评分自适应过滤策略，使用边界一致性和轮廓规则性标准评估过滤教师输出；2)椭圆约束伪标签精炼，通过最小二乘椭圆拟合强化中心像素并抑制噪声；3)对称多一致性正则化，在扰动图像、对称区域和原始预测与伪标签之间实施多级一致性

Result: 在HC18数据集上，使用10%和20%标注数据分别达到92.05%和95.36%的Dice分数；在PSFH数据集上，相同设置下分别达到91.68%和93.70%的Dice分数

Conclusion: ERSR框架通过创新的伪标签生成和一致性正则化方法，有效解决了胎儿头部超声分割中的挑战，在两个基准数据集上实现了最先进的性能表现

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [65] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 通过频域视角分析分布偏移问题，提出低频筛波策略和梯度基于校准机制，在无需目标域信息的情况下提升分布偏移时的模型校准性


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在分布偏移条件下容易产生过份自信的预测，影响可靠性。现有方法需要目标域信息或模拟，在实际应用中有限

Method: 从频域角度分析分布偏移对高频视觉线索的影响，采用低频筛波策略促进模型依赖域不变特征，并通过梯度基于校准机制确保分布内校准性

Result: 在CIFAR-10/100-C和WILDS等合成和实际偏移数据集上，方法显著提升了分布偏移条件下的校准性，同时保持了强劲的分布内性能

Conclusion: 该方法提供了一种无需目标域信息的实用方案，通过频域分析和校准约束有效解决了分布偏移下的模型校准问题

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [66] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 这篇论文提出了一种机器视角中心的图像质量评估（MIQA）框架，通过区域感知MIQA模型进行空间降级分析，在多个维度上显示优异性能，解决了传统人视觉系统评估方法在机器视角系统中的不足。


<details>
  <summary>Details</summary>
Motivation: 机器视角系统（MVS）在恶劣可见条件下容易出现性能降级，而传统基于人视觉系统（HVS）的图像质量评估方法对于MVS质量预测效果差强，需要专门为机器视角设计的质量评估方法。

Method: 构建了包含250万样本的机器视角图像质量数据库（MIQD-2.5M），涵盖75个视觉模型、250种降级类型和三个典型视觉任务。提出区域感知MIQA（RA-MIQA）模型，通过细粒度空间降级分析来评估MVS视觉质量。

Result: RA-MIQA在多个维度上显示优异性能，在图像分类任务上一致性和准确性方面分别获得13.56%和13.37%的SRCC提升。实验还发现了任务特定的降级敏感性，以及HVS基于指标在MVS质量预测中的不足。

Conclusion: 该研究提供了一种有效的机器视角中心图像质量评估方法，能够提升MVS的可靠性，为机器视角图像处理和优化奠定了基础。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [67] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出统一的两阶段预测框架，联合建模自我中心场景中的动作和视觉未来，通过手部轨迹条件化，实现动作预测和视频生成的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时建模动作预测和视觉结果。VLA模型只关注动作预测，缺乏对视觉场景影响的显式建模；视频预测模型生成未来帧但不基于特定动作，常产生不合理结果。需要统一框架来同时处理这两个方面。

Method: 两阶段框架：第一阶段进行连续状态建模处理多模态输入，显式预测未来手部轨迹；第二阶段引入因果交叉注意力融合多模态线索，利用推断的动作信号指导基于图像的潜在扩散模型进行逐帧未来视频生成。

Result: 在Ego4D、BridgeData和RLBench数据集上的实验表明，该方法在动作预测和未来视频合成方面均优于最先进的基线方法。

Conclusion: 该方法是第一个统一处理自我中心人类活动理解和机器人操作任务的模型，能够显式预测即将发生的动作及其视觉后果，为人类-物体交互理解和机器人规划提供了有效解决方案。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [68] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN是一个多模态条件网格生成对抗网络，用于3D主动脉瘤生长预测，结合局部KNN卷积和全局图卷积网络，在几何精度和临床直径估计方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主动脉瘤进展的个性化准确预测对于及时干预至关重要，但由于需要同时建模复杂3D几何中的细微局部变形和全局解剖变化，这一任务仍然具有挑战性。

Method: 提出MCMeshGAN，采用双分支架构：新颖的局部KNN卷积网络（KCN）保留精细几何细节，全局图卷积网络（GCN）捕捉长程结构上下文。专用条件分支编码临床属性和目标时间间隔，生成解剖学合理、时间可控的预测。

Result: 在包含590个多模态记录的TAAMesh数据集上，MCMeshGAN在几何精度和临床重要直径估计方面始终优于最先进的基线方法。

Conclusion: 该框架为临床可部署的个性化3D疾病轨迹建模提供了稳健的一步，源代码已公开。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [69] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于ProtoScale模块的自监督学习方法，通过语义分组、实例分离和层次结构来建立结构化的视觉表征，在对象检测任务上超过了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 虽然现有自监督学习方法在全局图像理解上表现强劲，但在抓取场景中的结构化表征方面存在限制，特别是在密集预测任务中。

Method: 提出基于ProtoScale模块的自监督学习方法，通过语义分组、实例级分离和层次结构来渐进式建立结构化表征。不同于DINO等依赖随机裁剪的方法，该方法在增强视图中保持完整场景上下文。

Result: 在COCO和UA-DETRAC数据集组合子集上的对象检测任务中，该方法学习到了以对象为中心的表征，显著提升了监督学习对象检测的性能，超过了现有最佳方法，甚至在标注数据有限和少量微调训练时也能保持优异表现。

Conclusion: 该研究提供了一种有效的自监督学习方法，能够渐进式构建结构化的视觉表征，特别适合密集预测任务，为少量标注数据情况下的对象检测提供了更好的解决方案。

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [70] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet是一个基于transformer的新模型，通过结合未来行人轨迹和车辆速度预测来预测行人过街意图，在推理时间和性能方面都达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆上路，预测行人过街意图成为重要研究领域。现有方法需要改进以更准确高效地预测行人是否会过马路。

Method: 提出TrajFusionNet模型，包含序列注意力模块(SAM)和视觉注意力模块(VAM)两个分支。SAM学习观察到的和预测的行人轨迹与车辆速度的序列表示，VAM通过将预测的行人边界框叠加到场景图像上来学习视觉表示。

Result: 模型实现了最低的总推理时间（包括模型运行时间和数据预处理），在三个最常用的行人过街意图预测数据集上都达到了最先进的性能。

Conclusion: TrajFusionNet通过轻量级模态的组合，在行人过街意图预测任务中实现了优异的性能和效率，为自动驾驶系统提供了有效的解决方案。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [71] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出基于互信息的天空背景估计模型SMI，通过双网络结构解决传统天空光纤平均光谱缺乏环境建模的问题，在LAMOST光谱数据上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 当前天空背景扣除主要依赖天空光纤光谱构建超天空平均光谱，但缺乏对目标周围环境的建模，需要更精确的天空背景估计方法

Method: SMI模型包含两个主要网络：第一个网络使用波长校准模块从光谱中提取天空特征，解决特征偏移问题；第二个网络采用增量训练方法最大化不同光谱表示间的互信息来捕获共同成分，同时最小化相邻光谱表示的互信息来获得个体成分

Result: 在LAMOST光谱数据上的实验表明，SMI能够在观测过程中获得更好的目标天空背景，特别是在蓝端表现更优

Conclusion: 基于互信息的SMI模型能够有效估计天空背景，解决了传统方法的局限性，为多目标光纤光谱处理提供了更精确的天空背景扣除方案

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [72] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 本研究探讨了使用多光谱LiDAR和深度学习模型进行城市树木点云提取的方法，评估了三种先进模型的性能，其中SPT模型在时间效率和准确性方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 监测城市树木动态对支持绿化政策和减少电力设施风险至关重要。虽然机载雷达扫描提高了大规模树木管理，但复杂的城市环境和树木变异性仍带来挑战。多光谱LiDAR能同时获取3D空间和光谱数据，为详细地图制作提供了可能。

Method: 研究使用多光谱LiDAR数据，评估了三种先进的深度学习模型：Superpoint Transformer (SPT)、Point Transformer V3 (PTv3)和Point Transformer V1 (PTv1)。比较了仅使用空间信息与结合伪归一化植被指数(pNDVI)的性能差异。

Result: SPT模型在时间效率和准确性方面表现最优，平均交并比(mIoU)达到85.28%。结合pNDVI和空间数据的方法获得最高检测准确性，与仅使用空间信息相比，错误率减少10.61个百分点。

Conclusion: 这些发现显示了多光谱LiDAR和深度学习在改善树木提取和推进树木资产清查方面的潜力，为城市树木管理提供了更加高效准确的技术支撑。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [73] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了PersonaAnimator框架，通过从无约束视频中学习个性化运动模式，解决了现有运动生成方法在风格学习、数据依赖和物理合理性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法存在三个主要问题：(1)姿态引导的运动迁移方法仅复制动作而不学习风格特征；(2)运动风格迁移方法严重依赖难以获取的动作捕捉数据；(3)生成的运动有时违反物理规律。

Method: 提出PersonaAnimator框架，直接从无约束视频学习个性化运动模式，支持个性化运动迁移。构建首个基于视频的个性化运动数据集PersonaVid（包含20个运动内容类别和120个运动风格类别），并提出物理感知的运动风格正则化机制确保生成运动的物理合理性。

Result: 大量实验表明，PersonaAnimator在运动迁移方法中表现优于现有最先进方法，为视频到视频运动个性化任务设立了新的基准。

Conclusion: 该研究开创了视频到视频运动个性化新任务，提出的框架能够有效解决现有方法的局限性，实现高质量的个性化运动生成。

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [74] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文首次全面综述了高光谱成像(HSI)在汽车ADAS/AD应用中的现状，分析了216款商用HSI相机，发现仅有4款满足性能阈值且无一款符合AEC-Q100标准，揭示了HSI研究潜力与商业就绪度之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能够提供超越传统RGB成像的光谱分辨率，实现材料级别的场景理解，为高级驾驶辅助系统和自动驾驶应用提供变革性的感知能力，但目前缺乏对其在汽车领域应用现状的全面评估。

Method: 通过定性综述分析当前HSI技术的优势和局限性，并对216款商用HSI和多光谱成像相机进行基准测试，评估帧率、空间分辨率、光谱维度和AEC-Q100温度标准符合性等关键汽车标准。

Result: 分析显示仅有4款相机满足性能阈值，无一款符合AEC-Q100要求。现有HSI数据集在规模、光谱一致性、光谱通道数量和环境多样性方面存在局限，制约了感知算法的开发和HSI真正潜力的验证。

Conclusion: HSI在汽车应用中存在研究潜力与商业就绪度之间的显著差距，需要朝着实际集成到ADAS和自动驾驶系统的关键研究方向努力。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [75] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 提出了对象级集合相似度(OSS)指标，无需训练检测器即可评估主动学习方法效果，并选择代表性验证集，解决了目标检测中主动学习的高计算成本和评估可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界目标检测中的主动学习面临计算成本高（单个检测器训练需282 GPU小时）和评估可靠性差（不同验证集方法排名差异大）的挑战，限制了在安全关键系统中的应用。

Method: 开发了对象级集合相似度(OSS)指标，通过对象级特征量化训练集与目标域的相似性，无需训练检测器即可评估主动学习方法效果，并能选择代表性验证集。

Result: 在三个自动驾驶数据集(KITTI、BDD100K、CODA)上验证了OSS的有效性，使用两种检测器架构(EfficientDet、YOLOv3)和基于不确定性的主动学习方法作为案例研究。

Conclusion: OSS是第一个基于对象相似性统一目标检测中主动学习训练和评估策略的方法，具有检测器无关性、仅需标注对象裁剪、可与现有主动学习管道集成等优点，为实际应用提供了实用框架。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [76] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 通过利用2D基础模型生成的分割面具来扩充稀疏的3D标注，并通过几何对应和一致性正则化提升3D弱监督分割性能


<details>
  <summary>Details</summary>
Motivation: 解决大规模3D点云数据标注困难问题，充分利用2D和3D数据的互补性，突破现有方法对伪标签利用不充分和噪声处理不当的限制

Method: 利用2D基础模型生成分割面具，通过几何对应关系传播到3D空间，扩充稀疏标注；采用信心度和不确定性基于的一致性正则化，选择可靠伪标签并在3D面具上传播

Result: 完成了从限制的3D标注到强大的2D基础模型的跨模态导入，实现了标注数量的大幅扩充和质量提升

Conclusion: 该方法有效解决了3D弱监督分割中的标注不足问题，通过利用2D基础模型的优势显著提升了3D分割性能

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [77] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: 提出WaveHiT-SR方法，通过将小波变换嵌入分层Transformer框架，使用自适应分层窗口替代静态小窗口，在降低计算复杂度的同时提升超分辨率性能


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的超分辨率方法由于窗口自注意力机制的二次计算复杂度，只能使用小而固定的窗口，限制了感受野范围

Method: 1) 使用自适应分层窗口替代静态小窗口；2) 利用小波变换将图像分解为多频率子带；3) 通过分层处理逐步重建高分辨率图像

Result: 在SwinIR-Light、SwinIR-NG和SRFormer-Light等模型上实现了最先进的超分辨率结果，参数量更少、FLOPs更低、速度更快

Conclusion: WaveHiT-SR方法有效解决了Transformer在超分辨率任务中的计算复杂度问题，同时保持了优异的性能表现

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [78] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是一个专门针对韩语的文本丰富视觉问答基准，填补了低资源语言在VLM评估方面的空白，包含多领域评估和半自动化数据生成流程。


<details>
  <summary>Details</summary>
Motivation: 现有文本丰富VQA基准主要集中在英语等高资源语言，韩语等低资源语言缺乏全面的评估基准，阻碍了稳健模型评估和多语言VLM研究发展。

Method: 开发了半自动化VQA生成流程，采用分步图像分解方法和七指标评估协议来确保数据质量，支持15个领域和26种图像类型的多维度评估。

Result: 构建了KRETA基准数据集，专门针对韩语文本理解与推理能力评估，提供了可扩展的基准生成方法。

Conclusion: KRETA不仅为韩语VLM研究提供了重要基准，其可适配的生成流程也有助于其他语言类似基准的开发，推动多语言VLM研究进展。

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [79] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: 对Chan-Vese图像分割算法的全面研究，提出了基于活动轮廓的功能性分割损失方法，并在PyTorch中实现，与传统损失函数进行性能比较


<details>
  <summary>Details</summary>
Motivation: 深入研究Chan-Vese算法在图像分割中的应用，探索基于活动轮廓的功能性损失函数在现代计算机视觉中的有效性

Method: 采用离散化方案分析Chan-Vese模型的函数能量和偏微分方程，基于水平集函数实现，使用MATLAB和PyTorch进行实现，提出基于活动轮廓的功能性分割损失

Result: 开发了基于Chan-Vese算法的功能性分割损失方法，与常见计算机视觉分割数据集进行比较，评估了传统损失函数与提出方法的性能差异

Conclusion: 基于Chan-Vese算法的功能性分割损失方法在现代计算机视觉分割任务中表现出良好性能，为图像分割提供了新的损失函数设计思路

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [80] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 视觉-语言模型在图片地理定位中存在严重隐私风险，尤其在社交媒体类图片上达到61%准确率


<details>
  <summary>Details</summary>
Motivation: 评估生成式视觉-语言模型的地理定位精度、限制和潜在风险，因为这些模型可能带来监视和追踪等严重隐私风险

Method: 对25个最先进的视觉-语言模型在4个多样环境的标准图片数据集上进行综合评估

Result: 当前VLM在普通街道级图片上表现较差，但在社交媒体类图片上达到61%的高准确率

Conclusion: 研究给出了VLM内部推理的洞察，强调了其强项、限制和潜在社会风险，尤其是在隐私保护方面的紧急问题

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [81] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim是一个无需训练的目标幻觉检测框架，通过结合全局和局部嵌入相似性信号，在多种场景下实现更准确可靠的目标幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的目标幻觉问题限制了其在现实应用中的安全部署。现有方法通常单独采用全局或局部视角，可能限制了检测可靠性。

Method: 提出GLSim框架，利用图像和文本模态之间的互补性全局和局部嵌入相似性信号，无需训练即可进行目标幻觉检测。

Result: 在全面的基准测试中，GLSim实现了优越的检测性能，显著优于竞争基线方法。

Conclusion: GLSim通过结合全局和局部视角，为目标幻觉检测提供了更准确可靠的解决方案，有助于提升视觉语言模型的安全部署。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [82] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS（生成式分割）是一个新颖的框架，将分割任务重新定义为通过标签扩散的生成式任务，直接从噪声生成分割掩码，在语言驱动图像分割任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将语言驱动图像分割视为判别式问题，现有扩散模型方法仍以图像为中心，将分割作为辅助过程。本文旨在将分割本身作为生成式任务来处理。

Method: 提出GS框架，通过标签扩散将分割建模为生成任务：直接从噪声生成分割掩码，同时以输入图像和语言描述为条件，实现端到端训练和显式空间语义控制。

Result: 在Panoptic Narrative Grounding基准测试中，GS显著优于现有的判别式和基于扩散的方法，建立了语言驱动分割的新state-of-the-art。

Conclusion: 将分割重新定义为生成式任务的方法是有效的，GS框架通过直接生成分割掩码实现了更好的性能，为多模态分割任务提供了新的解决方案。

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [83] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: 提出SegAssist方法，通过分割辅助主动标注来解决视觉语言模型在测试时遇到的未知类别和域偏移问题，实现持续增量适应


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中部署的视觉语言模型遇到未知对象和分布偏移时的泛化挑战，特别是测试时持续出现未知类别和未知域的场景

Method: 建立ITTA新基准，结合单图像TTA方法和主动标注技术，提出训练无关的分割辅助主动标注模块SegAssist，利用VLM的分割能力精化主动样本选择

Result: 在多个基准数据集上的广泛实验证明了SegAssist在需要持续适应新兴数据的真实场景中提升VLM性能的潜力

Conclusion: SegAssist方法有效解决了VLM在测试时增量适应的问题，为处理连续出现的未知类别和域偏移提供了实用解决方案

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [84] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D是一个无需人工标注的开词汇多视角室内3D目标检测器，通过2D诱导体素特征和CLIP特征对齐实现高效检测


<details>
  <summary>Details</summary>
Motivation: 开词汇3D目标检测领域主要基于3D点云方法，基于图像的方法探索有限，需要开发无需人工标注的高效检测方案

Method: 采用单阶段检测器架构，结合2D诱导体素特征、类无关3D定位损失和体素语义对齐损失，使用图嵌入技术生成高质量3D伪框

Result: 在ScanNet200和ARKitScenes基准测试中实现了更高的准确率和速度（每场景0.3秒），优于现有的两阶段方法

Conclusion: OpenM3D证明了无需人工标注的高效开词汇3D目标检测的可行性，在准确率和速度方面均优于现有方法

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [85] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 该研究参与MARIO挑战赛，开发了基于CNN融合网络和模型集成的分类方法，以及使用补丁进展掩码自编码器预测AMD进展，在两个任务中均进入前十名。


<details>
  <summary>Details</summary>
Motivation: 年龄相关性黄斑变性(AMD)是影响视力的常见眼病，抗VEGF治疗可减缓新生血管性AMD进展，通过及时诊断和持续监测可获得更好疗效。追踪渗出性AMD患者OCT扫描中的新生血管活动进展，有助于制定更个性化和有效的治疗计划。

Method: 任务1：使用融合CNN网络和模型集成对连续OCT采集的2D切片对进行分类；任务2：提出补丁进展掩码自编码器，生成下一次检查的OCT图像，然后使用任务1的解决方案对当前OCT和生成的OCT进行分类。

Result: 在两个任务中均取得了前十名的成绩，但由于部分团队成员与挑战赛组织者属于同一机构，不符合获奖资格。

Conclusion: 提出的方法在AMD进展监测方面表现出色，融合CNN和掩码自编码器的组合为OCT图像分析和AMD进展预测提供了有效解决方案。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [86] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 本文提出了PAUL框架来解决跨视角地理定位中的噪声对应问题，通过不确定性学习和选择性增强来处理GPS漂移导致的图像对未对齐问题


<details>
  <summary>Details</summary>
Motivation: 现有跨视角地理定位方法假设训练图像对完美对齐，但现实中GPS漂移等因素导致系统性的对齐偏移，只有部分对应关系存在，这种噪声对应问题在实际应用中普遍但研究较少

Method: 提出PAUL框架，通过不确定性感知协同增强和证据协同训练，基于估计的数据不确定性对训练数据进行分区和增强，选择性增强高置信度区域并利用不确定性估计来精化特征学习

Result: 综合实验验证了PAUL各组成部分的有效性，在各种噪声比例下 consistently 优于其他竞争性噪声对应驱动方法

Conclusion: PAUL框架有效解决了跨视角地理定位中的噪声对应问题，为实际应用中的GPS漂移等现实挑战提供了鲁棒的解决方案

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个统一的文本到音频生成框架，通过整合大语言模型和TTA系统来生成长篇叙事音频，解决了现有方法在时序连贯性和组合推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到音频生成方法在合成短音频片段方面表现出色，但在生成长篇叙事音频时存在困难，需要时序连贯性和组合推理能力。

Method: 使用大语言模型将复杂叙事查询分解为时序有序的子任务，采用解耦的桥接机制（语义对齐桥接查询和连贯性保持残差查询），并通过端到端训练统一指令理解和音频生成。

Result: 在AudioStory-10K基准测试中，AudioStory在单音频生成和叙事音频生成方面均优于现有基线方法，在指令跟随能力和音频保真度方面表现优异。

Conclusion: AudioStory通过LLM与TTA系统的有效整合，成功解决了长叙事音频生成的挑战，为音频叙事生成提供了新的解决方案。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 提出轻量级分类方法，结合专家标注数据和BioCLIP2知识蒸馏，在丹麦蛾类识别中实现高精度和低计算成本


<details>
  <summary>Details</summary>
Motivation: 解决自动化相机系统拍摄的蛾类图像识别难题，克服策展图像与野外图像之间的域偏移问题，为昆虫监测提供实用解决方案

Method: 使用有限的专家标注野外数据，通过知识蒸馏将高性能BioCLIP2基础模型压缩到ConvNeXt-tiny架构中

Result: 在101种丹麦蛾类的AMI相机系统实验中，BioCLIP2显著优于其他方法，蒸馏后的轻量模型达到相当精度且计算成本大幅降低

Conclusion: 为高效昆虫监测系统开发提供实用指导，弥合细粒度分类的域差距，推动昆虫衰退研究

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一个可训练的组合框架，通过两阶段训练流程整合通用规划器和专业执行器，在科学计算GUI任务中实现了卓越的执行精度和跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决GUI自主代理在科学计算领域面临的长期规划与精确执行之间的权衡问题，现有组合框架通常是静态不可训练的，无法从经验中适应

Method: 两阶段训练流程：1)专业化阶段-使用解耦GRPO方法为每个科学应用单独训练专家规划器；2)泛化阶段-聚合成功轨迹进行监督微调

Result: 在ScienceBenchmark的四个挑战性应用中显著超越基线方法，在开源模型中建立了新的最先进水平

Conclusion: CODA框架成功解决了科学计算GUI任务中的规划-执行权衡问题，通过可训练的组合架构实现了强大的执行能力和跨域泛化

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [90] [A goal-driven ruin and recreate heuristic for the 2D variable-sized bin packing problem with guillotine constraints](https://arxiv.org/abs/2508.19306)
*Jeroen Gardeyn,Tony Wauters*

Main category: cs.CG

TL;DR: 提出基于破坏重建范式和目标驱动方法的新启发式算法，解决带断头台约束的二维装箱问题及其变体，在基准测试中优于现有最优算法


<details>
  <summary>Details</summary>
Motivation: 解决二维装箱问题中需要断头台切割约束的实际情况，包括允许物品90度旋转和异质箱子的变体问题，目标是最大化利用箱体面积

Method: 采用破坏重建范式结合目标驱动方法的新启发式算法，通过系统性地破坏当前解并重新构建来寻找更优解

Result: 在文献基准测试实例中，所提算法在所有考虑的问题变体上都优于当前最先进算法，解决方案质量更优

Conclusion: 基于破坏重建和目标驱动的新启发式方法能有效解决带断头台约束的二维装箱问题，为实际工业切割应用提供了高质量的解决方案

Abstract: This paper addresses the two-dimensional bin packing problem with guillotine
constraints. The problem requires a set of rectangular items to be cut from
larger rectangles, known as bins, while only making use of edge-to-edge
(guillotine) cuts. The goal is to minimize the total bin area needed to cut all
required items. This paper also addresses variants of the problem which permit
90{\deg} rotation of items and/or a heterogeneous set of bins. A novel
heuristic is introduced which is based on the ruin and recreate paradigm
combined with a goal-driven approach. When applying the proposed heuristic to
benchmark instances from the literature, it outperforms the current
state-of-the-art algorithms in terms of solution quality for all variants of
the problem considered.

</details>


### [91] [A Walk on the Wild Side: a Shape-First Methodology for Orthogonal Drawings](https://arxiv.org/abs/2508.19416)
*Giordano Andreola,Susanna Caroppo,Giuseppe Di Battista,Fabrizio Grosso,Maurizio Patrignani,Allegra Strippoli*

Main category: cs.CG

TL;DR: 这篇论文提出了一种新的正交绘图方法DOMUS，通过重点优化折点而非交叉点来提高绘图质量，在多项指标上超过传统TSM方法。


<details>
  <summary>Details</summary>
Motivation: 传统正交绘图算法过度关注交叉点最小化，导致边段过长、面积过大和几何不均匀。而交叉点对可读性影响有限，因此需要重新考虑优化目标。

Method: 提出一种翻转传统TSM流水线的方法：首先尝试构造无折点的直线绘图，如果不可行则逐步分割边添加假顶点（对应未来折点）。通过SAT编码解决直线可绘性问题，利用SAT求解器的证明来指导边分割。

Result: 实现了DOMUS系统，在中小型图形上进行广泛实验。结果显示在大多数标准绘图指标上，DOMUS都一贯地超过了OGDF的TSM基础方法。

Conclusion: 通过重新设计优化目标从交叉点最小化转向折点最小化，能够生成更高质量的正交绘图，为图形可视化领域提供了新的思路。

Abstract: Several algorithms for the construction of orthogonal drawings of graphs,
including those based on the Topology-Shape-Metrics (TSM) paradigm, tend to
prioritize the minimization of crossings. This emphasis has two notable side
effects: some edges are drawn with unnecessarily long sequences of segments and
bends, and the overall drawing area may become excessively large. As a result,
the produced drawings often lack geometric uniformity. Moreover, orthogonal
crossings are known to have a limited impact on readability, suggesting that
crossing minimization may not always be the optimal goal. In this paper, we
introduce a methodology that 'subverts' the traditional TSM pipeline by
focusing on minimizing bends. Given a graph $G$, we ideally seek to construct a
rectilinear drawing of $G$, that is, an orthogonal drawing with no bends. When
not possible, we incrementally subdivide the edges of $G$ by introducing dummy
vertices that will (possibly) correspond to bends in the final drawing. This
process continues until a rectilinear drawing of a subdivision of the graph is
found, after which the final coordinates are computed. We tackle the
(NP-complete) rectilinear drawability problem by encoding it as a SAT formula
and solving it with state-of-the-art SAT solvers. If the SAT formula is
unsatisfiable, we use the solver's proof to determine which edge to subdivide.
Our implementation, DOMUS, which is fairly simple, is evaluated through
extensive experiments on small- to medium-sized graphs. The results show that
it consistently outperforms OGDF's TSM-based approach across most standard
graph drawing metrics.

</details>


### [92] [Approximating mixed volumes to arbitrary accuracy](https://arxiv.org/abs/2508.19582)
*Hariharan Narayanan,Sourav Roy*

Main category: cs.CG

TL;DR: 这篇论文提出了一种随机化算法，用于在多项式时间内估计凸多面体的混合体积，达到了1±ε的乘性误差精度。


<details>
  <summary>Details</summary>
Motivation: 混合体积计算在计算数学和优化领域具有重要意义，但现有算法在高维度和任意次数情况下计算复杂度过高。需要开发更高效的近似算法。

Method: 结合了凸优化、Lorentzian多项式理论和多面体分割技术，设计了一种随机化算法，通过优化过程估计混合体积。

Result: 算法能够在多项式时间内产生1±ε乘性误差的估计，且成功概率大于1-δ。当k为常数时，该算法是首个能处理任意α_i次数的随机多项式时间算法。

Conclusion: 这项研究为高维凸多面体混合体积计算提供了第一个随机多项式时间算法，在理论和应用上都具有重要价值。

Abstract: We study the problem of approximating the mixed volume $V(P_1^{(\alpha_1)},
\dots, P_k^{(\alpha_k)})$ of an $k$-tuple of convex polytopes $(P_1, \dots,
P_k)$, each of which is defined as the convex hull of at most $m_0$ points in
$\mathbb{Z}^n$. We design an algorithm that produces an estimate that is within
a multiplicative $1 \pm \epsilon$ factor of the true mixed volume with a
probability greater than $1 - \delta.$ Let the constant $ \prod_{i=2}^{k}
\frac{(\alpha_{i}+1)^{\alpha_{i}+1}}{\alpha_{i}^{\,\alpha_{i}}}$ be denoted by
$\tilde{A}$. When each $P_i \subseteq B_\infty(2^L)$, we show in this paper
that the time complexity of the algorithm is bounded above by a polynomial in
$n, m_0, L, \tilde{A}, \epsilon^{-1}$ and $\log \delta^{-1}$. In fact, a
stronger result is proved in this paper, with slightly more involved
terminology.
  In particular, we provide the first randomized polynomial time algorithm for
computing mixed volumes of such polytopes when $k$ is an absolute constant, but
$\alpha_1, \dots, \alpha_k$ are arbitrary. Our approach synthesizes tools from
convex optimization, the theory of Lorentzian polynomials, and polytope
subdivision.

</details>


### [93] [Simpler is Faster: Practical Distance Reporting by Sorting Along a Space-Filling Curve](https://arxiv.org/abs/2508.19891)
*Sarita de Berg,Ivor van der Hoog,Eva Rotenberg,Emil Toftegaard Gæde*

Main category: cs.CG

TL;DR: 这篇论文重新评估了使用空间填充曲线的简单距离查询方法，实验结果显示该方法在静态和动态场景下都具有竞争力，说明空间填充曲线本身是性能的关键因素


<details>
  <summary>Details</summary>
Motivation: 重新评估空间填充曲线在距离查询中的效果，对比现代复杂的数据结构，探索简单方法的实际性能

Method: 将输入点集沿空间填充曲线排序，距离查询简化为扫描四个连续的曲线范围，并与现代数据结构进行实验对比

Result: 在静态场景下，该简单方法与优化最好的范围报告数据结构竞争；在动态场景下，该方法甚至更优

Conclusion: 空间填充曲线本身，而非周围的复杂机制，所起到了性能的关键作用，这为数据结构设计提供了新的视角

Abstract: Range reporting is a classical problem in computational geometry. A
(rectangular) reporting data structure stores a point set $P$ of $n$ points,
such that, given a (rectangular) query region $\Delta$, it returns all points
in $P \cap \Delta$. A variety of data structures support such queries with
differing asymptotic guarantees such as $k$-d trees, range trees, $R$-trees,
and quadtrees. A common variant of range queries are distance reporting
queries, where the input is a query point $q$ and a radius $\delta$, and the
goal is to report all points in $P$ within distance $\delta$ of $q$. Such
queries frequently arise as subroutines in geometric data structure
construction and in Fr\'echet distance computations. Modern implementations
typically reduce distance queries to rectangular range queries using the data
structures listed above.
  We revisit a simple and practical heuristic for distance reporting. The
approach is straightforward: sort the input point set $P$ along a space-filling
curve. Queries then reduce to scanning at most four contiguous ranges along the
sorted curve. We show extensive experimental evaluation of modern distance and
range reporting data structures. In a static scenario, we show that this simple
technique is competitive with all but the most highly optimised range reporting
data structures. Notably, these involved structures use space-filling curves
themselves to speed up computation. In a dynamic setting, our simpler method
even becomes the preferred technique.
  This leads to a perhaps unexpected insight: while modern data structures
invest heavily in leveraging space-filling curves for optimising their layout
and traversal, it is the curve itself, rather than the surrounding machinery,
that delivers much of the performance.

</details>


### [94] [Internally-Convex Drawings of Outerplanar Graphs in Small Area](https://arxiv.org/abs/2508.19913)
*Michael A. Bekos,Giordano Da Lozzo,Fabrizio Frati,Giuseppe Liotta,Antonios Symvonis*

Main category: cs.CG

TL;DR: 本文改进了外平面图的网格直线绘制算法，将面积从O(n²)优化到O(n¹.⁵)，并对特定类别的外平面图提出了严格凸多边形绘制的算法。


<details>
  <summary>Details</summary>
Motivation: Kant在1996年提出的算法虽然能够为外平面图生成保持嵌入的平面直线网格绘制，但需要O(n²)的面积。本文旨在减少绘制面积，并探索在要求内部面为严格凸多边形情况下的绘制方法。

Method: 提出了新的绘制算法，针对一般外平面图优化面积至O(n¹.⁵)。对于弱对偶为路径的外平面图，设计了专门的算法来生成严格凸多边形内部面，面积复杂度为Θ(nk²)，其中k是最大内部面循环大小。

Result: 成功将外平面图的网格直线绘制面积从二次复杂度降低到1.5次复杂度。对于特定类别的外平面图，实现了严格凸多边形内部面的绘制，面积与最大面大小相关。

Conclusion: 本文在外平面图绘制领域取得了面积复杂度的显著改进，为保持嵌入的平面直线绘制提供了更高效的解决方案，同时扩展了对严格凸多边形绘制的研究。

Abstract: A well-known result by Kant [Algorithmica, 1996] implies that n-vertex
outerplane graphs admit embedding-preserving planar straight-line grid drawings
where the internal faces are convex polygons in $O(n^2)$ area. In this paper,
we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also
consider outerplanar drawings in which the internal faces are required to be
strictly-convex polygons. In this setting, we consider outerplanar graphs whose
weak dual is a path and give a drawing algorithm that achieves $\Theta(nk^2)$
area, where $k$ is the maximum size of an internal facial cycle.

</details>


### [95] [Visualizing Treewidth](https://arxiv.org/abs/2508.19935)
*Alvin Chiu,Thomas Depian,David Eppstein,Michael T. Goodrich,Martin Nöllenburg*

Main category: cs.CG

TL;DR: 本文研究并实现了展示图的有界路径宽度和树宽度的见证绘图方法，通过绘制树分解或路径分解作为包树结构，并在每个包中显示诱导子图，使用轨道连接顶点在多个包中的副本。


<details>
  <summary>Details</summary>
Motivation: 为了清晰地可视化展示图的有界路径宽度和树宽度属性，帮助理解图的分解结构和宽度特性。

Method: 实现可视化原型，使用动态规划优化小宽度图的交叉最小化，启发式方法处理大宽度图。引入分类法：每个包的子图呈现为单页/双页弧图或圆形布局，轨道使用直线或轨道径向路径渲染。

Result: 开发了多种绘图范式，能够有效展示图的路径宽度和树宽度特性，通过优化的顶点布局减少边和轨道的交叉。

Conclusion: 提出的见证绘图方法为可视化图的有界宽度属性提供了有效的解决方案，不同的绘图风格适应不同的可视化需求。

Abstract: A witness drawing of a graph is a visualization that clearly shows a given
property of a graph. We study and implement various drawing paradigms for
witness drawings to clearly show that graphs have bounded pathwidth or
treewidth. Our approach draws the tree decomposition or path decomposition as a
tree of bags, with induced subgraphs shown in each bag, and with ''tracks'' for
each graph vertex connecting its copies in multiple bags. Within bags, we
optimize the vertex layout to avoid crossings of edges and tracks. We implement
a visualization prototype for crossing minimization using dynamic programming
for graphs of small width and heuristic approaches for graphs of larger width.
We introduce a taxonomy of drawing styles, which render the subgraph for each
bag as an arc diagram with one or two pages or as a circular layout with
straight-line edges, and we render tracks either with straight lines or with
orbital-radial paths.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: 提出了PARCC框架，基于区域连接演算的形式化逻辑系统，用于描述物体间的空间关系，并通过演示学习算法来推断人类可接受的物体配置规则。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作能力在拾取放置任务中有所提升，但理解人类可接受的物体配置方法在表达空间关系方面仍有局限，需要更好的框架来捕捉人类的空间排列规则。

Method: 基于区域连接演算(RCC)开发了位置增强的PARCC形式化逻辑框架，并设计了通过演示学习的推理算法来学习物体配置规范。

Result: 人类研究结果表明，该框架能够有效捕捉人类的意图规范，且演示学习方法优于人类直接提供的规范说明。

Conclusion: PARCC框架为机器人理解人类物体排列规则提供了有效的形式化表示和学习方法，演示学习方式能够更好地获取人类的空间关系偏好。

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [97] [FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain](https://arxiv.org/abs/2508.19380)
*Diancheng Li,Nia Ralston,Bastiaan Hagen,Phoebe Tan,Matthew A. Robertson*

Main category: cs.RO

TL;DR: FlipWalker是一种受雅各布天梯玩具启发的欠驱动机器人系统，通过翻转运动在复杂地形中移动，比轮式机器人更具优势


<details>
  <summary>Details</summary>
Motivation: 传统轮式机器人在不规则户外地形中表现不佳，需要一种新的运动策略来应对草地、岩石和雪地等挑战性环境

Method: 采用两段式互联结构，通过柔性电缆连接，电机驱动腿部推动地面或相对段体，基于物理模型分析翻转动力学和关键设计参数

Result: 原型机重0.78kg，最大翻转速度达0.2体长/秒，在人工草地、河石和雪地等不同地形上成功验证了运动性能

Conclusion: FlipWalker的翻转策略利用垂直于表面的地面反作用力，为不规则户外地形的导航提供了一种有前景的传统运动替代方案

Abstract: This paper introduces FlipWalker, a novel underactuated robot locomotion
system inspired by Jacob's Ladder illusion toy, designed to traverse
challenging terrains where wheeled robots often struggle. Like the Jacob's
Ladder toy, FlipWalker features two interconnected segments joined by flexible
cables, enabling it to pivot and flip around singularities in a manner
reminiscent of the toy's cascading motion. Actuation is provided by
motor-driven legs within each segment that push off either the ground or the
opposing segment, depending on the robot's current configuration. A
physics-based model of the underactuated flipping dynamics is formulated to
elucidate the critical design parameters governing forward motion and obstacle
clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a
maximum flipping speed of 0.2 body lengths per second. Experimental trials on
artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping
strategy, which relies on ground reaction forces applied normal to the surface,
offers a promising alternative to traditional locomotion for navigating
irregular outdoor terrain.

</details>


### [98] [LaVA-Man: Learning Visual Action Representations for Robot Manipulation](https://arxiv.org/abs/2508.19391)
*Chaoran Zhu,Hengyi Wang,Yik Lung Pang,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种通过自监督预训练任务学习视觉-文本关联的方法，通过重构被遮挡的目标图像来学习视觉-动作表示，无需机器人动作监督，然后在少量演示样本上微调用于操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有的两阶段方法（先编码视觉观察和文本指令的相似度，再映射到机器人动作）限制了模型捕捉视觉观察与文本指令之间关系的能力，导致操作任务精度降低。

Method: 使用自监督预训练任务：基于输入图像和文本指令重构被遮挡的目标图像，学习视觉-文本关联。然后使用少量演示样本对学习到的表示进行微调，用于操作任务。

Result: 在五个基准测试（包括仿真和真实机器人验证）上，该方法优于现有技术。还引入了包含180个物体类别和3,200个实例的Omni-Object Pick-and-Place数据集。

Conclusion: 通过自监督预训练学习视觉-文本关联的方法能够有效提升语言引导机器人操作的性能，在少样本学习场景下表现出色，具有很好的泛化能力。

Abstract: Visual-textual understanding is essential for language-guided robot
manipulation. Recent works leverage pre-trained vision-language models to
measure the similarity between encoded visual observations and textual
instructions, and then train a model to map this similarity to robot actions.
However, this two-step approach limits the model to capture the relationship
between visual observations and textual instructions, leading to reduced
precision in manipulation tasks. We propose to learn visual-textual
associations through a self-supervised pretext task: reconstructing a masked
goal image conditioned on an input image and textual instructions. This
formulation allows the model to learn visual-action representations without
robot action supervision. The learned representations can then be fine-tuned
for manipulation tasks with only a few demonstrations. We also introduce the
\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot
tabletop manipulation episodes, including 180 object classes and 3,200
instances with corresponding textual instructions. This dataset enables the
model to acquire diverse object priors and allows for a more comprehensive
evaluation of its generalisation capability across object instances.
Experimental results on the five benchmarks, including both simulated and
real-robot validations, demonstrate that our method outperforms prior art.

</details>


### [99] [From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation](https://arxiv.org/abs/2508.19425)
*John M. Scanlon,Timothy L McMurry,Yin-Hsiu Chen,Kristofer D. Kusano,Trent Victor*

Main category: cs.RO

TL;DR: 本文创建了美国基地自动驾驶系统(ADS)的连通道减速率基准，特别关注高速公路的减速风险评估，揭示了减速率的地理差异性和不同严重程度下减速类型分布的异质性。


<details>
  <summary>Details</summary>
Motivation: 扩展以前仅聚焦补充道路的基准，包含高速公路减速风险评估，以提供更全面的ADS安全性能评估基准。

Method: 利用公开的警察报告减速数据和车辆行驶里程(VMT)数据，进行在运乘客车隔离、道路类型分类和减速类型学分析。

Result: 发现高速公路减速率存在显著地理差异，亚特兰大的伤害减速率(2.4 IPMM)比凤凰城(0.7 IPMM)高几乎3.5倍。不同严重程度减速的类型分布异质，高严重减速中单车、易受伤害行人和对向碰撞占比更高。

Conclusion: 需要基于具体位置的基准来避免偏差的安全性能评估，低严重性能表现不能预测高严重结果。本文首次创建了高速公路特定基准，为未来ADS评测提供了基础框架。

Abstract: This paper presents crash rate benchmarks for evaluating US-based Automated
Driving Systems (ADS) for multiple urban areas. The purpose of this study was
to extend prior benchmarks focused only on surface streets to additionally
capture freeway crash risk for future ADS safety performance assessments. Using
publicly available police-reported crash and vehicle miles traveled (VMT) data,
the methodology details the isolation of in-transport passenger vehicles, road
type classification, and crash typology. Key findings revealed that freeway
crash rates exhibit large geographic dependence variations with
any-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4
IPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results
show the critical need for location-specific benchmarks to avoid biased safety
evaluations and provide insights into the vehicle miles traveled (VMT) required
to achieve statistical significance for various safety impact levels. The
distribution of crash types depended on the outcome severity level. Higher
severity outcomes (e.g., fatal crashes) had a larger proportion of
single-vehicle, vulnerable road users (VRU), and opposite-direction collisions
compared to lower severity (police-reported) crashes. Given heterogeneity in
crash types by severity, performance in low-severity scenarios may not be
predictive of high-severity outcomes. These benchmarks are additionally used to
quantify at the required mileage to show statistically significant deviations
from human performance. This is the first paper to generate freeway-specific
benchmarks for ADS evaluation and provides a foundational framework for future
ADS benchmarking by evaluators and developers.

</details>


### [100] [An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals](https://arxiv.org/abs/2508.19429)
*Gustavo A. Cardona,Kaier Liang,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 提出了一种迭代方法，用于在资源分布未知的环境中规划异构多智能体路径，通过动态平衡探索和任务执行来处理资源不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队在资源分布未知环境中执行任务时的规划挑战，特别是处理资源初始分布和数量的不确定性。

Method: 采用迭代算法，结合Capability Temporal Logic (CaTL)形式化框架，动态平衡环境探索和任务完成，机器人通过探索识别资源位置和数量，同时基于当前信息最大化满足任务目标。

Result: 通过模拟案例研究证明了方法的有效性和性能，能够在动态、资源受限的环境中提供鲁棒的规划解决方案。

Conclusion: 该方法能够有效协调异构团队在不确定性条件下的工作，为动态资源约束环境中的多智能体规划提供了实用解决方案。

Abstract: This paper presents an iterative approach for heterogeneous multi-agent route
planning in environments with unknown resource distributions. We focus on a
team of robots with diverse capabilities tasked with executing missions
specified using Capability Temporal Logic (CaTL), a formal framework built on
Signal Temporal Logic to handle spatial, temporal, capability, and resource
constraints. The key challenge arises from the uncertainty in the initial
distribution and quantity of resources in the environment. To address this, we
introduce an iterative algorithm that dynamically balances exploration and task
fulfillment. Robots are guided to explore the environment, identifying resource
locations and quantities while progressively refining their understanding of
the resource landscape. At the same time, they aim to maximally satisfy the
mission objectives based on the current information, adapting their strategies
as new data is uncovered. This approach provides a robust solution for planning
in dynamic, resource-constrained environments, enabling efficient coordination
of heterogeneous teams even under conditions of uncertainty. Our method's
effectiveness and performance are demonstrated through simulated case studies.

</details>


### [101] [Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning](https://arxiv.org/abs/2508.19476)
*Dane Brouwer,Joshua Citron,Heather Nolte,Jeannette Bohg,Mark Cutkosky*

Main category: cs.RO

TL;DR: 论文研究机器人如何利用非抓取式触觉传感从密集物体集合中安全提取物体，通过模仿学习训练策略，发现力传感能显著提高成功率


<details>
  <summary>Details</summary>
Motivation: 日常生活中密集可移动物体集合很常见，人类能轻松安全地从中提取物体，但这对机器人来说很困难。研究旨在探索非抓取式触觉传感在机器人操作中的作用

Method: 使用模仿学习训练策略，在随机生成的场景上进行演示训练，然后对力和触觉信息进行消融研究。评估了5种传感模态：手眼视觉、本体感觉、三轴触觉传感、关节力矩估计的接触力矩、真空吸盘成功获取的监测

Result: 使用任何力传感的策略都表现出更少的过度力失败、更高的整体成功率和更快的完成时间。同时使用触觉和力矩信息时性能最佳，比无力信息基线提高了80%

Conclusion: 非抓取式触觉传感对于机器人在受限杂乱环境中安全提取物体至关重要，力传感信息能显著提升机器人操作的性能和安全性

Abstract: Dense collections of movable objects are common in everyday spaces -- from
cabinets in a home to shelves in a warehouse. Safely retracting objects from
such collections is difficult for robots, yet people do it easily, using
non-prehensile tactile sensing on the sides and backs of their hands and arms.
We investigate the role of such sensing for training robots to gently reach
into constrained clutter and extract objects. The available sensing modalities
are (1) "eye-in-hand" vision, (2) proprioception, (3) non-prehensile triaxial
tactile sensing, (4) contact wrenches estimated from joint torques, and (5) a
measure of successful object acquisition obtained by monitoring the vacuum line
of a suction cup. We use imitation learning to train policies from a set of
demonstrations on randomly generated scenes, then conduct an ablation study of
wrench and tactile information. We evaluate each policy's performance across 40
unseen environment configurations. Policies employing any force sensing show
fewer excessive force failures, an increased overall success rate, and faster
completion times. The best performance is achieved using both tactile and
wrench information, producing an 80% improvement above the baseline without
force information.

</details>


### [102] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: DATR框架通过两阶段方法从稀疏视角重建苹果树3D模型，结合基础模型和扩散模型，在田间条件下实现高精度重建，比工业级激光扫描仪效率提升360倍


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法在田间稀疏和遮挡视角下表现不佳，限制了农业数字孪生系统的实时监测和机器人仿真应用

Method: 两阶段框架：第一阶段使用机载传感器和基础模型半自动生成树掩码；第二阶段结合扩散模型和大重建模型进行单图像到3D重建，使用Real2Sim数据生成器训练

Result: 在田间和合成数据集上均优于现有方法，域特征估计与工业级激光扫描仪相当，吞吐量提升约360倍

Conclusion: DATR框架展示了可扩展农业数字孪生系统的强大潜力，能够有效处理田间复杂条件下的3D重建问题

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [103] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 这篇论文提出了一种轻量级的实时宏观流派预测模型，专门用于人群运动预测，在保持准确性的同时大幅提升计算效率，使得机器人能够在密集环境中安全高效导航。


<details>
  <summary>Details</summary>
Motivation: 传统微观模型在密集人群中计算成本过高，而现有宏观模型要么过于简单要么计算复杂度太高，无法满足机器人实时导航的需求。

Method: 基于行人流动的内在特征，简化空间和时间处理流程，设计了一种轻量级宏观预测模型，在不使用复杂结构的情况下实现了稳健的泛化能力。

Result: 该模型将推理时间减少3.6倍，同时将预测准确性提高3.1%，集成到社交意识规划框架后，能够在动态环境中实现高效且符合社会规范的机器人导航。

Conclusion: 这项工作证明了通过高效的人群建模方法，机器人可以在不需要高成本计算的情况下完成密集环境中的导航任务。

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [104] [Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks](https://arxiv.org/abs/2508.19607)
*Amin Berjaoui Tahmaz,Ravi Prakash,Jens Kober*

Main category: cs.RO

TL;DR: 提出了一种阻抗原语增强的分层强化学习框架，用于序列接触任务中的高效机器人操作，通过可变刚度控制和原语组合提升学习效率和成功率


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人操作中接触任务的复杂性和适应性需求，需要开发能够动态调整刚度并高效组合行为原语的智能控制框架

Method: 采用分层强化学习结构，包含三个关键组件：支持可变刚度控制的动作空间、自适应刚度控制器（动态调整原语执行时的刚度）、以及促进合规性和高效探索的affordance耦合机制

Result: 在方块举升、开门、物体推动和表面清洁等任务中，相比现有技术表现出更高的学习效率、更好的原语组合性和更高的成功率，同时验证了良好的sim2real迁移能力

Conclusion: 该框架为开发更自适应和多功能的机器人操作系统奠定了基础，在复杂接触任务中具有广阔的应用前景

Abstract: This paper presents an Impedance Primitive-augmented hierarchical
reinforcement learning framework for efficient robotic manipulation in
sequential contact tasks. We leverage this hierarchical structure to
sequentially execute behavior primitives with variable stiffness control
capabilities for contact tasks. Our proposed approach relies on three key
components: an action space enabling variable stiffness control, an adaptive
stiffness controller for dynamic stiffness adjustments during primitive
execution, and affordance coupling for efficient exploration while encouraging
compliance. Through comprehensive training and evaluation, our framework learns
efficient stiffness control capabilities and demonstrates improvements in
learning efficiency, compositionality in primitive selection, and success rates
compared to the state-of-the-art. The training environments include block
lifting, door opening, object pushing, and surface cleaning. Real world
evaluations further confirm the framework's sim2real capability. This work lays
the foundation for more adaptive and versatile robotic manipulation systems,
with potential applications in more complex contact-based tasks.

</details>


### [105] [Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning](https://arxiv.org/abs/2508.19608)
*Dongjae Lee,Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 基于多旋翼的空中操作器只能在小角度操作，本文提出了一种全向空中操作器（OAM）的几何稳定控制和全身运动规划框架，能够在任意6D姿态下稳定悬停并执行精细操作任务。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼基础的空中操作器因为基底的不充分驱动性，只能在小滚转和俯仰角度下进行操作，限制了操作工作空间。如果能够在任意方位悬停，可以显著扩大操作范围并实现原本不可行的操作任务。

Method: 首先提出了一种浮动基底的几何稳定控制器，能够应对机械手臂运动和交互力的影响。然后设计了一个两步优化的全身运动规划器，统筹考虑浮动基底的姿态和机械手关节角度，以利用整个配置空间。这种两步方法有助于实时应用和改善非凸非欧几里得搜索空间的收敛性。

Result: 提出的框架使得基底能够在任意6D姿态下稳定悬停，同时自主执行复杂的操作任务而不会发生碰撞。通过实验验证，OAM能够在多种场景下执行捕捉和拉勘物体的任务，包括在近90度和甚至180度俯仰角度附近的极端情况。

Conclusion: 该研究成功开发了一种能够在任意方位悬停并执行精细操作任务的全向空中操作器控制和规划框架，显著扩大了空中操作的应用范围和能力。

Abstract: Aerial manipulators based on conventional multirotors can conduct
manipulation only in small roll and pitch angles due to the underactuatedness
of the multirotor base. If the multirotor base is capable of hovering at
arbitrary orientation, the robot can freely locate itself at any point in
$\mathsf{SE}(3)$, significantly extending its manipulation workspace and
enabling a manipulation task that was originally not viable. In this work, we
present a geometric robust control and whole-body motion planning framework for
an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,
we first propose a geometric robust controller for a floating base. Since the
motion of the robotic arm and the interaction forces during manipulation affect
the stability of the floating base, the base should be capable of mitigating
these adverse effects while controlling its 6D pose. We then design a two-step
optimization-based whole-body motion planner, jointly considering the pose of
the floating base and the joint angles of the robotic arm to harness the entire
configuration space. The devised two-step approach facilitates real-time
applicability and enhances convergence of the optimization problem with
non-convex and non-Euclidean search space. The proposed approach enables the
base to be stationary at any 6D pose while autonomously carrying out
sophisticated manipulation near obstacles without any collision. We demonstrate
the effectiveness of the proposed framework through experiments in which an OAM
performs grasping and pulling of an object in multiple scenarios, including
near $90^\circ$ and even $180^\circ$ pitch angles.

</details>


### [106] [Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control](https://arxiv.org/abs/2508.19684)
*Ghadeer Elmkaiel,Syn Schmitt,Michael Muehlebach*

Main category: cs.RO

TL;DR: Floaty是一种形状可变机器人，通过被动翱翔利用风能，实现高效悬停和机动，能耗比推进器系统低一个数量级。


<details>
  <summary>Details</summary>
Motivation: 解决空中机器人同时实现敏捷机动性和高能效的挑战，特别是在动态风环境中。传统推进器系统能耗高，固定翼设计缺乏悬停能力。

Method: 采用仿鸟类的智能形态控制，通过实验学习的气动模型设计控制策略，实现被动稳定性和精确姿态位置控制，无需主动推进。

Result: 风洞实验显示Floaty能在10m/s垂直气流中悬停、机动和抗干扰，比功耗仅为10W/kg，比推进器系统低一个数量级。

Conclusion: Floaty引入了能效空中机器人的新范式，利用形态智能和控制技术在挑战性风条件下可持续运行。

Abstract: Achieving both agile maneuverability and high energy efficiency in aerial
robots, particularly in dynamic wind environments, remains challenging.
Conventional thruster-powered systems offer agility but suffer from high energy
consumption, while fixed-wing designs are efficient but lack hovering and
maneuvering capabilities. We present Floaty, a shape-changing robot that
overcomes these limitations by passively soaring, harnessing wind energy
through intelligent morphological control inspired by birds. Floaty's design is
optimized for passive stability, and its control policy is derived from an
experimentally learned aerodynamic model, enabling precise attitude and
position control without active propulsion. Wind tunnel experiments demonstrate
Floaty's ability to hover, maneuver, and reject disturbances in vertical
airflows up to 10 m/s. Crucially, Floaty achieves this with a specific power
consumption of 10 W/kg, an order of magnitude lower than thruster-powered
systems. This introduces a paradigm for energy-efficient aerial robotics,
leveraging morphological intelligence and control to operate sustainably in
challenging wind conditions.

</details>


### [107] [Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](https://arxiv.org/abs/2508.19731)
*Maryam Kazemi Eskeri,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: 提出了一种基于动态地图(MoDs)的多机器人任务分配方法，通过考虑人类移动模式来优化任务执行时间，相比传统方法最多减少26%的任务完成时间


<details>
  <summary>Details</summary>
Motivation: 现有MRTA方法大多忽略人类动态运动模式，依赖静态地图，导致在多机器人协作环境中效率低下和延迟增加

Method: 利用动态地图(MoDs)这一时空可查询模型来捕捉历史人类移动模式，构建包含MoDs的随机成本函数来估计人类对任务执行时间的影响

Result: 实验结果显示，集成MoDs的方法相比动态无关方法最多减少26%的任务完成时间，相比基线方法最多减少19%

Conclusion: 在共享环境中考虑人类动态对MRTA至关重要，该方法为在人类密集环境中部署多机器人系统提供了高效框架

Abstract: Multi-robot systems are increasingly deployed in applications, such as
intralogistics or autonomous delivery, where multiple robots collaborate to
complete tasks efficiently. One of the key factors enabling their efficient
cooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this
problem optimize task distribution among robots to minimize the overall
execution time. In shared environments, apart from the relative distance
between the robots and the tasks, the execution time is also significantly
impacted by the delay caused by navigating around moving people. However, most
existing MRTA approaches are dynamics-agnostic, relying on static maps and
neglecting human motion patterns, leading to inefficiencies and delays. In this
paper, we introduce \acrfull{method name}. This method leverages Maps of
Dynamics (MoDs), spatio-temporal queryable models designed to capture
historical human movement patterns, to estimate the impact of humans on the
task execution time during deployment. \acrshort{method name} utilizes a
stochastic cost function that includes MoDs. Experimental results show that
integrating MoDs enhances task allocation performance, resulting in reduced
mission completion times by up to $26\%$ compared to the dynamics-agnostic
method and up to $19\%$ compared to the baseline. This work underscores the
importance of considering human dynamics in MRTA within shared environments and
presents an efficient framework for deploying multi-robot systems in
environments populated by humans.

</details>


### [108] [Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles](https://arxiv.org/abs/2508.19771)
*Liding Zhang,Zhenshan Bing,Yu Zhang,Kuanqi Cai,Lingyun Chen,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: FDIT*是一种基于采样的路径规划算法，通过利用无效顶点信息和库仑定律物理原理，在EIT*基础上改进，提供更快的收敛速度和更低成本的路径解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决高维运动规划中的挑战，利用传统规划器中常被忽视的无效顶点信息，结合物理力原理来提高路径规划效率和成本效益。

Method: 基于EIT*算法，引入库仑定律物理原理，提出椭圆k近邻搜索方法，利用无效顶点数据创建基于力方向的搜索区域，探索更多问题特定的有价值搜索区域。

Result: 在R^4到R^16维度的问题上优于现有单查询采样规划器，在受限高维环境中表现出更高的搜索效率和成本降低，并在真实移动操作任务中得到验证。

Conclusion: FDIT*通过融合无效顶点信息和物理动力学原理，显著提高了路径规划的收敛速度和解决方案质量，是高维运动规划的有效方法。

Abstract: Path planning has long been an important and active research area in
robotics. To address challenges in high-dimensional motion planning, this study
introduces the Force Direction Informed Trees (FDIT*), a sampling-based planner
designed to enhance speed and cost-effectiveness in pathfinding. FDIT* builds
upon the state-of-the-art informed sampling planner, the Effort Informed Trees
(EIT*), by capitalizing on often-overlooked information in invalid vertices. It
incorporates principles of physical force, particularly Coulomb's law. This
approach proposes the elliptical $k$-nearest neighbors search method, enabling
fast convergence navigation and avoiding high solution cost or infeasible paths
by exploring more problem-specific search-worthy areas. It demonstrates
benefits in search efficiency and cost reduction, particularly in confined,
high-dimensional environments. It can be viewed as an extension of nearest
neighbors search techniques. Fusing invalid vertex data with physical dynamics
facilitates force-direction-based search regions, resulting in an improved
convergence rate to the optimum. FDIT* outperforms existing single-query,
sampling-based planners on the tested problems in R^4 to R^16 and has been
demonstrated on a real-world mobile manipulation task.

</details>


### [109] [Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization](https://arxiv.org/abs/2508.19776)
*Liding Zhang,Yao Ling,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: G3T*是一种新颖的双向运动规划算法，通过贪婪嫁接无效边连接来重建树连通性，实现快速路径收敛，在多个维度上优于现有单查询采样规划器


<details>
  <summary>Details</summary>
Motivation: 传统双向运动规划中，前向和反向搜索树的连接可能失败并导致非对称双向搜索重启，限制了lazy-reverse搜索的效果

Method: 采用贪婪方法使用GuILD子集的最小Lebesgue测度来优化路径，动态调整采样分布，嫁接无效边连接以重建树连通性

Result: 在R^2到R^8维度的基准实验和真实机器人评估中，G3T*相比现有单查询采样规划器表现出更快的收敛速度和更低的解成本

Conclusion: G3T*通过动态采样分布调整和贪婪嫁接机制，有效增强了前向搜索向反向树的生长，确保了渐近最优性，实现了优越的性能表现

Abstract: Bidirectional motion planning often reduces planning time compared to its
unidirectional counterparts. It requires connecting the forward and reverse
search trees to form a continuous path. However, this process could fail and
restart the asymmetric bidirectional search due to the limitations of
lazy-reverse search. To address this challenge, we propose Greedy GuILD
Grafting Trees (G3T*), a novel path planner that grafts invalid edge
connections at both ends to re-establish tree-based connectivity, enabling
rapid path convergence. G3T* employs a greedy approach using the minimum
Lebesgue measure of guided incremental local densification (GuILD) subsets to
optimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling
distribution between the informed set and GuILD subsets based on historical and
current cost improvements, ensuring asymptotic optimality. These features
enhance the forward search's growth towards the reverse tree, achieving faster
convergence and lower solution costs. Benchmark experiments across dimensions
from R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior
performance compared to existing single-query sampling-based planners. A video
showcasing our experimental results is available at:
https://youtu.be/3mfCRL5SQIU

</details>


### [110] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 基于语义图的风险传播算法，用于预测室内环境中易发生意外的区域，提升服务机器人的安全意识


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益深入日常生活，特别是在家庭环境中，机器人需要能够预兆环境风险并做出回应，以确保用户安全、建立信任和实现有效的人机交互

Method: 通过语义图基础的风险传播算法建模，将每个物体表示为具有风险分数的节点，基于空间近距离和意外关系，风险从高风险物体向低风险物体不对称传播

Result: 在人工标注的风险区域数据集上验证，二元风险检测准确率达到75%，尤其在涉及尖利或不稳定物体的场景中与人类感知强对齐

Conclusion: 该框架显示了语境感知风险推理在提升机器人场景理解和主动安全行为方面的潜力，可作为未来语境驱动安全决策、实时报警或自主协助用户避免危险的基础

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>


### [111] [APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors](https://arxiv.org/abs/2508.19790)
*Liding Zhang,Sicheng Wang,Kuanqi Cai,Zhenshan Bing,Fan Wu,Chaoqun Wang,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: APT*是一种新型采样运动规划器，通过自适应批量大小和椭圆最近邻模块动态调整路径搜索过程，在4-16维空间中优于现有单查询采样规划器


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法通常使用固定批量大小且忽略障碍物信息，缺乏问题特异性，需要更智能的自适应规划方法

Method: 基于FDIT*扩展，集成自适应批量调整和椭圆r最近邻模块，将顶点视为遵循库仑定律的电荷来定义虚拟力，通过非线性方法自适应调整电荷

Result: 在ℝ⁴到ℝ¹⁶维度空间中优于现有单查询采样规划器，收敛速度更快且解成本更低，并通过真实机器人操作任务验证

Conclusion: APT*通过自适应机制有效提升了高维空间中的路径规划性能，为复杂环境下的运动规划提供了有效解决方案

Abstract: Optimal path planning aims to determine a sequence of states from a start to
a goal while accounting for planning objectives. Popular methods often
integrate fixed batch sizes and neglect information on obstacles, which is not
problem-specific. This study introduces Adaptively Prolated Trees (APT*), a
novel sampling-based motion planner that extends based on Force Direction
Informed Trees (FDIT*), integrating adaptive batch-sizing and elliptical
$r$-nearest neighbor modules to dynamically modulate the path searching process
based on environmental feedback. APT* adjusts batch sizes based on the
hypervolume of the informed sets and considers vertices as electric charges
that obey Coulomb's law to define virtual forces via neighbor samples, thereby
refining the prolate nearest neighbor selection. These modules employ
non-linear prolate methods to adaptively adjust the electric charges of
vertices for force definition, thereby improving the convergence rate with
lower solution costs. Comparative analyses show that APT* outperforms existing
single-query sampling-based planners in dimensions from $\mathbb{R}^4$ to
$\mathbb{R}^{16}$, and it was further validated through a real-world robot
manipulation task. A video showcasing our experimental results is available at:
https://youtu.be/gCcUr8LiEw4

</details>


### [112] [A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](https://arxiv.org/abs/2508.19816)
*Ricardo J. Manríquez-Cisterna,Ankit A. Ravankar,Jose V. Salazar Luces,Takuro Hatsukari,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 开发了一款名为Moby的站立式移动辅助机器人，旨在帮助老年人在日常活动中保持独立性和安全性，特别是在如厕转移等场景中。


<details>
  <summary>Details</summary>
Motivation: 传统坐式移动辅助设备存在局限性，无法维持使用者直立姿势，导致身体负担增加、社交互动受限，且影响自我效能感。需要一种新型辅助设备来提升老年人的生活质量和独立性。

Method: 采用机器人操作系统（ROS）进行控制，具备手动和自主操作模式。集成NAV2和LiDAR实现稳健导航，设计轻量化且多功能的机械结构，提供被动支持和移动辅助功能。

Result: 通过NASA-TLX方法和时间对比实验验证，Moby机器人表现出易用性、舒适性和有效性，在坐立转换辅助方面表现优异，用户体验得到显著提升。

Conclusion: Moby站立式移动机器人成功解决了传统移动辅助设备的不足，为老年人提供了更自然、安全和独立的日常活动支持，具有重要的实际应用价值。

Abstract: This paper presents a standing support mobility robot "Moby" developed to
enhance independence and safety for elderly individuals during daily activities
such as toilet transfers. Unlike conventional seated mobility aids, the robot
maintains users in an upright posture, reducing physical strain, supporting
natural social interaction at eye level, and fostering a greater sense of
self-efficacy. Moby offers a novel alternative by functioning both passively
and with mobility support, enabling users to perform daily tasks more
independently. Its main advantages include ease of use, lightweight design,
comfort, versatility, and effective sit-to-stand assistance. The robot
leverages the Robot Operating System (ROS) for seamless control, featuring
manual and autonomous operation modes. A custom control system enables safe and
intuitive interaction, while the integration with NAV2 and LiDAR allows for
robust navigation capabilities. This paper reviews existing mobility solutions
and compares them to Moby, details the robot's design, and presents objective
and subjective experimental results using the NASA-TLX method and time
comparisons to other methods to validate our design criteria and demonstrate
the advantages of our contribution.

</details>


### [113] [FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control](https://arxiv.org/abs/2508.19926)
*Tan Jing,Shiting Chen,Yangfan Li,Weisheng Xu,Renjing Xu*

Main category: cs.RO

TL;DR: FARM框架通过帧加速增强、基础控制器和残差混合专家模型，显著提升了人形控制器在爆炸性高动态动作上的性能，同时保持日常低动态动作的精准跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的人形控制器在温和日常动作上表现良好，但在爆炸性高动态动作上容易失败，限制了实际应用部署。需要解决这一性能差距。

Method: 提出FARM端到端框架：1) 帧加速增强技术扩大帧间间隔以暴露模型于高速姿态变化；2) 稳健基础控制器处理低动态动作；3) 残差混合专家模型自适应分配额外网络容量处理高动态动作。

Result: 在HDHM数据集上，FARM将跟踪失败率降低42.8%，全局平均关节位置误差降低14.6%，同时在低动态动作上保持近乎完美的准确性。

Conclusion: FARM为高动态人形控制设立了新基准，并首次提供了专门针对这一挑战的开放基准数据集，代码和数据集将开源发布。

Abstract: Unified physics-based humanoid controllers are pivotal for robotics and
character animation, yet models that excel on gentle, everyday motions still
stumble on explosive actions, hampering real-world deployment. We bridge this
gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),
an end-to-end framework composed of frame-accelerated augmentation, a robust
base controller, and a residual mixture-of-experts (MoE). Frame-accelerated
augmentation exposes the model to high-velocity pose changes by widening
inter-frame gaps. The base controller reliably tracks everyday low-dynamic
motions, while the residual MoE adaptively allocates additional network
capacity to handle challenging high-dynamic actions, significantly enhancing
tracking accuracy. In the absence of a public benchmark, we curate the
High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically
plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\% and
lowers global mean per-joint position error by 14.6\% relative to the baseline,
while preserving near-perfect accuracy on low-dynamic motions. These results
establish FARM as a new baseline for high-dynamic humanoid control and
introduce the first open benchmark dedicated to this challenge. The code and
dataset will be released at https://github.com/Colin-Jing/FARM.

</details>


### [114] [Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors](https://arxiv.org/abs/2508.19953)
*Rafael Cathomen,Mayank Mittal,Marin Vlastelica,Marco Hutter*

Main category: cs.RO

TL;DR: 提出模块化无监督技能发现框架，通过状态空间分解、对称性偏置和风格因子来提高机器人技能的安全性、可解释性和部署性


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督技能发现方法在现实机器人应用中面临的安全性、可解释性和部署性挑战

Method: 使用用户定义的状态空间分解学习解耦技能表示，为不同因子分配特定技能发现算法，引入对称性归纳偏置和风格因子，并加入正则化惩罚项

Result: 在四足机器人仿真中验证框架有效性，实现零样本迁移到真实硬件，发现结构化人类可解释行为，安全性和多样性得到提升，下游任务性能与手工奖励策略相当

Conclusion: 状态空间分解和对称性偏置有助于发现结构化技能，风格因子和惩罚项增强安全性和鲁棒性，框架具有良好的实际部署潜力

Abstract: Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.

</details>


### [115] [Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](https://arxiv.org/abs/2508.19958)
*Yiguo Fan,Pengxiang Ding,Shuanghao Bai,Xinyang Tong,Yuyang Zhu,Hongchao Lu,Fengqi Dai,Wei Zhao,Yang Liu,Siteng Huang,Zhaoxin Fan,Badong Chen,Donglin Wang*

Main category: cs.RO

TL;DR: Long-VLA是首个专门针对长时域机器人任务的端到端视觉-语言-动作模型，通过相位感知输入掩码策略将子任务分为移动和交互阶段，显著提升了长时域操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要处理短时域任务，在长时域、多步骤的机器人操作中由于技能链和子任务依赖性的挑战而效果有限。

Method: 提出相位感知输入掩码策略，自适应地将每个子任务分割为移动和交互阶段，使模型能够专注于阶段相关的感知线索，增强子任务兼容性。该架构无关模块可无缝集成到现有VLA模型中。

Result: 在模拟和真实世界任务上的大量实验表明，Long-VLA显著优于先前的最先进方法，为长时域机器人控制建立了新的基准。

Conclusion: Long-VLA通过统一的相位感知策略保持了VLA训练的可扩展性和数据效率，是长时域机器人任务的有效解决方案。

Abstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic
policy learning, leveraging large-scale multimodal data for robust and scalable
control. However, existing VLA frameworks primarily address short-horizon
tasks, and their effectiveness on long-horizon, multi-step robotic manipulation
remains limited due to challenges in skill chaining and subtask dependencies.
In this work, we introduce Long-VLA, the first end-to-end VLA model
specifically designed for long-horizon robotic tasks. Our approach features a
novel phase-aware input masking strategy that adaptively segments each subtask
into moving and interaction phases, enabling the model to focus on
phase-relevant sensory cues and enhancing subtask compatibility. This unified
strategy preserves the scalability and data efficiency of VLA training, and our
architecture-agnostic module can be seamlessly integrated into existing VLA
models. We further propose the L-CALVIN benchmark to systematically evaluate
long-horizon manipulation. Extensive experiments on both simulated and
real-world tasks demonstrate that Long-VLA significantly outperforms prior
state-of-the-art methods, establishing a new baseline for long-horizon robotic
control.

</details>


### [116] [Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech](https://arxiv.org/abs/2508.20037)
*Henk H. A. Jekel,Alejandro Díaz Rosales,Luka Peternel*

Main category: cs.RO

TL;DR: 提出了一种结合视觉（眼动追踪）和语音交互的远程机器人刚度控制界面，通过视觉语言模型处理操作者的注视和语音指令来生成合适的刚度矩阵


<details>
  <summary>Details</summary>
Motivation: 传统远程阻抗控制需要复杂的手动参数调整，本文旨在通过自然的人机交互方式（注视+语音）来简化3D刚度椭球的命令过程

Method: 使用眼动追踪器捕捉操作者注视点，结合GPT-4o处理的语音指令，通过视觉语言模型理解场景上下文并生成相应的刚度矩阵

Result: 实验验证了界面在提示配置优化和具体任务（滑槽任务）中的功能表现，展示了系统的可行性和有效性

Conclusion: 视-语遥阻抗界面为远程机器人控制提供了一种直观自然的交互方式，减少了传统方法的复杂性

Abstract: The paper presents a visio-verbal teleimpedance interface for commanding 3D
stiffness ellipsoids to the remote robot with a combination of the operator's
gaze and verbal interaction. The gaze is detected by an eye-tracker, allowing
the system to understand the context in terms of what the operator is currently
looking at in the scene. Along with verbal interaction, a Visual Language Model
(VLM) processes this information, enabling the operator to communicate their
intended action or provide corrections. Based on these inputs, the interface
can then generate appropriate stiffness matrices for different physical
interaction actions. To validate the proposed visio-verbal teleimpedance
interface, we conducted a series of experiments on a setup including a Force
Dimension Sigma.7 haptic device to control the motion of the remote Kuka LBR
iiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,
while human verbal commands are processed by a VLM using GPT-4o. The first
experiment explored the optimal prompt configuration for the interface. The
second and third experiments demonstrated different functionalities of the
interface on a slide-in-the-groove task.

</details>


### [117] [HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation](https://arxiv.org/abs/2508.20085)
*Zhecheng Yuan,Tianming Wei,Langzhe Gu,Pu Hua,Tianhai Liang,Yuanpei Chen,Huazhe Xu*

Main category: cs.RO

TL;DR: HERMES是一个从人类运动到机器人学习的框架，用于移动双手机器人灵巧操作，通过统一强化学习方法将多源人手运动转化为机器人行为，并解决sim2real差距和环境适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将多源人手运动转化为高维灵巧手的可行机器人行为，且缺乏适应多样化环境条件的能力，需要开发能够处理复杂动作空间并实现真实世界泛化的解决方案。

Method: 1) 统一强化学习方法转换异构人手运动为机器人行为；2) 基于深度图像的端到端sim2real迁移方法；3) 增强导航基础模型，加入闭环PnP定位机制，连接自主导航和灵巧操作。

Result: 实验结果表明HERMES在多样化真实场景中表现出良好的泛化行为，成功完成众多复杂的移动双手机器人灵巧操作任务。

Conclusion: HERMES框架有效解决了从人类运动到机器人行为的转换问题，实现了在非结构化环境中的自主操作，为移动双手机器人灵巧操作提供了可行的解决方案。

Abstract: Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.

</details>


### [118] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: 提出DGD框架，结合离散MAPF求解器和生成扩散模型，解决多机器人运动规划问题，在100机器人规模下实现高效规划和高成功率


<details>
  <summary>Details</summary>
Motivation: 现有离散MAPF方法可扩展但轨迹质量差，连续优化方法质量高但维度灾难导致可扩展性差，需要结合两者优势

Method: 将非凸MRMP问题分解为凸配置空间的子问题，用离散MAPF解引导扩散模型捕获时空依赖，加入轻量约束修复确保可行性

Result: 在大规模复杂环境中达到最先进性能，可扩展到100个机器人，实现高效规划和高成功率

Conclusion: DGD框架成功整合离散和连续方法，为多机器人运动规划提供了可扩展且高质量的解决方案

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [119] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: 提出了一种基于重心UV转换的快速面部纹理传输方法，相比传统方法速度提升7000倍，同时显著改善纹理质量


<details>
  <summary>Details</summary>
Motivation: 传统基于仿射变换的方法速度慢且容易产生视觉伪影，需要一种更高效、质量更好的面部纹理传输方案

Method: 使用重心UV转换技术，预先计算整个UV映射到单个变换矩阵中，实现单次操作的纹理传输

Result: 速度比基线方法快7000倍以上，显著消除了边界伪影，提高了最终纹理质量

Conclusion: 该方法为沉浸式XR应用中的个性化提供了实用解决方案，代码已在线提供

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>
