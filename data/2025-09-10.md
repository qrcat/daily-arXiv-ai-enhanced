<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning](https://arxiv.org/abs/2509.07493)
*Wenzhi Guo,Bing Wang*

Main category: cs.CV

TL;DR: DiGS将SDF学习嵌入3D高斯泼溅框架，通过几何引导的网格增长策略提升表面重建精度和完整性，同时保持高渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在渲染方面表现出色，但由于表示的无结构性和缺乏显式几何监督，难以实现准确完整的表面重建。

Method: 为每个高斯关联可学习的SDF值，设计几何引导的网格增长策略，在多尺度层次下自适应分布高斯到几何一致区域。

Result: 在DTU、Mip-NeRF 360和Tanks&Temples等标准基准测试中，DiGS一致提高了重建精度和完整性，同时保持高渲染保真度。

Conclusion: DiGS通过将SDF学习集成到3DGS中，成功实现了更好的表面重建质量，为3D重建提供了统一且有效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.

</details>


### [2] [CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis](https://arxiv.org/abs/2509.06986)
*Cedric Caruzzo,Jong Chul Ye*

Main category: cs.CV

TL;DR: CellPainTR是一个基于Transformer的架构，用于学习对批次效应具有鲁棒性的细胞形态基础表示，无需在新数据上重新训练即可实现有效的跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 大规模生物发现需要整合海量异构数据集，但技术批次效应和缺乏可泛化模型仍是关键障碍。

Method: 采用Transformer架构，设计源特定上下文标记，允许在未见数据集上进行有效的分布外泛化而无需微调。

Result: 在JUMP数据集上优于ComBat和Harmony等现有方法，在批次整合和生物信号保留方面表现优异；在Bray等未见数据集上保持高性能。

Conclusion: 这项工作代表了创建真正基于图像的形态分析基础模型的重要一步，实现了更可靠和可扩展的跨研究生物分析。

Abstract: Large-scale biological discovery requires integrating massive, heterogeneous
datasets like those from the JUMP Cell Painting consortium, but technical batch
effects and a lack of generalizable models remain critical roadblocks. To
address this, we introduce CellPainTR, a Transformer-based architecture
designed to learn foundational representations of cellular morphology that are
robust to batch effects. Unlike traditional methods that require retraining on
new data, CellPainTR's design, featuring source-specific context tokens, allows
for effective out-of-distribution (OOD) generalization to entirely unseen
datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP
dataset, where it outperforms established methods like ComBat and Harmony in
both batch integration and biological signal preservation. Critically, we
demonstrate its robustness through a challenging OOD task on the unseen Bray et
al. dataset, where it maintains high performance despite significant domain and
feature shifts. Our work represents a significant step towards creating truly
foundational models for image-based profiling, enabling more reliable and
scalable cross-study biological analysis.

</details>


### [3] [FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection](https://arxiv.org/abs/2509.06987)
*Alexey Zhukov,Jenny Benois-Pineau,Amira Youssef,Akka Zemmari,Mohamed Mosbah,Virginie Taillandier*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于域规则的多模态融合架构，结合YOLOv8n和Vision Transformer，通过融合视觉和音频信息来提高铁路缺陷检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 单一视觉模态方法在铁路缺陷检测中存在过检测问题，当正常结构元件外观类似缺陷时会产生偏差。音频信息虽然不含高级语义，但可以提供补充信息来提升检测精度。

Method: 构建了基于YOLOv8n和Vision Transformer的多模态融合架构，从多个层（7、16、19）提取特征图，并与合成的音频表征进行融合，重点检测铁路破裂和表面缺陷两类缺陷。

Result: 在真实铁路数据集上评估，多模态融合方法比单纯视觉方法提高了准确性和总体准确度约0.2个百分点，Student t检验证实了均值准确性差异的统计显著性。

Conclusion: 该研究证明了通过融合视觉和音频多模态信息可以有效提升铁路缺陷检测的性能，为工业检测应用提供了更加准确的解决方案。

Abstract: Multimodal fusion is a multimedia technique that has become popular in the
wide range of tasks where image information is accompanied by a signal/audio.
The latter may not convey highly semantic information, such as speech or music,
but some measures such as audio signal recorded by mics in the goal to detect
rail structure elements or defects. While classical detection approaches such
as You Only Look Once (YOLO) family detectors can be efficiently deployed for
defect detection on the image modality, the single modality approaches remain
limited. They yield an overdetection in case of the appearance similar to
normal structural elements. The paper proposes a new multimodal fusion
architecture built on the basis of domain rules with YOLO and Vision
transformer backbones. It integrates YOLOv8n for rapid object detection with a
Vision Transformer (ViT) to combine feature maps extracted from multiple layers
(7, 16, and 19) and synthesised audio representations for two defect classes:
rail Rupture and Surface defect. Fusion is performed between audio and image.
Experimental evaluation on a real-world railway dataset demonstrates that our
multimodal fusion improves precision and overall accuracy by 0.2 points
compared to the vision-only approach. Student's unpaired t-test also confirms
statistical significance of differences in the mean accuracy.

</details>


### [4] [Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection](https://arxiv.org/abs/2509.06988)
*Yingsheng Wang,Shuo Lu,Jian Liang,Aihua Zheng,Ran He*

Main category: cs.CV

TL;DR: 提出了一种无需训练数据的后处理OOD检测方法ClaFR，通过分类器权重的正交分解获得类已知子空间，计算特征重构误差来识别分布外数据。


<details>
  <summary>Details</summary>
Motivation: 现有的基于特征的后处理方法需要访问训练数据，这在数据隐私保护场景下不适用，因此需要开发不依赖训练数据的OOD检测方法。

Method: ClaFR方法：1）对分类器权重进行正交分解提取类已知子空间；2）将原始特征映射到该子空间获得新表示；3）计算子空间内的特征重构误差作为OOD得分。

Result: 在多个OOD基准测试中取得了领先性能，且不需要访问训练数据。

Conclusion: ClaFR是一种简单有效的后处理OOD检测方法，解决了数据隐私场景下的限制，在性能上优于现有方法。

Abstract: Out-of-distribution (OOD) detection helps models identify data outside the
training categories, crucial for security applications. While feature-based
post-hoc methods address this by evaluating data differences in the feature
space without changing network parameters, they often require access to
training data, which may not be suitable for some data privacy scenarios. This
may not be suitable in scenarios where data privacy protection is a concern. In
this paper, we propose a simple yet effective post-hoc method, termed
Classifier-based Feature Reconstruction (ClaFR), from the perspective of
subspace projection. It first performs an orthogonal decomposition of the
classifier's weights to extract the class-known subspace, then maps the
original data features into this subspace to obtain new data representations.
Subsequently, the OOD score is determined by calculating the feature
reconstruction error of the data within the subspace. Compared to existing OOD
detection algorithms, our method does not require access to training data while
achieving leading performance on multiple OOD benchmarks. Our code is released
at https://github.com/Aie0923/ClaFR.

</details>


### [5] [DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining](https://arxiv.org/abs/2509.06990)
*Bryan Rodas,Natalie Montesino,Jakob Ambsdorf,David Klindt,Randall Balestriero*

Main category: cs.CV

TL;DR: DIET-CP是一种简单的持续预训练策略，可以在小规模专业领域数据集上有效调整基础模型，无需标签和额外超参数，仅用1000张图像就能显著提升DINOv3等先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决专业领域中数据集规模小、SSL方法不适用、预训练模型缺乏重要信息等问题，提供一种简单有效的持续预训练方案。

Method: 基于简单目标函数的持续预训练策略，无需标签，不引入额外超参数，适用于不同数据模态和骨干网络。

Result: 在仅使用1000张图像的情况下，能够显著提升DINOv3等先进模型的性能，且在不同数据模态和骨干网络选择上表现稳定。

Conclusion: DIET-CP为小规模专业领域数据集的持续预训练提供了一种简单、稳定且高效的解决方案，填补了现有方法的空白。

Abstract: Continued pretraining offers a promising solution for adapting foundation
models to a new target domain. However, in specialized domains, available
datasets are often very small, limiting the applicability of SSL methods
developed for large-scale pretraining and making hyperparameter search
infeasible. In addition, pretrained models are usually released as
backbone-weights only, lacking important information to continue pretraining.
We propose to bridge this gap with DIET-CP, a simple continued pretraining
strategy, where any strong foundation model can be steered towards the new data
distribution of interest. DIET-CP relies on a very simple objective, requires
no labels, and introduces no more hyperparameters than supervised finetuning.
It is stable across data modalities and backbone choices, while providing a
significant performance boost for state-of-the-art models such as DINOv3 using
only 1000 images.

</details>


### [6] [FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2509.06992)
*Kun Zhai,Siheng Chen,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: FedAPT是一种联邦对抗提示调优方法，通过类别感知提示生成器和跨层生成器共享策略，解决了非IID设置下的类别信息差距问题，显著提升了联邦提示调优的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦提示调优(FPT)方法在面对对抗攻击时容易导致下游任务错误分类，特别是在非独立同分布(non-IID)设置下存在客户端与全局模型之间的类别信息差距问题。

Method: 提出类别感知提示生成器，利用全局标签嵌入作为引导生成视觉提示；采用跨层生成器共享策略增强不同层间提示的耦合性。

Result: 在多个图像分类数据集上的实验表明，FedAPT在对抗鲁棒性方面显著优于现有方法，并在跨域和跨数据集场景中表现出优异的泛化能力。

Conclusion: FedAPT有效解决了联邦提示调优中的对抗鲁棒性问题，特别是在非IID设置下的类别信息差距挑战，具有实际应用价值。

Abstract: Federated Prompt Tuning (FPT) is an efficient method for cross-client
collaborative fine-tuning of large Vision-Language Models (VLMs). However,
models tuned using FPT are vulnerable to adversarial attacks, leading to
misclassification in downstream tasks. In this work, we introduce Federated
Adversarial Prompt Tuning (\textbf{FedAPT}), a novel method designed to enhance
the adversarial robustness of FPT. We identify a key issue in FedAPT under
non-independent and identically distributed (non-IID) settings: a \textit{class
information gap} between clients and the global model. Clients rely solely on
limited local label information to generate adversarial samples for training,
while the global model must defend against adversarial attacks from global
labels. To address this issue, we propose a \textbf{class-aware prompt
generator} that generates visual prompts from text prompts. This generator is
guided by a \emph{Global Label Embedding} (serving as a ``beacon") which
encodes cross-client label information to create more globally-aligned visual
prompts. Additionally, we propose a \textbf{cross-layer generator sharing}
strategy to enhance prompt coupling across different layers of the model,
further boosting adversarial robustness. Extensive experiments on multiple
image classification datasets demonstrate the superiority of FedAPT in
improving adversarial robustness, outperforming existing methods by a large
margin. FedAPT also exhibits exceptional generalization in cross-domain and
cross-dataset scenarios, indicating its effectiveness in real-world
applications.

</details>


### [7] [Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)](https://arxiv.org/abs/2509.06993)
*Zirui Xu,Raphael Tang,Mike Bianco,Qi Zhang,Rishi Madhok,Nikolaos Karianakis,Fuxun Yu*

Main category: cs.CV

TL;DR: 本文介绍了CVPR 2025 EarthVision Embed2Scale挑战赛的冠军解决方案，该挑战赛旨在开发基础地理空间模型，将SSL4EO-S12高光谱地理空间数据立方体嵌入到向量表示中，以支持分类、回归等下游任务。


<details>
  <summary>Details</summary>
Motivation: 开发能够有效处理高光谱地理空间数据的基础模型，为各种下游地理空间分析任务提供高质量的嵌入表示。

Method: 技术报告未详细说明具体方法，但提到这是Top-1获胜解决方案，可能涉及先进的自监督学习或对比学习技术来处理高光谱数据立方体。

Result: 该方法在Embed2Scale挑战赛中获得了第一名，证明了其在处理地理空间高光谱数据嵌入任务上的有效性。

Conclusion: 提出的解决方案在CVPR 2025 EarthVision挑战赛中表现优异，为地理空间高光谱数据处理提供了有效的嵌入方法，有助于推动基础地理空间模型的发展。

Abstract: EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational
geospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into
embedding vectors that faciliatetes various downstream tasks, e.g.,
classification, regression, etc. In this technical report, we introduce our
proposed method for the Top-1 winning solution on the Embed2Scale Challenge.

</details>


### [8] [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
*Srihari Bandraupalli,Anupam Purwar*

Main category: cs.CV

TL;DR: ViLD框架填补了学术评估与企业部署需求之间的差距，通过10个关键业务任务和创新的BlockWeaver算法，在7500个真实样本上评估开源VLM模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于选择题和合成数据的基准测试无法反映企业实际应用需求，需要建立更贴近真实业务场景的评估框架。

Method: 提出ViLD框架，定义10个企业关键任务；开发BlockWeaver算法用于无序OCR输出比较；构建7500个真实样本的数据集；结合语义匹配、传统指标和新方法进行评估。

Result: 对领先开源VLM模型（Qwen、MIMO、InternVL）进行了行业基础的、任务驱动的能力评估，提供了企业部署的可操作见解。

Conclusion: ViLD框架为开源视觉语言模型在企业环境中的部署提供了实用的评估标准和指导，填补了学术研究与企业应用之间的重要空白。

Abstract: Open-source Vision-Language Models show immense promise for enterprise
applications, yet a critical disconnect exists between academic evaluation and
enterprise deployment requirements. Current benchmarks rely heavily on
multiple-choice questions and synthetic data, failing to capture the complexity
of real-world business applications like social media content analysis. This
paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge
this gap by evaluating VLMs on operational enterprise requirements. We define
ten business-critical tasks: logo detection, OCR, object detection, human
presence and demographic analysis, human activity and appearance analysis,
scene detection, camera perspective and media quality assessment, dominant
colors, comprehensive description, and NSFW detection. To this framework, we
bring an innovative BlockWeaver Algorithm that solves the challenging problem
of comparing unordered, variably-grouped OCR outputs from VLMs without relying
on embeddings or LLMs, achieving remarkable speed and reliability. To
demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500
diverse samples, carefully stratified from a corpus of one million real-world
images and videos. ViLD provides actionable insights by combining semantic
matching (both embedding-based and LLM-as-a-judge approaches), traditional
metrics, and novel methods to measure the completeness and faithfulness of
descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and
InternVL) against a powerful proprietary baseline as per ViLD framework, we
provide one of the first industry-grounded, task-driven assessment of VLMs
capabilities, offering actionable insights for their deployment in enterprise
environments.

</details>


### [9] [The Protocol Genome A Self Supervised Learning Framework from DICOM Headers](https://arxiv.org/abs/2509.06995)
*Jimmy Joseph*

Main category: cs.CV

TL;DR: Protocol Genome是一个自监督学习系统，通过分析DICOM头部数据学习协议相关性，在外部验证中达到AUROC 0.901（基线0.847）和ECE 0.036（基线0.058），提高了跨模态和厂商的校准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床影像通过PACS/DICOM系统处理，扫描协议选择（扫描仪型号、序列、参数等）会影响图像对比度、噪声和伪影，这些潜在混杂因素阻碍了仅基于图像的网络在不同站点间的泛化能力。

Method: 将结构化DICOM头部作为标签，通过三种方式学习协议感知但临床鲁棒的图像表示：(1)协议-图像对比学习，(2)掩码协议预测，(3)协议-协议转换。使用126万项研究数据进行实验。

Result: 在三个临床任务上表现优异：肺栓塞CT分诊（+0.046 AUROC）、脑胶质瘤MRI分级（+0.058 AUROC）、心胸比X线检测（+0.041 AUROC），获得25-37%的校准改进，且在10-20%标注数据下仍保持优势。

Conclusion: Protocol Genome技术减少了协议边界处的假阳性，适用于PACS系统，提供了模型卡和部署指南，包括去标识化和偏倚审计。

Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning
system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs
0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.
Our method also improves calibration and robustness across modalities (CT, MRI,
CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where
procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice
thickness) have consequences for contrast, noise, and artifact. These latent
confounders impede the generalization of image-only networks across sites. We
consider structured DICOM headers as a label and learn protocol-aware but
clinically robust image representations. Protocol Genome obtains tokenized
embeddings of de-identified header fields and models them along with image
features using: (1) protocol-image contrastive learning, (2) masked protocol
prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health
systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT
triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph
cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well
as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:
cardiomegaly) is associated with higher external AUROC; 25-37% calibration
improvements are obtained (p < 0.01, DeLong tests). While the gains may be
task-dependent, they are preserved with 10-20% of labeled data. From a clinical
point of view, the technique reduces false positives at protocol borders and is
applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a
model card and deployment guide, complete with both de-identification and bias
audits.

</details>


### [10] [Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems](https://arxiv.org/abs/2509.06996)
*Jie Zhang,Ting Xu,Gelei Deng,Runyi Hu,Han Qiu,Tianwei Zhang,Qing Guo,Ivor Tsang*

Main category: cs.CV

TL;DR: 这篇论文研究视觉语言模型在文字识别中的弹性问题，发现当文字字符出现碎片化、融合或遮挡时，VLMs模型性能会大幅下降，而人类仍能识别。


<details>
  <summary>Details</summary>
Motivation: 研究人类在识别碎片化、融合或遮挡文字时的强大弹性，并探索先进视觉语言模型是否具备相同的能力。

Method: 构建两个受心理物理学启发的测试标准（中文象形文字和英语字母词汇），通过切割、重组和叠加字形来创造"可见但不可读"的刺激物，而人类仍能识别。

Result: 虽然在清晰文本上表现强劲，但当代VLMs在这些干扰下性能严重下降，经常产生无关或不连贯的输出。

Conclusion: 模型存在结构性限制：过度依赖通用视觉不变性而少使用组合性先验知识。需要开发能够编码符号分割、组合和绑定的架构和训练策略。

Abstract: Writing is a universal cultural technology that reuses vision for symbolic
communication. Humans display striking resilience: we readily recognize words
even when characters are fragmented, fused, or partially occluded. This paper
investigates whether advanced vision language models (VLMs) share this
resilience. We construct two psychophysics inspired benchmarks across distinct
writing systems, Chinese logographs and English alphabetic words, by splicing,
recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli
for models while remaining legible to humans. Despite strong performance on
clean text, contemporary VLMs show a severe drop under these perturbations,
frequently producing unrelated or incoherent outputs. The pattern suggests a
structural limitation: models heavily leverage generic visual invariances but
under rely on compositional priors needed for robust literacy. We release
stimuli generation code, prompts, and evaluation protocols to facilitate
transparent replication and follow up work. Our findings motivate architectures
and training strategies that encode symbol segmentation, composition, and
binding across scripts, and they delineate concrete challenges for deploying
multimodal systems in education, accessibility, cultural heritage, and
security.

</details>


### [11] [K-Syn: K-space Data Synthesis in Ultra Low-data Regimes](https://arxiv.org/abs/2509.06997)
*Guan Yu,Zhang Jianhua,Liang Dong,Liu Qiegen*

Main category: cs.CV

TL;DR: 提出了一种在频率域进行特征级学习的方法，通过时间融合策略生成k空间数据，用于动态心脏MRI重建，在低数据情况下表现出强大的生成能力。


<details>
  <summary>Details</summary>
Motivation: 由于心脏磁共振成像的动态性和复杂性，高质量且多样化的k空间数据在实践中很少可用，这阻碍了动态心脏MRI的稳健重建。

Method: 在频率域进行特征级学习，利用傅里叶变换的全局表示能力，将频域视为自然全局特征空间；采用时间融合策略整合不同时间帧的k空间数据来引导和优化生成轨迹。

Result: 实验结果表明，该方法在低数据情况下具有强大的生成能力，能够有效缓解动态MRI重建中的数据稀缺问题。

Conclusion: 该方法通过频率域特征级建模和时间融合策略，为动态心脏MRI重建提供了一种实用的解决方案，特别是在数据稀缺的情况下表现出良好的性能。

Abstract: Owing to the inherently dynamic and complex characteristics of cardiac
magnetic resonance (CMR) imaging, high-quality and diverse k-space data are
rarely available in practice, which in turn hampers robust reconstruction of
dynamic cardiac MRI. To address this challenge, we perform feature-level
learning directly in the frequency domain and employ a temporal-fusion strategy
as the generative guidance to synthesize k-space data. Specifically, leveraging
the global representation capacity of the Fourier transform, the frequency
domain can be considered a natural global feature space. Therefore, unlike
traditional methods that use pixel-level convolution for feature learning and
modeling in the image domain, this letter focuses on feature-level modeling in
the frequency domain, enabling stable and rich generation even with ultra
low-data regimes. Moreover, leveraging the advantages of feature-level modeling
in the frequency domain, we integrate k-space data across time frames with
multiple fusion strategies to steer and further optimize the generative
trajectory. Experimental results demonstrate that the proposed method possesses
strong generative ability in low-data regimes, indicating practical potential
to alleviate data scarcity in dynamic MRI reconstruction.

</details>


### [12] [Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories](https://arxiv.org/abs/2509.06998)
*Liviu Nicolae Fircă,Antonio Bărbălau,Dan Oneata,Elena Burceanu*

Main category: cs.CV

TL;DR: 这篇论文首次明确评估模型在语义和感知上相差很大的类别间进行属性预测的稳健性，发现性能随着训练集和测试集相关性降低而冲击性下降


<details>
  <summary>Details</summary>
Motivation: 评估模型是否能够将属性知识模拟到语义和感知上差异很大的类别之间，比如识别"四条腿"属性同时应用于"狗"和"椅子"

Method: 采用四种训练-测试集分割策略：LLM驱动的语义分组、嵌入相似度阈值、嵌入基于聚类、以及使用真实标签的超级类分割

Result: 随着训练集和测试集类别间相关性降低，模型性能冲击性下降，聚类方法在减少隐藏相关性和保持可学习性之间实现最佳找平衡

Conclusion: 当前表示学习方法在属性推理任务中存在显著局限性，需要更好的分割策略来构建更健壮的基准测试

Abstract: Can models generalize attribute knowledge across semantically and
perceptually dissimilar categories? While prior work has addressed attribute
prediction within narrow taxonomic or visually similar domains, it remains
unclear whether current models can abstract attributes and apply them to
conceptually distant categories. This work presents the first explicit
evaluation for the robustness of the attribute prediction task under such
conditions, testing whether models can correctly infer shared attributes
between unrelated object types: e.g., identifying that the attribute "has four
legs" is common to both "dogs" and "chairs". To enable this evaluation, we
introduce train-test split strategies that progressively reduce correlation
between training and test sets, based on: LLM-driven semantic grouping,
embedding similarity thresholding, embedding-based clustering, and
supercategory-based partitioning using ground-truth labels. Results show a
sharp drop in performance as the correlation between training and test
categories decreases, indicating strong sensitivity to split design. Among the
evaluated methods, clustering yields the most effective trade-off, reducing
hidden correlations while preserving learnability. These findings offer new
insights into the limitations of current representations and inform future
benchmark construction for attribute reasoning.

</details>


### [13] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于人在循环的量化评估框架，用于评估大语言模型生成的3D模型的几何和结构保真度，通过综合性相似性指标实现更快的收敛效果。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统的方法来量化评估LLM生成3D模型的几何和结构保真度，以支持CAD设计民主化、反向工程和快速原型制作等应用。

Method: 提出了一套综合的相似性和复杂度指标，包括体积准确度、表面对齐度、尺寸保真度和拓扑复杂性，并在L垂尾组件上比较四种输入模态的表现。

Result: 识别到语义丰富度越高的输入模态带来更好的生成保真度，代码级提示实现了完美重建，量化评估方法对比传统定性方法显著提高了收敛速度。

Conclusion: 该研究不仅推进了对AI辅助形状合成的理解，还提供了一种可扩展的方法论来验证和精炼生成模型，为多样化CAD应用提供支持。

Abstract: Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


### [14] [MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning](https://arxiv.org/abs/2509.07021)
*Jiarui Chen,Yikeng Chen,Yingshuang Zou,Ye Huang,Peng Wang,Yuan Liu,Yujing Sun,Wenping Wang*

Main category: cs.CV

TL;DR: MEGS²是一个针对3D高斯泼溅技术的内存优化框架，通过联合优化基元数量和每个基元的参数，实现了前所未有的内存压缩，在保持渲染质量的同时显著降低了显存使用。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然在新视角合成中表现出色，但其高内存消耗严重限制了在边缘设备上的应用。现有压缩方法大多只关注存储压缩，未能解决渲染内存的关键瓶颈问题。

Method: 1) 用轻量级的任意方向球面高斯瓣替代内存密集的球谐函数作为颜色表示；2) 提出统一的软剪枝框架，将基元数量和瓣数量剪枝建模为单一约束优化问题。

Result: MEGS²相比现有方法实现了50%的静态显存减少和40%的渲染显存减少，同时保持了相当的渲染质量。

Conclusion: 该框架通过联合优化基元数量和参数配置，有效解决了3D高斯泼溅技术的内存瓶颈问题，为边缘设备部署提供了可行的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

</details>


### [15] [Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models](https://arxiv.org/abs/2509.07027)
*Jisung Hwang,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出了一种新颖的正则化损失函数，通过空间域矩正则化和频谱域功率谱正则化的组合，强制潜在空间样本符合标准高斯分布，提升文本到图像模型的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有高斯性正则化方法存在局限性，需要一种更有效的方法来确保文本到图像模型潜在空间中的样本符合标准高斯分布，以支持各种下游优化任务。

Method: 将高维样本元素视为一维标准高斯变量，结合空间域矩正则化和频谱域功率谱正则化构建复合损失函数，并通过随机排列输入确保排列不变性。

Result: 该方法在生成建模中有效提升了美学质量和文本对齐效果，防止了奖励黑客行为，加速了收敛速度，性能优于现有高斯性正则化方法。

Conclusion: 提出的统一框架为高斯性正则化提供了更高效和全面的解决方案，在文本到图像模型的潜在空间优化任务中表现出色。

Abstract: We propose a novel regularization loss that enforces standard Gaussianity,
encouraging samples to align with a standard Gaussian distribution. This
facilitates a range of downstream tasks involving optimization in the latent
space of text-to-image models. We treat elements of a high-dimensional sample
as one-dimensional standard Gaussian variables and define a composite loss that
combines moment-based regularization in the spatial domain with power
spectrum-based regularization in the spectral domain. Since the expected values
of moments and power spectrum distributions are analytically known, the loss
promotes conformity to these properties. To ensure permutation invariance, the
losses are applied to randomly permuted inputs. Notably, existing
Gaussianity-based regularizations fall within our unified framework: some
correspond to moment losses of specific orders, while the previous
covariance-matching loss is equivalent to our spectral loss but incurs higher
time complexity due to its spatial-domain computation. We showcase the
application of our regularization in generative modeling for test-time reward
alignment with a text-to-image model, specifically to enhance aesthetics and
text alignment. Our regularization outperforms previous Gaussianity
regularization, effectively prevents reward hacking and accelerates
convergence.

</details>


### [16] [SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards](https://arxiv.org/abs/2509.07047)
*Kamyar Barakati,Utkarsh Pratiush,Sheryl L. Sanchez,Aditya Raghavan,Delia J. Milliron,Mahshid Ahmadi,Philip D. Rack,Sergei V. Kalinin*

Main category: cs.CV

TL;DR: 提出基于奖励函数的优化方法来微调基础分割模型SAM，通过物理驱动的奖励函数实现实时流数据分割，创建了优化版本SAM*


<details>
  <summary>Details</summary>
Motivation: 基础模型如SAM虽然通用性强，但包含大量不透明的调优参数需要手动优化，限制了在实时流数据分析中的可用性，特别是在显微镜成像等需要精确分割的领域

Method: 引入基于奖励函数的优化框架来微调基础模型，奖励函数可以构建为反映成像系统的物理特性（如粒子尺寸分布、几何形状等标准），通过奖励驱动的优化增强模型的适应性和性能

Result: 开发了优化变体SAM*，能够更好地满足多样化分割任务的需求，特别适用于实时流数据分割，在显微镜成像中表现出色

Conclusion: 奖励函数优化方法有效提升了基础分割模型在特定领域（特别是显微镜成像）的性能和实时处理能力，为复杂视觉数据分析提供了更实用的解决方案

Abstract: Image segmentation is a critical task in microscopy, essential for accurately
analyzing and interpreting complex visual data. This task can be performed
using custom models trained on domain-specific datasets, transfer learning from
pre-trained models, or foundational models that offer broad applicability.
However, foundational models often present a considerable number of
non-transparent tuning parameters that require extensive manual optimization,
limiting their usability for real-time streaming data analysis. Here, we
introduce a reward function-based optimization to fine-tune foundational models
and illustrate this approach for SAM (Segment Anything Model) framework by
Meta. The reward functions can be constructed to represent the physics of the
imaged system, including particle size distributions, geometries, and other
criteria. By integrating a reward-driven optimization framework, we enhance
SAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,
that better aligns with the requirements of diverse segmentation tasks and
particularly allows for real-time streaming data segmentation. We demonstrate
the effectiveness of this approach in microscopy imaging, where precise
segmentation is crucial for analyzing cellular structures, material interfaces,
and nanoscale features.

</details>


### [17] [Enhancing Classification of Streaming Data with Image Distillation](https://arxiv.org/abs/2509.07049)
*Rwad Khatib,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 本文提出了一种基于数据蒸馏的流数据分类方法(DBC)，在内存和计算资源有限的环境下，通过提取数据流中的关键特征，实现了73.1%的分类准确率，超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在有限内存和计算资源环境下高效分类流数据的挑战，特别是针对图像流数据分类精度提升的需求。

Method: 采用数据蒸馏方法提取数据流中的关键特征，与传统的Hoeffding Trees和Adaptive Random Forest算法进行比较，并通过嵌入技术适配图像数据。

Result: DBC方法达到73.1%的准确率，显著优于传统方法和Reservoir Sampling Based Classification技术。

Conclusion: 该方法在流数据分类领域取得了重要进展，在处理复杂数据流方面展现了高效性和准确性，为准确性和效率设立了新标准。

Abstract: This study tackles the challenge of efficiently classifying streaming data in
envi-ronments with limited memory and computational resources. It delves into
the application of data distillation as an innovative approach to improve the
precision of streaming image data classification. By focusing on distilling
essential features from data streams, our method aims to minimize computational
demands while preserving crucial information for accurate classification. Our
investigation com-pares this approach against traditional algorithms like
Hoeffding Trees and Adap-tive Random Forest, adapted through embeddings for
image data. The Distillation Based Classification (DBC) demonstrated superior
performance, achieving a 73.1% accuracy rate, surpassing both traditional
methods and Reservoir Sam-pling Based Classification (RBC) technique. This
marks a significant advance-ment in streaming data classification, showcasing
the effectiveness of our method in processing complex data streams and setting
a new standard for accuracy and efficiency.

</details>


### [18] [Automated Evaluation of Gender Bias Across 13 Large Multimodal Models](https://arxiv.org/abs/2509.07050)
*Juan Manuel Contreras*

Main category: cs.CV

TL;DR: 本文提出了Aymara图像公平性评估基准，用于评估AI生成图像中的社会偏见，发现大型多模态模型会系统性地放大职业性别刻板印象，且存在显著的默认男性偏见，但不同模型间的偏见程度差异很大。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在文本到图像生成方面取得了革命性进展，但存在延续训练数据中有害社会偏见的风险。先前研究虽然识别了性别偏见，但方法学限制阻碍了大规模、可比较的跨模型分析。

Method: 使用75个程序生成的性别中性提示词，测试13个商业可用的大型多模态模型，生成刻板男性、刻板女性和非刻板职业的人物图像，然后使用经过验证的LLM-as-a-judge系统对965张结果图像进行性别代表性评分。

Result: 研究发现：1）LMMs系统性地放大职业性别刻板印象，在男性刻板职业中生成93.0%男性图像，在女性刻板职业中只生成22.5%男性图像；2）模型存在强烈默认男性偏见，在非刻板职业中68.3%生成男性；3）不同模型偏见程度差异显著，总体男性代表性从46.7%到73.3%不等。

Conclusion: 研究提供了迄今为止最全面的跨模型性别偏见基准，强调了标准化自动化评估工具对于促进AI开发问责制和公平性的必要性，高偏见并非不可避免的结果而是设计选择的结果。

Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation,
but they risk perpetuating the harmful social biases in their training data.
Prior work has identified gender bias in these models, but methodological
limitations prevented large-scale, comparable, cross-model analysis. To address
this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for
assessing social bias in AI-generated images. We test 13 commercially available
LMMs using 75 procedurally-generated, gender-neutral prompts to generate people
in stereotypically-male, stereotypically-female, and non-stereotypical
professions. We then use a validated LLM-as-a-judge system to score the 965
resulting images for gender representation. Our results reveal (p < .001 for
all): 1) LMMs systematically not only reproduce but actually amplify
occupational gender stereotypes relative to real-world labor data, generating
men in 93.0% of images for male-stereotyped professions but only 22.5% for
female-stereotyped professions; 2) Models exhibit a strong default-male bias,
generating men in 68.3% of the time for non-stereotyped professions; and 3) The
extent of bias varies dramatically across models, with overall male
representation ranging from 46.7% to 73.3%. Notably, the top-performing model
de-amplified gender stereotypes and approached gender parity, achieving the
highest fairness scores. This variation suggests high bias is not an inevitable
outcome but a consequence of design choices. Our work provides the most
comprehensive cross-model benchmark of gender bias to date and underscores the
necessity of standardized, automated evaluation tools for promoting
accountability and fairness in AI development.

</details>


### [19] [Faster VGGT with Block-Sparse Global Attention](https://arxiv.org/abs/2509.07120)
*Chung-Shien Brian Wang,Christian Schmidt,Jens Piekenbrinck,Bastian Leibe*

Main category: cs.CV

TL;DR: 本文提出了一种基于块稀疏核的注意力机制替代方案，用于解决多视图重建中transformer模型的二次复杂度瓶颈问题，实现了4倍推理加速且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的transformer模型如VGGT和π³在多视图重建中表现出色，但全局注意力层的二次复杂度限制了其在大规模图像集上的可扩展性。研究发现注意力矩阵的概率质量集中在少量与跨视图几何匹配相关的patch交互上。

Method: 基于结构化注意力的观察，受大语言模型进展启发，提出使用高度优化的块稀疏核来替代密集的全局注意力操作，无需重新训练主干网络。

Result: 在多个多视图基准测试上的评估表明，该方法实现了高达4倍的推理加速，同时保持了可比较的任务性能，支持大规模图像集处理。

Conclusion: 提出的块稀疏注意力机制有效解决了transformer模型在多视图重建中的计算瓶颈问题，为VGGT和π³等模型提供了无需重新训练的高效改造方案。

Abstract: Efficient and accurate feed-forward multi-view reconstruction has long been
an important task in computer vision. Recent transformer-based models like VGGT
and $\pi^3$ have achieved impressive results with simple architectures, yet
they face an inherent runtime bottleneck, due to the quadratic complexity of
the global attention layers, that limits the scalability to large image sets.
In this paper, we empirically analyze the global attention matrix of these
models and observe that probability mass concentrates on a small subset of
patch-patch interactions that correspond to cross-view geometric matches.
Motivated by the structured attention and inspired by recent advancement in
large language models, we propose a replacement for the dense global attention
operation based on highly optimized block-sparse kernels, yielding up to
$4\times$ faster inference with comparable task performance. Our retrofit
requires no retraining of the backbone, extends to both VGGT and $\pi^3$, and
supports large image collections. Evaluations on a comprehensive suite of
multi-view benchmarks demonstrate the effectiveness of our approach.

</details>


### [20] [Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry](https://arxiv.org/abs/2509.07130)
*Soruya Saha,Md Nurul Absur,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 本文研究边缘服务器VIO系统中的位姿欺骗威胁，提出无监督检测与恢复机制，通过攻击自由会话学习运动规律来检测异常并恢复位姿一致性。


<details>
  <summary>Details</summary>
Motivation: 当前VIO系统向边缘服务器卸载的趋势导致服务器端存在安全威胁，细微的位姿欺骗可能累积成显著漂移，同时逃避启发式检查。

Method: 提出无监督、无标签的检测和恢复机制，在无攻击会话上训练模型学习运动时间规律，检测运行时偏差并启动恢复以保持位姿一致性。

Result: 在ILLIXR测试平台上评估，多种欺骗强度下的实验结果显示，相比无防御基线，轨迹和位姿误差显著降低。

Conclusion: 该方法能有效检测和恢复VIO系统中的位姿欺骗攻击，提高系统安全性和鲁棒性。

Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by
fusing camera and Inertial Measurement Unit (IMU) data for real-time pose.
However, current trend of offloading VIO to edge servers can lead server-side
threat surface where subtle pose spoofing can accumulate into substantial
drift, while evading heuristic checks. In this paper, we study this threat and
present an unsupervised, label-free detection and recovery mechanism. The
proposed model is trained on attack-free sessions to learn temporal
regularities of motion to detect runtime deviations and initiate recovery to
restore pose consistency. We evaluate the approach in a realistic offloaded-VIO
environment using ILLIXR testbed across multiple spoofing intensities.
Experimental results in terms of well-known performance metrics show
substantial reductions in trajectory and pose error compared to a no-defense
baseline.

</details>


### [21] [Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement](https://arxiv.org/abs/2509.07178)
*Muhammad Saad Saeed,Ijaz Ul Haq,Khalid Malik*

Main category: cs.CV

TL;DR: 人脸增强技术虽然能改善面部外观，但会扭曲生物特征，显著降低深度伪造检测器的准确率，最高攻击成功率可达75.12%。


<details>
  <summary>Details</summary>
Motivation: 研究假设人脸增强技术在提升感知质量的同时会降低深度伪造检测器的性能，旨在系统评估这些技术是否具有反取证作用。

Method: 使用传统图像处理方法和先进的GAN增强技术，在FaceForensics++、DeepFakeDetection和CelebDF-v2数据集上评估对Naïve、空间和频域检测方法的影响，并进行对抗训练实验。

Result: 基础增强滤镜可显著降低检测准确率（ASR达64.63%），GAN技术进一步利用这些漏洞（ASR达75.12%）。

Conclusion: 人脸增强方法能有效作为反取证工具，强调需要更具弹性和适应性的取证方法。

Abstract: Face enhancement techniques are widely used to enhance facial appearance.
However, they can inadvertently distort biometric features, leading to
significant decrease in the accuracy of deepfake detectors. This study
hypothesizes that these techniques, while improving perceptual quality, can
degrade the performance of deepfake detectors. To investigate this, we
systematically evaluate whether commonly used face enhancement methods can
serve an anti-forensic role by reducing detection accuracy. We use both
traditional image processing methods and advanced GAN-based enhancements to
evaluate the robustness of deepfake detectors. We provide a comprehensive
analysis of the effectiveness of these enhancement techniques, focusing on
their impact on Na\"ive, Spatial, and Frequency-based detection methods.
Furthermore, we conduct adversarial training experiments to assess whether
exposure to face enhancement transformations improves model robustness.
Experiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2
datasets indicate that even basic enhancement filters can significantly reduce
detection accuracy achieving ASR up to 64.63\%. In contrast, GAN-based
techniques further exploit these vulnerabilities, achieving ASR up to 75.12\%.
Our results demonstrate that face enhancement methods can effectively function
as anti-forensic tools, emphasizing the need for more resilient and adaptive
forensic methods.

</details>


### [22] [Dimensionally Reduced Open-World Clustering: DROWCULA](https://arxiv.org/abs/2509.07184)
*Erencem Ozbey,Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: 这篇论文提出了一种全无监督的方法，利用Vision Transformer和流形学习技术来发现图像数据中的新类别，在多个标准数据集上创造了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，标注数据需要大量人工工作，而且新类别可能会不断出现。虽然之前的研究多采用半监督方法，但本文说急地提出了全无监督方法来解决这个问题。

Method: 使用Vision Transformer产生向量嵌入，通过注意力机制估计聚类数量，并结合流形学习技术来精炼嵌入表征，利用数据的内在几何结构来提升图像聚类性能。

Result: 在CIFAR-10、CIFAR-100、ImageNet-100和Tiny ImageNet数据集上创造了单模态聚类和新类发现的新的State-of-the-Art结果，无论聚类数量是否预先知道。

Conclusion: 该方法提供了一种有效的全无监督方案来处理开放世界中的新类发现问题，为图像分类预处理提供了新的视角。

Abstract: Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.

</details>


### [23] [XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning](https://arxiv.org/abs/2509.07213)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: XBusNet是一种新颖的双提示双分支多模态模型，结合图像特征和临床文本提示，用于精确的乳腺超声分割，在BLU数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 精确的乳腺超声分割对于可靠测量和定量分析至关重要，但小病灶或低对比度病灶由于边缘模糊和斑点噪声而难以分割。文本提示可以提供临床背景，但直接应用弱定位的文本-图像线索往往会产生粗糙的分割结果。

Method: 提出XBusNet模型，包含全局通路（基于CLIP Vision Transformer编码整体图像语义）和局部通路（U-Net强调精确边界），通过描述形状、边缘和BI-RADS术语的文本提示进行调制。提示从结构化元数据自动组装，无需手动点击。

Result: 在BLU数据集上达到平均Dice系数0.8765和IoU 0.8149，优于六个强基线方法。小病灶获得最大提升，减少了遗漏区域和虚假激活。消融研究显示全局上下文、局部边界建模和提示调制的互补贡献。

Conclusion: 双提示双分支多模态设计将全局语义与局部精度相结合，能够产生准确的乳腺超声分割掩码，并提高对小而低对比度病灶的鲁棒性。

Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable
measurement, quantitative analysis, and downstream classification, yet remains
difficult for small or low-contrast lesions with fuzzy margins and speckle
noise. Text prompts can add clinical context, but directly applying weakly
localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce
coarse, blob-like responses that smear boundaries unless additional mechanisms
recover fine edges. Methods: We propose XBusNet, a novel dual-prompt,
dual-branch multimodal model that combines image features with clinically
grounded text. A global pathway based on a CLIP Vision Transformer encodes
whole-image semantics conditioned on lesion size and location, while a local
U-Net pathway emphasizes precise boundaries and is modulated by prompts that
describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)
terms. Prompts are assembled automatically from structured metadata, requiring
no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using
five-fold cross-validation. Primary metrics are Dice and Intersection over
Union (IoU); we also conduct size-stratified analyses and ablations to assess
the roles of the global and local paths and the text-driven modulation.
Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice
of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions
show the largest gains, with fewer missed regions and fewer spurious
activations. Ablation studies show complementary contributions of global
context, local boundary modeling, and prompt-based modulation. Conclusions: A
dual-prompt, dual-branch multimodal design that merges global semantics with
local precision yields accurate BUS segmentation masks and improves robustness
for small, low-contrast lesions.

</details>


### [24] [Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion](https://arxiv.org/abs/2509.07277)
*Sepehr Salem,M. Moein Esfahani,Jingyu Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 通过激光散射模型进行数据增帽，结合ResNet-50深度特征与手工特征融合，在乳腺癌热成像分类中达到98%准确率


<details>
  <summary>Details</summary>
Motivation: 解决医学形象领域深度学习数据稀缺问题，提高乳腺癌热成像分类的准确性

Method: 使用激光散射模型(DPM)进行数据增帽，融合ResNet-50深度特征和U-Net分割肿瘤提取的分形维度等非线性手工特征，使用XGBoost分类器

Result: 达到98.0%准确率和98.1%敏感度，显著超越传统方法和ProGAN基线

Conclusion: 高级生成模型与可解释特征的协同作用能构建高准确医疗诊断工具

Abstract: Data scarcity hinders deep learning for medical imaging. We propose a
framework for breast cancer classification in thermograms that addresses this
using a Diffusion Probabilistic Model (DPM) for data augmentation. Our
DPM-based augmentation is shown to be superior to both traditional methods and
a ProGAN baseline. The framework fuses deep features from a pre-trained
ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived
from U-Net segmented tumors. An XGBoost classifier trained on these fused
features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and
statistical tests confirm that both the DPM augmentation and the nonlinear
feature fusion are critical, statistically significant components of this
success. This work validates the synergy between advanced generative models and
interpretable features for creating highly accurate medical diagnostic tools.

</details>


### [25] [Reconstruction Alignment Improves Unified Multimodal Models](https://arxiv.org/abs/2509.07295)
*Ji Xie,Trevor Darrell,Luke Zettlemoyer,XuDong Wang*

Main category: cs.CV

TL;DR: RecA是一种资源高效的后训练方法，通过视觉理解编码器嵌入作为密集文本提示，无需标注即可提供丰富监督，显著提升多模态模型的生成和编辑性能


<details>
  <summary>Details</summary>
Motivation: 传统多模态模型训练依赖图像-文本对，但文本标注通常稀疏且缺乏细粒度视觉细节，导致理解与生成之间的对齐不足

Method: RecA利用模型自身的视觉理解嵌入作为条件，通过自监督重建损失优化图像重建，实现理解与生成的对齐

Result: 仅用27 GPU小时，在GenEval(0.73→0.90)和DPGBench(80.93→88.15)上大幅提升生成性能，编辑基准也有显著改善

Conclusion: RecA是一种高效通用的后训练对齐策略，适用于多种UMM架构，性能超越更大的开源模型

Abstract: Unified multimodal models (UMMs) unify visual understanding and generation
within a single architecture. However, conventional training relies on
image-text pairs (or sequences) whose captions are typically sparse and miss
fine-grained visual details--even when they use hundreds of words to describe a
simple image. We introduce Reconstruction Alignment (RecA), a
resource-efficient post-training method that leverages visual understanding
encoder embeddings as dense "text prompts," providing rich supervision without
captions. Concretely, RecA conditions a UMM on its own visual understanding
embeddings and optimizes it to reconstruct the input image with a
self-supervised reconstruction loss, thereby realigning understanding and
generation. Despite its simplicity, RecA is broadly applicable: across
autoregressive, masked-autoregressive, and diffusion-based UMMs, it
consistently improves generation and editing fidelity. With only 27 GPU-hours,
post-training with RecA substantially improves image generation performance on
GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while
also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit
6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models
and applies broadly across diverse UMM architectures, establishing it as an
efficient and general post-training alignment strategy for UMMs

</details>


### [26] [DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion](https://arxiv.org/abs/2509.07327)
*Shucong Li,Zhenyu Liu,Zijie Hong,Zhiheng Zhou,Xianghai Cao*

Main category: cs.CV

TL;DR: 提出DEPF方法，通过双域增强和优先级引导的Mamba融合解决无人机多光谱目标检测中的低光照图像质量差、局部小目标建模干扰和计算复杂度高的问题


<details>
  <summary>Details</summary>
Motivation: 解决无人机多光谱遥感目标检测面临的三个挑战：低光照图像降低多模态融合互补性、融合阶段冗余信息干扰局部小目标建模、基于Transformer的方法计算复杂度高难以在无人机平台应用

Method: 设计双域增强模块(DDE)包含跨尺度小波Mamba(CSWM)和傅里叶细节恢复块(FDR)来增强低光照图像；设计优先级引导Mamba融合模块(PGMF)通过优先级扫描从局部目标特征开始融合，减少冗余信息影响

Result: 在DroneVehicle和VEDAI数据集上的实验表明，DEPF在目标检测任务上表现优于最先进的方法

Conclusion: 提出的DEPF方法有效解决了无人机多光谱目标检测的关键挑战，通过双域增强和优先级引导的Mamba融合实现了更好的检测性能

Abstract: Multispectral remote sensing object detection is one of the important
application of unmanned aerial vehicle (UAV). However, it faces three
challenges. Firstly, the low-light remote sensing images reduce the
complementarity during multi-modality fusion. Secondly, the local small target
modeling is interfered with redundant information in the fusion stage easily.
Thirdly, due to the quadratic computational complexity, it is hard to apply the
transformer-based methods on the UAV platform. To address these limitations,
motivated by Mamba with linear complexity, a UAV multispectral object detector
with dual-domain enhancement and priority-guided mamba fusion (DEPF) is
proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain
Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba
(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba
scanning for the low-frequency components to enhance the global brightness of
images, while FDR constructs spectrum recovery network to enhance the frequency
spectra features for recovering the texture-details. Secondly, to enhance local
target modeling and reduce the impact of redundant information during fusion,
Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the
concept of priority scanning, which starts from local targets features
according to the priority scores obtained from modality difference. Experiments
on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on
object detection, comparing with state-of-the-art methods. Our code is
available in the supplementary material.

</details>


### [27] [G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2509.07335)
*Haiqing Ren,Zhongkai Luo,Heng Fan,Xiaohui Yuan,Guanchen Wang,Libo Zhang*

Main category: cs.CV

TL;DR: 提出G$^{3}$CN方法，通过高斯滤波优化骨架拓扑图和门控循环单元增强信息传播，有效提升骨架动作识别中模糊动作的区分能力


<details>
  <summary>Details</summary>
Motivation: 现有图卷积网络在骨架动作识别中对模糊动作的区分能力有限，学习到的拓扑和空间特征表示存在不足

Method: 结合高斯滤波器优化骨架拓扑图表示，并集成门控循环单元(GRU)到GCN框架中增强骨架点间的信息传播

Result: 在NTU RGB+D、NTU RGB+D 120和NW-UCLA基准测试中显著提升动作识别性能，特别是对模糊样本效果明显

Conclusion: G$^{3}$CN方法通过拓扑优化和信息传播增强，有效解决了骨架动作识别中模糊动作区分难题，具有良好的泛化性能

Abstract: Graph Convolutional Networks (GCNs) have proven to be highly effective for
skeleton-based action recognition, primarily due to their ability to leverage
graph topology for feature aggregation, a key factor in extracting meaningful
representations. However, despite their success, GCNs often struggle to
effectively distinguish between ambiguous actions, revealing limitations in the
representation of learned topological and spatial features. To address this
challenge, we propose a novel approach, Gaussian Topology Refinement Gated
Graph Convolution (G$^{3}$CN), to address the challenge of distinguishing
ambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates
a Gaussian filter to refine the skeleton topology graph, improving the
representation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs)
are integrated into the GCN framework to enhance information propagation
between skeleton points. Our method shows strong generalization across various
GCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA
benchmarks demonstrate that G$^{3}$CN effectively improves action recognition,
particularly for ambiguous samples.

</details>


### [28] [Parse Graph-Based Visual-Language Interaction for Human Pose Estimation](https://arxiv.org/abs/2509.07385)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

TL;DR: 提出了基于解析图的视觉-语言交互模型PGVL，通过引导模块实现多模态融合，提升遮挡场景下的人体姿态估计性能


<details>
  <summary>Details</summary>
Motivation: 现有解析图方法主要关注单模态建模，忽略了多模态融合的潜力。语言提供了丰富的空间关系先验，但现有的全局特征融合方法会削弱遮挡区域响应并导致对齐和定位失败

Method: 提出PGVL模型，包含引导模块(GM)。低层节点关注局部特征保持遮挡区域响应，高层节点整合全局特征推断遮挡部位。采用自上而下分解和自下而上组合的两阶段方法，第一阶段构建模态特定解析图，第二阶段使用递归双向交叉注意力并通过GM进行净化

Result: 在主要姿态估计数据集上验证了PGVL和相应网络的有效性

Conclusion: PGVL通过创新的引导模块实现了有效的多模态信息融合，特别适用于处理遮挡场景下的人体姿态估计问题

Abstract: Parse graphs boost human pose estimation (HPE) by integrating context and
hierarchies, yet prior work mostly focuses on single modality modeling,
ignoring the potential of multimodal fusion. Notably, language offers rich HPE
priors like spatial relations for occluded scenes, but existing visual-language
fusion via global feature integration weakens occluded region responses and
causes alignment and location failures. To address this issue, we propose Parse
Graph-based Visual-Language interaction (PGVL) with a core novel Guided Module
(GM). In PGVL, low-level nodes focus on local features, maximizing the
maintenance of responses in occluded areas and high-level nodes integrate
global features to infer occluded or invisible parts. GM enables high semantic
nodes to guide the feature update of low semantic nodes that have undergone
cross attention. It ensuring effective fusion of diverse information. PGVL
includes top-down decomposition and bottom-up composition. In the first stage,
modality specific parse graphs are constructed. Next stage. recursive
bidirectional cross-attention is used, purified by GM. We also design network
based on PGVL. The PGVL and our network is validated on major pose estimation
datasets. We will release the code soon.

</details>


### [29] [DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation](https://arxiv.org/abs/2509.07435)
*Ze-Xin Yin,Jiaxiong Qiu,Liu Liu,Xinjie Wang,Wei Sui,Zhizhong Su,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: LGAA是一个轻量级高斯资产适配器框架，通过多视角扩散先验统一建模几何和PBR材质，实现端到端的PBR就绪3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 传统3D资产创建需要大量人工和经验，现有方法主要关注几何建模而忽略材质合成，需要实现端到端的PBR就绪3D生成。

Method: 采用模块化设计：LGAA Wrapper重用多视角扩散模型层，LGAA Switcher对齐多个扩散先验，LGAA Decoder预测带PBR通道的2D高斯溅射，最后通过后处理提取高质量可重光照网格资产。

Result: 实验证明LGAA在文本和图像条件多视角扩散模型中表现优异，模块化设计支持灵活集成多个扩散先验，仅需69k多视角实例即可高效收敛。

Conclusion: LGAA成功实现了端到端的PBR就绪3D资产生成，统一了几何和材质的建模，具有高效收敛和灵活扩展的优势。

Abstract: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

</details>


### [30] [In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting](https://arxiv.org/abs/2509.07447)
*Taiying Peng,Jiacheng Hua,Miao Liu,Feng Lu*

Main category: cs.CV

TL;DR: 提出了EgoGazeVQA基准测试，利用注视信息改进多模态大语言模型对第一人称长视频的理解，通过注视引导的意图提示方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽视了注视作为用户意图指示器的重要作用，第一人称视频通过统一坐标系直接捕捉用户焦点、动作和上下文，为MLLMs提供实现主动个性化AI用户体验的机会

Method: 构建包含注视信息的QA对数据集，开发注视引导的意图提示方法，整合空间、时间和意图相关线索，并进行注视相关的微调实验

Result: 现有MLLMs难以准确解释用户意图，而注视引导方法显著提升性能，注视估计准确性影响提示效果

Conclusion: 注视信息对于在第一人称场景中实现更个性化和有效的AI助手具有重要价值

Abstract: The emergence of advanced multimodal large language models (MLLMs) has
significantly enhanced AI assistants' ability to process complex information
across modalities. Recently, egocentric videos, by directly capturing user
focus, actions, and context in an unified coordinate, offer an exciting
opportunity to enable proactive and personalized AI user experiences with
MLLMs. However, existing benchmarks overlook the crucial role of gaze as an
indicator of user intent. To address this gap, we introduce EgoGazeVQA, an
egocentric gaze-guided video question answering benchmark that leverages gaze
information to improve the understanding of longer daily-life videos.
EgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by
human annotators. Our experiments reveal that existing MLLMs struggle to
accurately interpret user intentions. In contrast, our gaze-guided intent
prompting methods significantly enhance performance by integrating spatial,
temporal, and intent-related cues. We further conduct experiments on
gaze-related fine-tuning and analyze how gaze estimation accuracy impacts
prompting effectiveness. These results underscore the value of gaze for more
personalized and effective AI assistants in egocentric settings.

</details>


### [31] [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
*Xudong Lu,Zhi Zheng,Yi Wan,Yongxiang Yao,Annan Wang,Renrui Zhang,Panwang Xia,Qiong Wu,Qingyun Li,Weifeng Lin,Xiangyu Zhao,Xue Yang,Hongsheng Li*

Main category: cs.CV

TL;DR: GLEAM-C是一个统一多视角多模态的跨视角地理定位基础模型，通过卫星图像对齐无人机、街景地图、全景图和地面照片。GLEAM-X结合了跨视角对应预测与可解释推理，利用多模态大语言模型提供匹配理由。


<details>
  <summary>Details</summary>
Motivation: 解决现有跨视角地理定位方法局限于单一视角或模态，且缺乏可解释性（仅预测是否匹配而不解释原因）的问题。

Method: GLEAM-C采用两阶段训练策略，通过卫星图像统一对齐多种视图和模态。GLEAM-X利用多模态大语言模型进行可解释推理，并使用GPT-4o和Doubao-1.5-Thinking-Vision-Pro构建双语基准数据集。

Result: GLEAM-C实现了与先前模态特定模型相当的准确性，同时提高了训练效率。GLEAM-X提供了系统化的可解释跨视角推理评估框架。

Conclusion: GLEAM-C和GLEAM-X形成了一个完整的跨视角地理定位流程，将多模态多视角对齐与可解释对应分析相结合，统一了准确的跨视角匹配与可解释推理，推动了地理定位的透明度和可扩展性。

Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences
between images captured from distinct perspectives of the same geographical
location. However, existing CVGL approaches are typically restricted to a
single view or modality, and their direct visual matching strategy lacks
interpretability: they merely predict whether two images correspond, without
explaining the rationale behind the match. In this paper, we present GLEAM-C, a
foundational CVGL model that unifies multiple views and modalities-including
UAV imagery, street maps, panoramic views, and ground photographs-by aligning
them exclusively with satellite imagery. Our framework enhances training
efficiency through optimized implementation while achieving accuracy comparable
to prior modality-specific CVGL models through a two-phase training strategy.
Moreover, to address the lack of interpretability in traditional CVGL methods,
we leverage the reasoning capabilities of multimodal large language models
(MLLMs) to propose a new task, GLEAM-X, which combines cross-view
correspondence prediction with explainable reasoning. To support this task, we
construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro
to generate training and testing data. The test set is further refined through
detailed human revision, enabling systematic evaluation of explainable
cross-view reasoning and advancing transparency and scalability in
geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL
pipeline that integrates multi-modal, multi-view alignment with interpretable
correspondence analysis, unifying accurate cross-view matching with explainable
reasoning and advancing Geo-Localization by enabling models to better Explain
And Match. Code and datasets used in this work will be made publicly accessible
at https://github.com/Lucky-Lance/GLEAM.

</details>


### [32] [XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning](https://arxiv.org/abs/2509.07455)
*Pooya Khosravi,Kun Han,Anthony T. Wu,Arghavan Rezvani,Zexin Feng,Xiaohui Xie*

Main category: cs.CV

TL;DR: XOCT是一个深度学习框架，通过跨维度监督和多尺度特征融合，从OCT图像生成高质量的OCTA血管图像，解决了现有方法忽略视网膜层间血管差异和细节重建不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统OCTA图像获取困难且成本高，现有深度学习方法在重建视网膜各层血管细节方面存在不足，无法满足临床诊断需求。

Method: 提出跨维度监督模块利用分层en-face投影作为监督信号，结合多尺度特征融合模块通过通道重加权策略增强血管细节提取。

Result: 在OCTA-500数据集上验证了XOCT的优越性能，特别是在对临床评估至关重要的en-face投影方面表现显著提升。

Conclusion: XOCT框架能够提高OCTA的可及性、可靠性和诊断价值，有望增强眼科疾病的检测和监测能力。

Abstract: Optical Coherence Tomography Angiography (OCTA) and its derived en-face
projections provide high-resolution visualization of the retinal and choroidal
vasculature, which is critical for the rapid and accurate diagnosis of retinal
diseases. However, acquiring high-quality OCTA images is challenging due to
motion sensitivity and the high costs associated with software modifications
for conventional OCT devices. Moreover, current deep learning methods for
OCT-to-OCTA translation often overlook the vascular differences across retinal
layers and struggle to reconstruct the intricate, dense vascular details
necessary for reliable diagnosis. To overcome these limitations, we propose
XOCT, a novel deep learning framework that integrates Cross-Dimensional
Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for
layer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise
en-face projections, generated via segmentation-weighted z-axis averaging, as
supervisory signals to compel the network to learn distinct representations for
each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF
module enhances vessel delineation through multi-scale feature extraction
combined with a channel reweighting strategy, effectively capturing vascular
details at multiple spatial scales. Our experiments on the OCTA-500 dataset
demonstrate XOCT's improvements, especially for the en-face projections which
are significant for clinical evaluation of retinal pathologies, underscoring
its potential to enhance OCTA accessibility, reliability, and diagnostic value
for ophthalmic disease detection and monitoring. The code is available at
https://github.com/uci-cbcl/XOCT.

</details>


### [33] [Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting](https://arxiv.org/abs/2509.07456)
*Sai Siddhartha Chary Aylapuram,Veeraraju Elluru,Shivang Agarwal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于机器遗忘技术的后处理偏置缓解方法，通过选择性移除偏置样本或特征表示来提升视觉模型的公平性，在多个数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练数据中容易依赖虚假相关性，导致在医疗和自动驾驶等安全关键领域产生有偏或不公平的预测。传统偏置缓解方法需要重新训练或重新设计数据管道，而机器遗忘技术提供了一种有前景的后处理替代方案。

Method: 基于隐私保护的遗忘技术，评估了多种策略包括梯度上升、LoRA和师生蒸馏方法。在三个基准数据集（CUB-200-2011、CIFAR-10和CelebA）上进行实证分析，针对不同形式的偏置（姿态偏置、合成补丁偏置、性别偏置）进行测试。

Result: 后处理遗忘方法显著减少了子组差异，在CUB-200上人口统计公平性提升94.86%，CIFAR-10上提升30.28%，CelebA上提升97.37%。这些改进在准确性损失最小的情况下实现，方法在效用、公平性、质量和隐私的联合评估中平均得分0.62。

Conclusion: 研究结果表明机器遗忘是一个实用的框架，可以在不需要完全重新训练的情况下增强已部署视觉系统的公平性。

Abstract: Deep neural networks often rely on spurious correlations in training data,
leading to biased or unfair predictions in safety-critical domains such as
medicine and autonomous driving. While conventional bias mitigation typically
requires retraining from scratch or redesigning data pipelines, recent advances
in machine unlearning provide a promising alternative for post-hoc model
correction. In this work, we investigate \textit{Bias-Aware Machine
Unlearning}, a paradigm that selectively removes biased samples or feature
representations to mitigate diverse forms of bias in vision models. Building on
privacy-preserving unlearning techniques, we evaluate various strategies
including Gradient Ascent, LoRA, and Teacher-Student distillation. Through
empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),
CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),
we demonstrate that post-hoc unlearning can substantially reduce subgroup
disparities, with improvements in demographic parity of up to \textbf{94.86\%}
on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These
gains are achieved with minimal accuracy loss and with methods scoring an
average of 0.62 across the 3 settings on the joint evaluation of utility,
fairness, quality, and privacy. Our findings establish machine unlearning as a
practical framework for enhancing fairness in deployed vision systems without
necessitating full retraining.

</details>


### [34] [ANYPORTAL: Zero-Shot Consistent Video Background Replacement](https://arxiv.org/abs/2509.07472)
*Wenshuo Gao,Xicheng Lan,Shuai Yang*

Main category: cs.CV

TL;DR: ANYPORTAL是一个零样本视频背景替换框架，利用预训练扩散模型实现高质量视频编辑，无需训练即可保持前景一致性和时间连贯性


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术难以实现细粒度控制，无法精确满足用户意图，特别是在视频背景替换时存在前景一致性和光照协调性的挑战

Method: 提出零样本框架，协同整合视频扩散模型的时间先验和图像扩散模型的重新光照能力，采用精炼投影算法实现像素级细节操作

Result: 在消费级GPU上实现高质量结果，有效解决了前景一致性和时间连贯重新光照的挑战

Conclusion: ANYPORTAL提供了一个实用高效的视频内容创作和编辑解决方案，无需训练即可实现精确的视频背景替换

Abstract: Despite the rapid advancements in video generation technology, creating
high-quality videos that precisely align with user intentions remains a
significant challenge. Existing methods often fail to achieve fine-grained
control over video details, limiting their practical applicability. We
introduce ANYPORTAL, a novel zero-shot framework for video background
replacement that leverages pre-trained diffusion models. Our framework
collaboratively integrates the temporal prior of video diffusion models with
the relighting capabilities of image diffusion models in a zero-shot setting.
To address the critical challenge of foreground consistency, we propose a
Refinement Projection Algorithm, which enables pixel-level detail manipulation
to ensure precise foreground preservation. ANYPORTAL is training-free and
overcomes the challenges of achieving foreground consistency and temporally
coherent relighting. Experimental results demonstrate that ANYPORTAL achieves
high-quality results on consumer-grade GPUs, offering a practical and efficient
solution for video content creation and editing.

</details>


### [35] [MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification](https://arxiv.org/abs/2509.07477)
*Patrick Wienholt,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: MedicalPatchNet是一种自解释的胸部X光分类架构，通过将图像分割成独立分类的补丁来提供透明决策归因，在保持分类性能的同时显著提升了解释性和病理定位准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在放射图像分类中表现出色但解释性差，限制了临床接受度。需要开发能够透明归因决策到图像特定区域的自解释架构。

Method: 将图像分割成不重叠的补丁，独立分类每个补丁，然后聚合预测结果，无需后处理技术即可直观可视化每个补丁的诊断贡献。

Result: 在CheXpert数据集上训练，分类性能与EfficientNet-B0相当（AUROC 0.907 vs 0.908），但解释性显著提升：在CheXlocalize数据集上病理定位准确率更高（平均命中率0.485 vs Grad-CAM的0.376）。

Conclusion: MedicalPatchNet通过提供明确可靠的可视化解释，减轻了捷径学习的风险，提高了临床信任度，为医学影像领域提供了更安全的可解释AI辅助诊断方案。

Abstract: Deep neural networks excel in radiological image classification but
frequently suffer from poor interpretability, limiting clinical acceptance. We
present MedicalPatchNet, an inherently self-explainable architecture for chest
X-ray classification that transparently attributes decisions to distinct image
regions. MedicalPatchNet splits images into non-overlapping patches,
independently classifies each patch, and aggregates predictions, enabling
intuitive visualization of each patch's diagnostic contribution without
post-hoc techniques. Trained on the CheXpert dataset (223,414 images),
MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)
of EfficientNet-B0, while substantially improving interpretability:
MedicalPatchNet demonstrates substantially improved interpretability with
higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with
Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable
explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks
associated with shortcut learning, thus improving clinical trust. Our model is
publicly available with reproducible training and inference scripts and
contributes to safer, explainable AI-assisted diagnostics across medical
imaging domains. We make the code publicly available:
https://github.com/TruhnLab/MedicalPatchNet

</details>


### [36] [LINR Bridge: Vector Graphic Animation via Neural Implicits and Video Diffusion Priors](https://arxiv.org/abs/2509.07484)
*Wenshuo Gao,Xicheng Lan,Luyao Zhang,Shuai Yang*

Main category: cs.CV

TL;DR: 通过隐式神经表示和文本到视频激浓模型的结合，自动化生成高质量的向量图形动画，充分保留向量图形的无限分辨率和精确控制特性。


<details>
  <summary>Details</summary>
Motivation: 向量图形动画能提升可理解性和可控性，但传统手动制作耗时耗力，需要自动化方案来解决灵活性和动画质量的限制问题。

Method: 采用分层隐式神经表示重建向量图形，通过视频评分蓄核核学习优化神经表示，利用预训练文本到视频激浓模型的运动先验知识，最后将向量图形变形以匹配神经表示生成平滑动画。

Result: 实验结果验证了方法的有效性，能够生成生动自然的向量图形动画，在灵活性和动画质量方面显著改善现有技术的限制。

Conclusion: 该方法成功地垩平了向量图形与激浓模型之间的领域差距，为向量图形动画的自动化生成提供了高效的解决方案。

Abstract: Vector graphics, known for their scalability and user-friendliness, provide a
unique approach to visual content compared to traditional pixel-based images.
Animation of these graphics, driven by the motion of their elements, offers
enhanced comprehensibility and controllability but often requires substantial
manual effort. To automate this process, we propose a novel method that
integrates implicit neural representations with text-to-video diffusion models
for vector graphic animation. Our approach employs layered implicit neural
representations to reconstruct vector graphics, preserving their inherent
properties such as infinite resolution and precise color and shape constraints,
which effectively bridges the large domain gap between vector graphics and
diffusion models. The neural representations are then optimized using video
score distillation sampling, which leverages motion priors from pretrained
text-to-video diffusion models. Finally, the vector graphics are warped to
match the representations resulting in smooth animation. Experimental results
validate the effectiveness of our method in generating vivid and natural vector
graphic animations, demonstrating significant improvement over existing
techniques that suffer from limitations in flexibility and animation quality.

</details>


### [37] [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/abs/2509.07488)
*Xiao Li,Bharat Gandhi,Ming Zhan,Mohit Nehra,Zhicheng Zhang,Yuchen Sun,Meijia Song,Naisheng Zhang,Xi Wang*

Main category: cs.CV

TL;DR: 基于视觉-语言模型的室内导航系统，通过微调BLIP-2模型和改进评估指标，为视障人士提供逐步导航指令


<details>
  <summary>Details</summary>
Motivation: 解决传统导航系统在室内环境中因缺乏精确定位数据而失效的问题，帮助视障人士提高室内导航的可达性和独立性

Method: 使用低秩适应(LoRA)方法在手动标注的室内导航数据集上微调BLIP-2模型，并提出改进的BERT F1评估指标，强调方向和顺序变量

Result: 应用LoRA后，模型在生成方向性指令方面显著改善，克服了原始BLIP-2模型的局限性

Conclusion: 该视觉-语言驱动方法有效提升了室内导航性能，为视障人士提供了更可靠的导航辅助

Abstract: We address vision-language-driven indoor navigation to assist visually
impaired individuals in reaching a target location using images and natural
language guidance. Traditional navigation systems are ineffective indoors due
to the lack of precise location data. Our approach integrates vision and
language models to generate step-by-step navigational instructions, enhancing
accessibility and independence. We fine-tune the BLIP-2 model with Low Rank
Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose
an evaluation metric that refines the BERT F1 score by emphasizing directional
and sequential variables, providing a more comprehensive measure of
navigational performance. After applying LoRA, the model significantly improved
in generating directional instructions, overcoming limitations in the original
BLIP-2 model.

</details>


### [38] [Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition](https://arxiv.org/abs/2509.07495)
*Chun Liu,Hailong Wang,Bingqian Zhu,Panpan Ding,Zheng Zheng,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 本文提出一种通过局部混合和logits优化的新框架，用于提高深度神经网络在遥感应用中的对抗性攻击转移性。


<details>
  <summary>Details</summary>
Motivation: 当前的混合基深度学习攻击策略存在两个主要问题：全局混合或区域交换可能破坏全局语义特征，以及交叉熵损失导致的梯度消失问题影响对抗示例质量。

Method: 提出三个核心技术：1)局部混合策略生成多样但语义一致的输入；2)将目标攻击中的logit损失适配到非目标攻击场景；3)应用波动平滑损失来压制高频噪声。

Result: 在FGSCR-42和MTARSI数据集上进行的实验显示，该方法在6个代理模型上超过12种最新方法。在MTARSI上以ResNet为代理模型时，黑盒攻击成功率平均提高17.28%。

Conclusion: 该方法通过保持全局语义特征和解决梯度消失问题，有效提高了对抗性攻击的转移性，为遥感应用中的深度神经网络安全部署提供了有力支撑。

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing
significant security threats to their deployment in remote sensing
applications. Research on adversarial attacks not only reveals model
vulnerabilities but also provides critical insights for enhancing robustness.
Although current mixing-based strategies have been proposed to increase the
transferability of adversarial examples, they either perform global blending or
directly exchange a region in the images, which may destroy global semantic
features and mislead the optimization of adversarial examples. Furthermore,
their reliance on cross-entropy loss for perturbation optimization leads to
gradient diminishing during iterative updates, compromising adversarial example
quality. To address these limitations, we focus on non-targeted attacks and
propose a novel framework via local mixing and logits optimization. First, we
present a local mixing strategy to generate diverse yet semantically consistent
inputs. Different from MixUp, which globally blends two images, and MixCut,
which stitches images together, our method merely blends local regions to
preserve global semantic information. Second, we adapt the logit loss from
targeted attacks to non-targeted scenarios, mitigating the gradient vanishing
problem of cross-entropy loss. Third, a perturbation smoothing loss is applied
to suppress high-frequency noise and enhance transferability. Extensive
experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance
over 12 state-of-the-art methods across 6 surrogate models. Notably, with
ResNet as the surrogate on MTARSI, our method achieves a 17.28% average
improvement in black-box attack success rate.

</details>


### [39] [MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection](https://arxiv.org/abs/2509.07507)
*Saad Lahlali,Alexandre Fournier Montgieux,Nicolas Granger,Hervé Le Borgne,Quoc Cuong Pham*

Main category: cs.CV

TL;DR: MVAT是一个新颖的弱监督3D目标检测框架，利用时序多视图信息解决2D标注到3D检测的投影模糊问题，通过教师-学生蒸馏范式实现无需3D标注的高性能检测。


<details>
  <summary>Details</summary>
Motivation: 3D数据标注成本高昂，而仅依赖2D框标注存在投影模糊和部分可见性问题，需要开发能够利用时序多视图信息的弱监督方法。

Method: 提出MVAT框架：1) 跨时间聚合目标点云构建完整3D表示；2) 采用教师-学生蒸馏范式，教师网络从单视图学习但目标来自时间聚合的静态对象；3) 引入多视图2D投影损失确保3D框与所有可用2D标注的一致性。

Result: 在nuScenes和Waymo Open数据集上达到最先进的弱监督3D目标检测性能，显著缩小了与全监督方法的差距，且无需任何3D框标注。

Conclusion: MVAT通过有效利用时序多视图信息，成功解决了弱监督3D检测中的投影模糊问题，为降低3D标注成本提供了有效解决方案。

Abstract: Annotating 3D data remains a costly bottleneck for 3D object detection,
motivating the development of weakly supervised annotation methods that rely on
more accessible 2D box annotations. However, relying solely on 2D boxes
introduces projection ambiguities since a single 2D box can correspond to
multiple valid 3D poses. Furthermore, partial object visibility under a single
viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges. Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible. A Teacher-Student distillation paradigm is employed: The Teacher
network learns from single viewpoints but targets are derived from temporally
aggregated static objects. Then the Teacher generates high quality
pseudo-labels that the Student learns to predict from a single viewpoint for
both static and moving objects. The whole framework incorporates a multi-view
2D projection loss to enforce consistency between predicted 3D boxes and all
available 2D annotations. Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations. % \footnote{Code
available upon acceptance} Our code is available in our public repository
(\href{https://github.com/CEA-LIST/MVAT}{code}).

</details>


### [40] [EHWGesture -- A dataset for multimodal understanding of clinical gestures](https://arxiv.org/abs/2509.07525)
*Gianluca Amprimo,Alberto Ancilotto,Alessandro Savino,Fabio Quazzolo,Claudia Ferraris,Gabriella Olmo,Elisabetta Farella,Stefano Di Carlo*

Main category: cs.CV

TL;DR: EHWGesture是一个多模态手势理解数据集，包含5种临床相关手势，1100+条记录（6小时），来自25名健康受试者，使用RGB-Depth相机和事件相机采集，配有精确的动作捕捉系统进行手部关键点追踪。


<details>
  <summary>Details</summary>
Motivation: 动态手势理解在人机交互和临床手部灵活性评估中很重要，但现有数据集缺乏多模态多视角多样性、精确的真值追踪以及嵌入动作质量评估的能力。

Method: 使用两台高分辨率RGB-Depth相机和事件相机采集数据，通过动作捕捉系统提供精确的手部地标追踪，所有设备经过空间校准和时间同步以确保跨模态对齐。

Result: 数据集支持手势分类、手势触发检测和动作质量评估等基线实验，展示了其在多模态临床手势理解方面的潜力。

Conclusion: EHWGesture可作为推进多模态临床手势理解的综合基准数据集。

Abstract: Hand gesture understanding is essential for several applications in
human-computer interaction, including automatic clinical assessment of hand
dexterity. While deep learning has advanced static gesture recognition, dynamic
gesture understanding remains challenging due to complex spatiotemporal
variations. Moreover, existing datasets often lack multimodal and multi-view
diversity, precise ground-truth tracking, and an action quality component
embedded within gestures. This paper introduces EHWGesture, a multimodal video
dataset for gesture understanding featuring five clinically relevant gestures.
It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects
using two high-resolution RGB-Depth cameras and an event camera. A motion
capture system provides precise ground-truth hand landmark tracking, and all
devices are spatially calibrated and synchronized to ensure cross-modal
alignment. Moreover, to embed an action quality task within gesture
understanding, collected recordings are organized in classes of execution speed
that mirror clinical evaluations of hand dexterity. Baseline experiments
highlight the dataset's potential for gesture classification, gesture trigger
detection, and action quality assessment. Thus, EHWGesture can serve as a
comprehensive benchmark for advancing multimodal clinical gesture
understanding.

</details>


### [41] [Universal Few-Shot Spatial Control for Diffusion Models](https://arxiv.org/abs/2509.07530)
*Kiet T. Nguyen,Chanhuyk Lee,Donggyun Kim,Dong Hoon Lee,Seunghoon Hong*

Main category: cs.CV

TL;DR: UFC是一个通用的少样本控制适配器，能够通过少量样本快速适应新的空间控制任务，在多种扩散模型架构上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的空间控制适配器在面对与训练任务差异较大的新空间控制条件时，适应性有限且训练成本高昂。

Method: 提出UFC方法，通过查询条件和支持条件之间的类比来构建任务特定的控制特征，使用匹配机制和小型任务特定参数更新。

Result: 在6个新空间控制任务上，仅用30个标注样本微调即可实现精细控制；使用0.1%的全训练数据即可达到与全监督基线相当的性能。

Conclusion: UFC是一个高效通用的少样本空间控制适配器，适用于多种扩散模型架构，显著降低了新控制任务的适应成本。

Abstract: Spatial conditioning in pretrained text-to-image diffusion models has
significantly improved fine-grained control over the structure of generated
images. However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks. To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions. Given a few
image-condition pairs of an unseen task and a query condition, UFC leverages
the analogy between query and support conditions to construct task-specific
control features, instantiated by a matching mechanism and an update on a small
set of task-specific parameters. Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions. Notably,
when fine-tuned with 0.1% of the full training data, UFC achieves competitive
performance with the fully supervised baselines in various control tasks. We
also show that UFC is applicable agnostically to various diffusion backbones
and demonstrate its effectiveness on both UNet and DiT architectures. Code is
available at https://github.com/kietngt00/UFC.

</details>


### [42] [HU-based Foreground Masking for 3D Medical Masked Image Modeling](https://arxiv.org/abs/2509.07534)
*Jin Lee,Vu Dang,Gwang-Hyun Yu,Anh Le,Zahid Rahman,Jin-Ho Jang,Heonzoo Lee,Kun-Yung Kim,Jin-Sul Kim,Jin-Young Kim*

Main category: cs.CV

TL;DR: 提出了一种基于HU值的医学图像前景掩码策略，改进了MIM在3D医学图像分割中的性能


<details>
  <summary>Details</summary>
Motivation: 传统随机掩码忽略了医学图像中解剖对象的密度分布，无法有效利用诊断相关的组织特征

Method: 利用Hounsfield Unit测量值实现基于强度的前景掩码，专注于内脏器官强度分布，排除空气和液体等无诊断意义的非组织区域

Result: 在5个公开3D医学影像数据集上显著提升分割质量和Dice分数（BTCV:84.64%, Flare22:92.43%, MM-WHS:90.67%, Amos22:88.64%, BraTS:78.55%）

Conclusion: 领域特定的MIM方法对医学图像表示学习具有重要价值，前景掩码策略为医学图像分割提供了有前景的研究方向

Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer
vision, its adoption in 3D medical image computing has been limited by the use
of random masking, which overlooks the density of anatomical objects. To
address this limitation, we enhance the pretext task with a simple yet
effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we
implement an HU-based Foreground Masking, which focuses on the intensity
distribution of visceral organs and excludes non-tissue regions, such as air
and fluid, that lack diagnostically meaningful features. Extensive experiments
on five public 3D medical imaging datasets demonstrate that our masking
consistently improves performance, both in quality of segmentation and Dice
score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,
BraTS:~78.55\%). These results underscore the importance of domain-centric MIM
and suggest a promising direction for representation learning in medical image
segmentation. Implementation is available at github.com/AISeedHub/SubFore/.

</details>


### [43] [TextlessRAG: End-to-End Visual Document RAG by Speech Without Text](https://arxiv.org/abs/2509.07538)
*Peijin Xie,Shun Qian,Bingquan Liu,Dexin Wang,Lin Sun,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: TextlessRAG是首个端到端的语音文档问答框架，直接在视觉文档图像上进行语音查询，无需ASR、TTS和OCR，实现了完全无文本的处理流程。


<details>
  <summary>Details</summary>
Motivation: 文档图像蕴含丰富知识，语音查询具有便携性和灵活性，但此前没有工作探索在视觉文档图像上直接使用语音查询进行知识库问答。

Method: 提出TextlessRAG端到端框架，直接解析语音、检索相关视觉知识并生成答案，集成布局感知重排序机制优化检索性能。

Result: 实验证明在效率和准确性方面都有显著提升，并发布了首个双语语音-文档RAG数据集（中英文语音查询+多模态文档内容）。

Conclusion: 该框架为语音文档问答开辟了新方向，提供了高效准确的无文本处理方案，相关数据集和代码将开源。

Abstract: Document images encapsulate a wealth of knowledge, while the portability of
spoken queries enables broader and flexible application scenarios. Yet, no
prior work has explored knowledge base question answering over visual document
images with queries provided directly in speech. We propose TextlessRAG, the
first end-to-end framework for speech-based question answering over large-scale
document images. Unlike prior methods, TextlessRAG eliminates ASR, TTS and OCR,
directly interpreting speech, retrieving relevant visual knowledge, and
generating answers in a fully textless pipeline. To further boost performance,
we integrate a layout-aware reranking mechanism to refine retrieval.
Experiments demonstrate substantial improvements in both efficiency and
accuracy. To advance research in this direction, we also release the first
bilingual speech--document RAG dataset, featuring Chinese and English voice
queries paired with multimodal document content. Both the dataset and our
pipeline will be made available at
repository:https://github.com/xiepeijinhit-hue/textlessrag

</details>


### [44] [PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image](https://arxiv.org/abs/2509.07552)
*Peng Li,Yisheng He,Yingdong Hu,Yuan Dong,Weihao Yuan,Yuan Liu,Zilong Dong,Yike Guo*

Main category: cs.CV

TL;DR: 从单张无姿势图片通过一次前向传播完成高斯全头部重建，避免了以往GAN逆向化和测试时优化的耗时问题


<details>
  <summary>Details</summary>
Motivation: 解决从单张无姿势图片快速重建9ad8高斯全头部模型的问题，避免传统GAN逆向化方法的耗时问题

Method: 使用大规模合成数据集训练，采用由粗到细的高斯头部生成流水线，通过双分支框架聚合球面三平面特征和点基特征，利用FLAME模型稀疏点与图像特征进行交互

Result: 实验结果显示该框架在高斯头部重建方面比现有方法更有效

Conclusion: 该前向传播框架能够实现快速高质量的单图高斯全头部重建，为3D头部生成提供了高效的解决方案

Abstract: We present a feed-forward framework for Gaussian full-head synthesis from a
single unposed image. Unlike previous work that relies on time-consuming GAN
inversion and test-time optimization, our framework can reconstruct the
Gaussian full-head model given a single unposed image in a single forward pass.
This enables fast reconstruction and rendering during inference. To mitigate
the lack of large-scale 3D head assets, we propose a large-scale synthetic
dataset from trained 3D GANs and train our framework using only synthetic data.
For efficient high-fidelity generation, we introduce a coarse-to-fine Gaussian
head generation pipeline, where sparse points from the FLAME model interact
with the image features by transformer blocks for feature extraction and coarse
shape reconstruction, which are then densified for high-fidelity
reconstruction. To fully leverage the prior knowledge residing in pretrained 3D
GANs for effective reconstruction, we propose a dual-branch framework that
effectively aggregates the structured spherical triplane feature and
unstructured point-based features for more effective Gaussian head
reconstruction. Experimental results show the effectiveness of our framework
towards existing work.

</details>


### [45] [Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks](https://arxiv.org/abs/2509.07581)
*Barkin Buyukcakir,Rocharles Cavalcante Fontenele,Reinhilde Jacobs,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 本文提出了Class Node Graph Attention Network (CGAT)架构，用于3D形状识别任务的可解释深度学习，通过注意力机制可视化决策过程，在牙齿发育阶段分类中取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑盒性质限制了其在高风险应用中的采用，特别是在需要信任和问责制的医学和法医学领域。本文旨在解决3D形状识别任务中的可解释性问题。

Method: 采用CGAT架构，结合图注意力卷积和内在注意力机制，使用注意力rollout可视化决策过程。评估了局部平均曲率和质心距离节点特征的不同组合，以及模型深度的影响。

Result: 结合局部平均曲率和质心距离作为节点特征获得了0.76加权F1分数，产生了更全面的注意力可视化。包含指向全局CLS节点的有向边的模型产生了更直观的注意力图。

Conclusion: CGAT架构能够生成人类可理解的注意力图，增强信任并促进专家验证模型决策。虽然基于牙科数据演示，但CGAT广泛适用于基于图的分类和回归任务，促进透明深度学习模型在高风险环境中的更广泛采用。

Abstract: Deep learning offers a promising avenue for automating many recognition tasks
in fields such as medicine and forensics. However, the black-box nature of
these models hinders their adoption in high-stakes applications where trust and
accountability are required. For 3D shape recognition tasks in particular, this
paper introduces the Class Node Graph Attention Network (CGAT) architecture to
address this need. Applied to 3D meshes of third molars derived from CBCT
images, for Demirjian stage allocation, CGAT utilizes graph attention
convolutions and an inherent attention mechanism, visualized via attention
rollout, to explain its decision-making process. We evaluated the local mean
curvature and distance to centroid node features, both individually and in
combination, as well as model depth, finding that models incorporating directed
edges to a global CLS node produced more intuitive attention maps, while also
yielding desirable classification performance. We analyzed the attention-based
explanations of the models, and their predictive performances to propose
optimal settings for the CGAT. The combination of local mean curvature and
distance to centroid as node features yielded a slight performance increase
with 0.76 weighted F1 score, and more comprehensive attention visualizations.
The CGAT architecture's ability to generate human-understandable attention maps
can enhance trust and facilitate expert validation of model decisions. While
demonstrated on dental data, CGAT is broadly applicable to graph-based
classification and regression tasks, promoting wider adoption of transparent
and competitive deep learning models in high-stakes environments.

</details>


### [46] [Temporal Image Forensics: A Review and Critical Evaluation](https://arxiv.org/abs/2509.07591)
*Robert Jöchl,Andreas Uhl*

Main category: cs.CV

TL;DR: 这是一篇关于时间图像准证学的综述性论文，重点分析了基于图像获取流程中时间依赖迹跡的图像年龄估计技术，并提出了内场传感器缺陷的新见解和可解释人工智能方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 提供时间图像准证学领域的全面概述，重点关注时间依赖迹跡的特性和技术，并指出内容偏差问题以及验证技术可靠性的重要性。

Method: 分析和重现现有研究工作，进行实验验证，包括：(i)提出新的准证设置；(ii)验证内场传感器缺陷的增长率和空间分布；(iii)分析方法实际利用的迹跡；(iv)深入研究神经网络学习的特征；(v)展示神经网络如何被分散注意力。

Result: 验证了内场传感器缺陷的主要特性，发现某些方法实际上利用的是内容偏差而非年龄迹跡，并展示了神经网络学习过程中的潜在问题。

Conclusion: 该综述为时间图像准证学领域提供了重要见解，强调了可解释人工智能方法在验证技术可靠性中的关键作用，并提出了更现实的准证设置方案。

Abstract: Temporal image forensics is the science of estimating the age of a digital
image. Usually, time-dependent traces (age traces) introduced by the image
acquisition pipeline are exploited for this purpose. In this review, a
comprehensive overview of the field of temporal image forensics based on
time-dependent traces from the image acquisition pipeline is given. This
includes a detailed insight into the properties of known age traces (i.e.,
in-field sensor defects and sensor dust) and temporal image forensics
techniques. Another key aspect of this work is to highlight the problem of
content bias and to illustrate how important eXplainable Artificial
Intelligence methods are to verify the reliability of temporal image forensics
techniques. Apart from reviewing material presented in previous works, in this
review: (i) a new (probably more realistic) forensic setting is proposed; (ii)
the main properties (growth rate and spatial distribution) of in-field sensor
defects are verified; (iii) it is shown that a method proposed to utilize
in-field sensor defects for image age approximation actually exploits other
traces (most likely content bias); (iv) the features learned by a neural
network dating palmprint images are further investigated; (v) it is shown how
easily a neural network can be distracted from learning age traces. For this
purpose, previous work is analyzed, re-implemented if required and experiments
are conducted.

</details>


### [47] [Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation](https://arxiv.org/abs/2509.07596)
*Yusuke Hirota,Ryo Hachiuma,Boyi Li,Ximing Lu,Michael Ross Boone,Boris Ivanovic,Yejin Choi,Marco Pavone,Yu-Chiang Frank Wang,Noa Garcia,Yuta Nakashima,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型的性别偏见评估容易受到非性别特征（如物体和背景）的干扰，即使微小扰动也会显著改变偏见分数，建议在报告偏见指标时同时报告特征敏感性测量。


<details>
  <summary>Details</summary>
Motivation: 现有基于真实图像的性别偏见基准测试存在性别与非性别特征之间的虚假相关性，这可能导致性别偏见评估的失真，需要研究这些虚假特征如何影响评估结果。

Method: 系统性地扰动四个广泛使用的基准测试（COCO-gender、FACET、MIAP、PHASE）中的非性别特征，量化它们对各种视觉语言模型偏见评估的影响。

Result: 即使最小扰动（如仅遮蔽10%物体或轻微模糊背景）也能显著改变偏见分数，生成式视觉语言模型的指标变化高达175%，CLIP变体变化达43%。

Conclusion: 当前偏见评估往往反映模型对虚假特征而非性别偏见的响应，建议报告偏见指标时同时报告特征敏感性测量以提高评估可靠性。

Abstract: Gender bias in vision-language foundation models (VLMs) raises concerns about
their safe deployment and is typically evaluated using benchmarks with gender
annotations on real-world images. However, as these benchmarks often contain
spurious correlations between gender and non-gender features, such as objects
and backgrounds, we identify a critical oversight in gender bias evaluation: Do
spurious features distort gender bias evaluation? To address this question, we
systematically perturb non-gender features across four widely used benchmarks
(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact
on bias evaluation. Our findings reveal that even minimal perturbations, such
as masking just 10% of objects or weakly blurring backgrounds, can dramatically
alter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in
CLIP variants. This suggests that current bias evaluations often reflect model
responses to spurious features rather than gender bias, undermining their
reliability. Since creating spurious feature-free benchmarks is fundamentally
challenging, we recommend reporting bias metrics alongside feature-sensitivity
measurements to enable a more reliable bias assessment.

</details>


### [48] [Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2509.07613)
*Fangqi Cheng,Surajit Ray,Xiaochen Yang*

Main category: cs.CV

TL;DR: 通过轻量级提示微调技术，将3D CT基础的医学视觉-语言模型适配到3D MRI数据，并通过合成报告和辅助预测任务提高防疑性癖症诊断性能


<details>
  <summary>Details</summary>
Motivation: 解决现有Med-VLMs模型对患者元数据利用不充分、缺乏临床诊断知识集成、计算资源消耗大以及在3D医学形态式上效果有限的问题

Method: 使用轻量级提示微调技术，将结构化元数据转换为合成报告以改善图像-文本对齐，并添加预测MMSE评分的辅助token作为辅助监督

Result: 在仅使用1,500张训练图像的情况下，在两个AD数据集上达到状态前沿性能，超过了需要10,000张图像进行微调的现有方法

Conclusion: 该方法通过数据高效利用和临床知识集成，为3D医学形态的Med-VLMs应用提供了一种轻量、高效的微调解决方案

Abstract: Medical vision-language models (Med-VLMs) have shown impressive results in
tasks such as report generation and visual question answering, but they still
face several limitations. Most notably, they underutilize patient metadata and
lack integration of clinical diagnostic knowledge. Moreover, most existing
models are typically trained from scratch or fine-tuned on large-scale 2D
image-text pairs, requiring extensive computational resources, and their
effectiveness on 3D medical imaging is often limited due to the absence of
structural information. To address these gaps, we propose a data-efficient
fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate
its application in Alzheimer's disease (AD) diagnosis. Our system introduces
two key innovations. First, we convert structured metadata into synthetic
reports, enriching textual input for improved image-text alignment. Second, we
add an auxiliary token trained to predict the mini-mental state examination
(MMSE) score, a widely used clinical measure of cognitive function that
correlates with AD severity. This provides additional supervision for
fine-tuning. Applying lightweight prompt tuning to both image and text
modalities, our approach achieves state-of-the-art performance on two AD
datasets using 1,500 training images, outperforming existing methods fine-tuned
on 10,000 images. Code will be released upon publication.

</details>


### [49] [Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2509.07623)
*Fangqi Cheng,Yingying Zhao,Xiaochen Yang*

Main category: cs.CV

TL;DR: 提出一种自盛盛督的跨编码器框架，利用纵向MRI扫描的时间连续性进行监督，解耦静态和动态表征学习，提高神经退行性疾病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量标签数据且表征可解释性差，需要解决标签数据缺乏和模型可解释性两大挑战。

Method: 设计自盛盛督跨编码器框架，通过对比学习约束静态表征捕获稳定解剖特征，通过输入梯度正则化指导动态表征反映时间变化。

Result: 在ADNI数据集上实现了更高的分类准确性和改善的可解释性，在OASIS数据集上展现出强大的零样本泛化能力，在PPMI数据集上实现了跨任务泛化。

Conclusion: 该方法有效解决了标签数据缺乏和可解释性问题，为神经退行性疾病诊断提供了一种高效可解释的解决方案。

Abstract: Deep learning has shown significant potential in diagnosing neurodegenerative
diseases from MRI data. However, most existing methods rely heavily on large
volumes of labeled data and often yield representations that lack
interpretability. To address both challenges, we propose a novel
self-supervised cross-encoder framework that leverages the temporal continuity
in longitudinal MRI scans for supervision. This framework disentangles learned
representations into two components: a static representation, constrained by
contrastive learning, which captures stable anatomical features; and a dynamic
representation, guided by input-gradient regularization, which reflects
temporal changes and can be effectively fine-tuned for downstream
classification tasks. Experimental results on the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset demonstrate that our method achieves
superior classification accuracy and improved interpretability. Furthermore,
the learned representations exhibit strong zero-shot generalization on the Open
Access Series of Imaging Studies (OASIS) dataset and cross-task generalization
on the Parkinson Progression Marker Initiative (PPMI) dataset. The code for the
proposed method will be made publicly available.

</details>


### [50] [Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity](https://arxiv.org/abs/2509.07647)
*Sung Ju Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出Hermitian对称傅里叶水印(SFW)方法，通过保持频率完整性来增强潜在扩散模型语义水印的鲁棒性和检测性能，同时引入中心感知嵌入策略抵抗裁剪攻击。


<details>
  <summary>Details</summary>
Motivation: 现有语义水印技术在潜在扩散模型中虽然能抵抗再生攻击，但由于频率完整性损失导致检测性能下降，需要解决这一根本问题。

Method: 提出Hermitian对称傅里叶水印(SFW)方法，通过强制Hermitian对称性保持频率完整性；引入中心感知嵌入策略确保裁剪攻击下的鲁棒信息保留。

Result: 实验表明该方法在各种攻击场景下达到最先进的验证和识别性能，在保持优异图像保真度(FID和CLIP分数)的同时获得最高检测准确率。

Conclusion: SFW是平衡鲁棒性和图像保真度的有效框架，成功解决了语义水印中的固有权衡问题。

Abstract: Semantic watermarking techniques for latent diffusion models (LDMs) are
robust against regeneration attacks, but often suffer from detection
performance degradation due to the loss of frequency integrity. To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry. Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention. To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.
Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios. Ablation studies confirm the impact of SFW on
detection capabilities, the effectiveness of the center-aware embedding against
cropping, and how message capacity influences identification accuracy. Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed
SFW is shown to be an effective framework for balancing robustness and image
fidelity, addressing the inherent trade-offs in semantic watermarking. Code
available at https://github.com/thomas11809/SFWMark

</details>


### [51] [Beyond Motion Cues and Structural Sparsity: Revisiting Small Moving Target Detection](https://arxiv.org/abs/2509.07654)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Zhihua Shen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出TenRPCANet深度学习框架，通过张量低秩稀疏分解解决小目标检测问题，在红外小目标和空间目标检测任务上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 小运动目标检测在国防应用中至关重要，但现有方法依赖目标特定特征或运动线索，在复杂环境中缺乏鲁棒性。背景往往具有强低秩结构，可作为检测的稳定先验

Method: 将任务重新表述为基于张量的低秩稀疏分解问题，提出TenRPCANet网络，使用tokenization策略通过自注意力机制隐式强制执行多阶张量低秩先验，并设计特征细化模块增强目标显著性

Result: 在两个高度不同且具有挑战性的任务上实现最先进性能：多帧红外小目标检测和空间目标检测

Conclusion: 该方法既有效又具有通用性，证明了通过张量低秩背景建模和小目标稀疏分解的深度学习框架的优越性

Abstract: Small moving target detection is crucial for many defense applications but
remains highly challenging due to low signal-to-noise ratios, ambiguous visual
cues, and cluttered backgrounds. In this work, we propose a novel deep learning
framework that differs fundamentally from existing approaches, which often rely
on target-specific features or motion cues and tend to lack robustness in
complex environments. Our key insight is that small target detection and
background discrimination are inherently coupled, even cluttered video
backgrounds often exhibit strong low-rank structures that can serve as stable
priors for detection. We reformulate the task as a tensor-based low-rank and
sparse decomposition problem and conduct a theoretical analysis of the
background, target, and noise components to guide model design. Building on
these insights, we introduce TenRPCANet, a deep neural network that requires
minimal assumptions about target characteristics. Specifically, we propose a
tokenization strategy that implicitly enforces multi-order tensor low-rank
priors through a self-attention mechanism. This mechanism captures both local
and non-local self-similarity to model the low-rank background without relying
on explicit iterative optimization. In addition, inspired by the sparse
component update in tensor RPCA, we design a feature refinement module to
enhance target saliency. The proposed method achieves state-of-the-art
performance on two highly distinct and challenging tasks: multi-frame infrared
small target detection and space object detection. These results demonstrate
both the effectiveness and the generalizability of our approach.

</details>


### [52] [EDFFDNet: Towards Accurate and Efficient Unsupervised Multi-Grid Image Registration](https://arxiv.org/abs/2509.07662)
*Haokai Zhu,Bo Qu,Si-Yuan Cao,Runmin Zhang,Shujie Chen,Bailin Yang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 提出EDFFDNet网络，使用指数衰减基函数的自由形变，结合自适应稀疏运动聚合器和渐进相关细化策略，在深度差异场景中实现高效准确的图像配准


<details>
  <summary>Details</summary>
Motivation: 现有基于单应性变换、多网格单应性或薄板样条的深度图像配准方法在处理包含深度差异的真实场景时存在固有局限性，需要更有效的解决方案

Method: 1) 指数衰减自由形变网络(EDFFDNet)使用指数衰减基函数实现自由形变；2) 自适应稀疏运动聚合器(ASMA)将密集交互转为稀疏交互；3) 渐进相关细化策略利用全局-局部相关模式进行从粗到细的运动估计

Result: EDFFDNet减少参数70.5%、内存32.6%、总运行时间33.7%，PSNR提升0.5dB；EDFFDNet-2进一步提升PSNR 1.06dB且保持较低计算成本，在多个数据集上表现出强泛化能力

Conclusion: 该方法通过创新的网络设计和优化策略，在深度差异场景中实现了高效准确的图像配准，显著优于现有深度学习方法

Abstract: Previous deep image registration methods that employ single homography,
multi-grid homography, or thin-plate spline often struggle with real scenes
containing depth disparities due to their inherent limitations. To address
this, we propose an Exponential-Decay Free-Form Deformation Network (EDFFDNet),
which employs free-form deformation with an exponential-decay basis function.
This design achieves higher efficiency and performs well in scenes with depth
disparities, benefiting from its inherent locality. We also introduce an
Adaptive Sparse Motion Aggregator (ASMA), which replaces the MLP motion
aggregator used in previous methods. By transforming dense interactions into
sparse ones, ASMA reduces parameters and improves accuracy. Additionally, we
propose a progressive correlation refinement strategy that leverages
global-local correlation patterns for coarse-to-fine motion estimation, further
enhancing efficiency and accuracy. Experiments demonstrate that EDFFDNet
reduces parameters, memory, and total runtime by 70.5%, 32.6%, and 33.7%,
respectively, while achieving a 0.5 dB PSNR gain over the state-of-the-art
method. With an additional local refinement stage,EDFFDNet-2 further improves
PSNR by 1.06 dB while maintaining lower computational costs. Our method also
demonstrates strong generalization ability across datasets, outperforming
previous deep learning methods.

</details>


### [53] [Nearest Neighbor Projection Removal Adversarial Training](https://arxiv.org/abs/2509.07673)
*Himanshu Singh,A. V. Subramanyam,Shivank Rajput,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 通过抛出跨类依赖性抓取特征的方法，提出了一种新的对抗训练框架，显著提升了神经网络的对抗性和清洁准确率


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在图像分类中表现优异但容易受到对抗样本攻击，标准对抗训练没有明确处理类间特征重叠问题

Method: 识别每个对抗样本的最近跨类邻居，然后在特征空间中移除这些邻居的投影以强化特征可分离性

Result: 在CIFAR-10、CIFAR-100和SVHN标准数据集上进行广泛实验，方法在对抗性和清洁准确率方面都取得了竞争力强的表现

Conclusion: 明确处理类间特征接近性对提升深度神经网络的对抗性至关重要

Abstract: Deep neural networks have exhibited impressive performance in image
classification tasks but remain vulnerable to adversarial examples. Standard
adversarial training enhances robustness but typically fails to explicitly
address inter-class feature overlap, a significant contributor to adversarial
susceptibility. In this work, we introduce a novel adversarial training
framework that actively mitigates inter-class proximity by projecting out
inter-class dependencies from adversarial and clean samples in the feature
space. Specifically, our approach first identifies the nearest inter-class
neighbors for each adversarial sample and subsequently removes projections onto
these neighbors to enforce stronger feature separability. Theoretically, we
demonstrate that our proposed logits correction reduces the Lipschitz constant
of neural networks, thereby lowering the Rademacher complexity, which directly
contributes to improved generalization and robustness. Extensive experiments
across standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that
our method demonstrates strong performance that is competitive with leading
adversarial training techniques, highlighting significant achievements in both
robust and clean accuracy. Our findings reveal the importance of addressing
inter-class feature proximity explicitly to bolster adversarial robustness in
DNNs.

</details>


### [54] [CAViAR: Critic-Augmented Video Agentic Reasoning](https://arxiv.org/abs/2509.07680)
*Sachit Menon,Ahmet Iscen,Arsha Nagrani,Tobias Weyand,Carl Vondrick,Cordelia Schmid*

Main category: cs.CV

TL;DR: 通过给大语言模型配备视频模块作为子工具，并使用评判器区分成功与失败的执行序列，提升了长视频复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然短视频感知性能有显著提升，但在复杂查询和长视频的推理任务上性能仍有限。研究考虑能否利用现有感知能力来完成更复杂的视频推理。

Method: 开发了一个大语言模型代理，给其配备视频模块作为子代理或工具。代理根据每次调用模块的结果来决定后续步骤，而非固定执行流程。受文本推理领域的启发，引入了一个评判器来区分成功和失败的执行序列。

Result: 组合代理和评判器的方法在LVBench、Neptune和ActivityNet-RTL等数据集上展现出强劲的性能。

Conclusion: 通过将大语言模型与视频模块相结合，并使用动态执行策略和评判机制，可以有效提升长视频复杂推理的性能。

Abstract: Video understanding has seen significant progress in recent years, with
models' performance on perception from short clips continuing to rise. Yet,
multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show
performance wanes for tasks requiring complex reasoning on videos as queries
grow more complex and videos grow longer. In this work, we ask: can existing
perception capabilities be leveraged to successfully perform more complex video
reasoning? In particular, we develop a large language model agent given access
to video modules as subagents or tools. Rather than following a fixed procedure
to solve queries as in previous work such as Visual Programming, ViperGPT, and
MoReVQA, the agent uses the results of each call to a module to determine
subsequent steps. Inspired by work in the textual reasoning domain, we
introduce a critic to distinguish between instances of successful and
unsuccessful sequences from the agent. We show that the combination of our
agent and critic achieve strong performance on the previously-mentioned
datasets.

</details>


### [55] [SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression](https://arxiv.org/abs/2509.07704)
*Chunhang Zheng,Zichang Ren,Dou Li*

Main category: cs.CV

TL;DR: SEEC提出了一种基于语义分割的多熵模型无损图像压缩方法，通过为不同语义区域分配专用熵模型来提升压缩性能


<details>
  <summary>Details</summary>
Motivation: 传统学习方法使用单一熵模型估计整个图像的概率分布，无法捕捉不同语义区域的多样化统计特征

Method: 利用语义分割识别不同区域，为每个区域分配专用熵模型，采用多通道离散逻辑混合似然建模像素值分布

Result: 在基准数据集上实现了最先进的压缩比，同时仅引入最小的编码解码延迟，支持基于分割掩码的感兴趣区域编码

Conclusion: SEEC框架通过语义分割指导的多熵模型选择，显著提升了无损图像压缩性能，同时保持了高效的编解码效率

Abstract: Recently, learned image compression has attracted considerable attention due
to its superior performance over traditional methods. However, most existing
approaches employ a single entropy model to estimate the probability
distribution of pixel values across the entire image, which limits their
ability to capture the diverse statistical characteristics of different
semantic regions. To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework
utilizes semantic segmentation to guide the selection and adaptation of
multiple entropy models, enabling more accurate probability distribution
estimation for distinct semantic regions. Specifically, SEEC first extracts
image features and then applies semantic segmentation to identify different
regions, each assigned a specialized entropy model to better capture its unique
statistical properties. Finally, a multi-channel discrete logistic mixture
likelihood is employed to model the pixel value distributions effectively.
Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency. With superior performance, the proposed model also supports
Regions of Interest (ROIs) coding condition on the provided segmentation mask.
Our code is available at https://github.com/chunbaobao/SEEC.

</details>


### [56] [XSRD-Net: EXplainable Stroke Relapse Detection](https://arxiv.org/abs/2509.07772)
*Christian Gapp,Elias Tappeiner,Martin Welk,Karl Fritscher,Stephanie Mangesius,Constantin Eisenschink,Philipp Deisl,Michael Knoflach,Astrid E. Grams,Elke R. Gizewski,Rainer Schubert*

Main category: cs.CV

TL;DR: 通过多模态深度学习模型，结合CTA图像和临床数据，提高脑冸中复发的时间预测准确度，发现心脏疾病和颈动脉是关键因素


<details>
  <summary>Details</summary>
Motivation: 脑冸中复发率高且死亡率极高（40%），早期识别高风险患者对于降低复发率至关重要

Method: 收集3D脑部CTA图像数据和临床数据，训练单模态和多模态深度学习模型，进行二分类复发检测和复发免病生存时间预测

Result: 多模态XSRD-net模型在测试集上获得c-index 0.68和AUC 0.71，表明心脏疾病和颈动脉是关键预测因素

Conclusion: 多模态深度学习模型能有效预测脑冸复发风险，为临床治疗提供重要参考，需要进一步数据收集和模型优化

Abstract: Stroke is the second most frequent cause of death world wide with an annual
mortality of around 5.5 million. Recurrence rates of stroke are between 5 and
25% in the first year. As mortality rates for relapses are extraordinarily high
(40%) it is of utmost importance to reduce the recurrence rates. We address
this issue by detecting patients at risk of stroke recurrence at an early stage
in order to enable appropriate therapy planning. To this end we collected 3D
intracranial CTA image data and recorded concomitant heart diseases, the age
and the gender of stroke patients between 2010 and 2024. We trained single- and
multimodal deep learning based neural networks for binary relapse detection
(Task 1) and for relapse free survival (RFS) time prediction together with a
subsequent classification (Task 2). The separation of relapse from non-relapse
patients (Task 1) could be solved with tabular data (AUC on test dataset:
0.84). However, for the main task, the regression (Task 2), our multimodal
XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to
modality contribution measures. The c-index with respect to relapses for the
multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final,
deeper interpretability analysis results could highlight a link between both
heart diseases (tabular) and carotid arteries (vision) for the detection of
relapses and the prediction of the RFS time. This is a central outcome that we
strive to strengthen with ongoing data collection and model retraining.

</details>


### [57] [HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.07774)
*Yimin Pan,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: 基于3D高斯泼溅技术，提出了一种从多视角图像重建发丝级头发几何的方法，通过多阶段流水线实现高效重建，并提出了评估拓扑准确性的新指标。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实和数字人建模应用中需要高质量的头发重建，现有方法往往忽略发丝的连接性和拓扑结构，需要更精确的评估和重建方法。

Method: 多阶段流水线：首先使用可微分高斯光栅化器重建详细几何，然后通过新颖的合并方案将高斯片段合并为连贯发丝，最后在光度监督下精炼和生长发丝。

Result: 在合成和真实数据集上的广泛实验表明，该方法能稳健处理各种发型，实现高效重建（通常在一小时内完成）。

Conclusion: 该方法成功扩展了3DGS框架，实现了发丝级头发重建，并提出了评估拓扑准确性的新指标，为头发重建领域提供了重要贡献。

Abstract: Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/

</details>


### [58] [RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis](https://arxiv.org/abs/2509.07782)
*Hugo Blanc,Jean-Emmanuel Deschaud,Alexis Paljic*

Main category: cs.CV

TL;DR: RayGaussX在RayGauss基础上进行优化，通过引入体积渲染加速策略、增强光线连贯性和尺度正则化等技术，实现了5-12倍训练加速和50-80倍渲染速度提升，同时提高了视觉质量。


<details>
  <summary>Details</summary>
Motivation: RayGauss虽然在合成和室内场景的新视角合成中达到了最先进的渲染质量，但其计算成本过高，无法在真实世界场景中实现实时渲染。

Method: 引入体积渲染加速策略（空域跳过和自适应采样）、增强光线连贯性、尺度正则化减少误报交集，并提出新的致密化准则改善远距离区域的密度分布。

Result: 在真实世界数据集上实现了5-12倍训练加速和50-80倍渲染速度提升（FPS），视觉质量提升高达+0.56 dB PSNR。

Conclusion: RayGaussX成功解决了RayGauss的计算效率问题，在保持高质量渲染的同时实现了显著的性能提升，适用于更大规模的场景。

Abstract: RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH). However, its
computational cost prevents real-time rendering on real-world scenes. Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference. Specifically, we incorporate volumetric
rendering acceleration strategies such as empty-space skipping and adaptive
sampling, enhance ray coherence, and introduce scale regularization to reduce
false-positive intersections. Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x
to 12x faster training and 50x to 80x higher rendering speeds (FPS) on
real-world datasets while improving visual quality by up to +0.56 dB in PSNR.
Project page with videos and code: https://raygaussx.github.io/.

</details>


### [59] [Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss](https://arxiv.org/abs/2509.07798)
*Maja Schlereth,Moritz Schillinger,Katharina Breininger*

Main category: cs.CV

TL;DR: 这篇论文提出了一种自监督的多视角神经网络方法，用于融合两个正交各异性低分辨率碳氢共振成像，以重建统一的高分辨率解剖细节，无需高分辨率数据训练。


<details>
  <summary>Details</summary>
Motivation: 在医学碳氢共振成像中，需要在扫描时间和图像质量之间取得平衡。传统方法对个别低分辨率扫描进行分析时间耗时且容易导致误判，因此需要一种能够融合多个低分辨率扫描的方法来提高效率和准确性。

Method: 提出了一种自监督的多视角神经网络方法，通过稀疏坐标基减损函数来优化模型，支持任意缩放比例的低分辨率图像融合。组合了病人无关的离线阶段和病人特定的在线阶段。

Result: 在两个独立的碳氢共振成像数据集上评估，与现有自监督超分辨率方法相比表现相似或更优。通过病人特定重建，实现了较传统方法进入十倍的速度提升，同时保持相似或更好的超分辨率质量。

Conclusion: 该方法为医学碳氢共振成像提供了一种高效的解决方案，能够在不需要高分辨率数据训练的情况下，通过融合多个低分辨率扫描来实现超分辨率重建，显著提高了重建速度和质量。

Abstract: Acquiring images in high resolution is often a challenging task. Especially
in the medical sector, image quality has to be balanced with acquisition time
and patient comfort. To strike a compromise between scan time and quality for
Magnetic Resonance (MR) imaging, two anisotropic scans with different
low-resolution (LR) orientations can be acquired. Typically, LR scans are
analyzed individually by radiologists, which is time consuming and can lead to
inaccurate interpretation. To tackle this, we propose a novel approach for
fusing two orthogonal anisotropic LR MR images to reconstruct anatomical
details in a unified representation. Our multi-view neural network is trained
in a self-supervised manner, without requiring corresponding high-resolution
(HR) data. To optimize the model, we introduce a sparse coordinate-based loss,
enabling the integration of LR images with arbitrary scaling. We evaluate our
method on MR images from two independent cohorts. Our results demonstrate
comparable or even improved super-resolution (SR) performance compared to
state-of-the-art (SOTA) self-supervised SR methods for different upsampling
scales. By combining a patient-agnostic offline and a patient-specific online
phase, we achieve a substantial speed-up of up to ten times for
patient-specific reconstruction while achieving similar or better SR quality.
Code is available at https://github.com/MajaSchle/tripleSR.

</details>


### [60] [SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting](https://arxiv.org/abs/2509.07809)
*Mahtab Dahaghin,Milind G. Padalkar,Matteo Toso,Alessio Del Bue*

Main category: cs.CV

TL;DR: SplatFill是一种基于深度引导的3D高斯溅射场景修复方法，通过联合深度监督和一致性感知优化，在视觉保真度和效率方面达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射(3DGS)虽然能创建高度逼真的3D场景表示，但在修复因遮挡或场景编辑导致的缺失区域时仍存在挑战，常导致模糊细节、伪影和几何不一致

Method: 结合两种关键思想：(1)联合深度基和对象基监督，确保修复的高斯点准确放置在3D空间中并与周围几何对齐；(2)提出一致性感知细化方案，选择性识别和修正不一致区域而不破坏场景其余部分

Result: 在SPIn-NeRF数据集上的评估显示，SplatFill不仅超越了现有的NeRF基和3DGS基修复方法在视觉保真度方面，还将训练时间减少了24.5%。定性结果显示该方法能提供更清晰的细节、更少的伪影和在不同挑战性视角下更好的连贯性

Conclusion: SplatFill通过深度引导和一致性优化，成功解决了3DGS场景修复中的关键问题，实现了高质量的修复效果和显著的效率提升

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D
scene representations from sets of multi-view images. However, inpainting
missing regions, whether due to occlusion or scene editing, remains a
challenging task, often leading to blurry details, artifacts, and inconsistent
geometry. In this work, we introduce SplatFill, a novel depth-guided approach
for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and
improved efficiency. Our method combines two key ideas: (1) joint depth-based
and object-based supervision to ensure inpainted Gaussians are accurately
placed in 3D space and aligned with surrounding geometry, and (2) we propose a
consistency-aware refinement scheme that selectively identifies and corrects
inconsistent regions without disrupting the rest of the scene. Evaluations on
the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing
NeRF-based and 3DGS-based inpainting methods in visual fidelity but also
reduces training time by 24.5%. Qualitative results show our method delivers
sharper details, fewer artifacts, and greater coherence across challenging
viewpoints.

</details>


### [61] [Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model](https://arxiv.org/abs/2509.07825)
*Zhuoxu Huang,Mingqi Gao,Jungong Han*

Main category: cs.CV

TL;DR: PLM框架通过对象中心判别表示和几何重激活解码器，解决了LLM与3D点云之间的表示对齐问题，在多个3D分割任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的3D对象分割方法存在表示对齐问题：LLM处理高级语义token而3D点云只有密集几何结构，导致输入输出阶段的限制和精度损失

Method: 提出Point Linguist Model (PLM)框架，包含Object-centric Discriminative Representation (OcDR)学习对象中心token，以及Geometric Reactivation Decoder (GRD)结合OcDR token和密集特征进行精确分割

Result: 在ScanNetv2上提升+7.3 mIoU，在Multi3DRefer上提升+6.0 mIoU，在7个基准测试的4个不同任务中均取得一致性能提升

Conclusion: PLM通过全面的对象中心推理有效解决了3D表示对齐问题，实现了鲁棒的3D理解

Abstract: 3D object segmentation with Large Language Models (LLMs) has become a
prevailing paradigm due to its broad semantics, task flexibility, and strong
generalization. However, this paradigm is hindered by representation
misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds
convey only dense geometric structures. In prior methods, misalignment limits
both input and output. At the input stage, dense point patches require heavy
pre-alignment, weakening object-level semantics and confusing similar
distractors. At the output stage, predictions depend only on dense features
without explicit geometric cues, leading to a loss of fine-grained accuracy. To
address these limitations, we present the Point Linguist Model (PLM), a general
framework that bridges the representation gap between LLMs and dense 3D point
clouds without requiring large-scale pre-alignment between 3D-text or
3D-images. Specifically, we introduce Object-centric Discriminative
Representation (OcDR), which learns object-centric tokens that capture target
semantics and scene relations under a hard negative-aware training objective.
This mitigates the misalignment between LLM tokens and 3D points, enhances
resilience to distractors, and facilitates semantic-level reasoning within
LLMs. For accurate segmentation, we introduce the Geometric Reactivation
Decoder (GRD), which predicts masks by combining OcDR tokens carrying
LLM-inferred geometry with corresponding dense features, preserving
comprehensive dense features throughout the pipeline. Extensive experiments
show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and
+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains
across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness
of comprehensive object-centric reasoning for robust 3D understanding.

</details>


### [62] [Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets](https://arxiv.org/abs/2509.07852)
*Seyd Teymoor Seydi*

Main category: cs.CV

TL;DR: 提出基于AlphaEarth数据集和Siamese U-Net的自动化火烧迹地制图方法，在测试集上达到95%总体精度、0.6 IoU和74% F1分数


<details>
  <summary>Details</summary>
Motivation: 精确及时的火烧迹地制图对环境监测、灾害管理和气候变化评估至关重要

Method: 使用AlphaEarth高分辨率光学和热红外影像数据集，结合Siamese U-Net深度学习架构，在美国MTBS数据集上训练，在欧洲17个区域评估

Result: 模型在测试集上总体精度95%、IoU 0.6、F1分数74%，能有效识别复杂背景下的火烧区域，特别擅长检测部分烧毁植被和火场边界

Conclusion: 该方法为自动化火灾损害评估提供了先进解决方案，利用AlphaEarth数据集为全球火烧迹地监测提供了可扩展方案

Abstract: Accurate and timely mapping of burned areas is crucial for environmental
monitoring, disaster management, and assessment of climate change. This study
presents a novel approach to automated burned area mapping using the AlphaEArth
dataset combined with the Siamese U-Net deep learning architecture. The
AlphaEArth Dataset, comprising high-resolution optical and thermal infrared
imagery with comprehensive ground-truth annotations, provides an unprecedented
resource for training robust burned area detection models. We trained our model
with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US
and evaluated it with 17 regions cross in Europe. Our experimental results
demonstrate that the proposed ensemble approach achieves superior performance
with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test
dataset. The model successfully identifies burned areas across diverse
ecosystems with complex background, showing particular strength in detecting
partially burned vegetation and fire boundaries and its transferability and
high generalization in burned area mapping. This research contributes to the
advancement of automated fire damage assessment and provides a scalable
solution for global burn area monitoring using the AlphaEarth dataset.

</details>


### [63] [D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics](https://arxiv.org/abs/2509.07864)
*Tiancheng Yang,Lin Zhang,Jiaye Lin,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 提出了D-LEAF方法，通过动态层级注意力分析来检测和修正MLLM中的幻觉错误，在图像描述和VQA任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型容易产生与视觉输入冲突的幻觉文本，现有方法无法准确定位问题层和注意力头

Method: 提出LIAE和IAF两个诊断指标来定位问题层和注意力头，并基于此开发D-LEAF方法进行动态错误修正

Result: 在标准图像描述基准上相对改进53%，VQA任务准确率和F1分数提升约4%，显著抑制幻觉同时保持效率

Conclusion: D-LEAF提供了一种任务无关的注意力引导方法，能够有效定位和修正MLLM中的幻觉错误，具有实际应用价值

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance on tasks
like image captioning and visual question answering, but remain prone to
hallucinations, where generated text conflicts with the visual input. Prior
work links this partly to insufficient visual attention, but existing
attention-based detectors and mitigation typically apply uniform adjustments
across layers and heads, obscuring where errors originate. In this paper, we
first show these methods fail to accurately localize problematic layers. Then,
we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags
anomalous layers, and Image Attention Focus (IAF) which scores attention heads
within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF
reliably ranks heads that warrant correction. Guided by these signals, we
propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a
task-agnostic, attention-guided method that dynamically localizes and corrects
errors during inference with negligible overhead. Results show our D-LEAF
delivers a 53% relative improvement on standard captioning benchmarks, and on
VQA both accuracy and F1-score improve by approximately 4%, substantially
suppressing hallucinations while preserving efficiency.

</details>


### [64] [Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning](https://arxiv.org/abs/2509.07879)
*Daniel DeAlcala,Aythami Morales,Julian Fierrez,Gonzalo Mancera,Ruben Tolosana,Javier Ortega-Garcia*

Main category: cs.CV

TL;DR: Active MINT是一种多任务学习方法，通过同时训练原始模型和检测模型来识别训练数据，提高AI模型透明度，准确率超过80%


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习模型训练数据检测问题，提高AI部署的安全性、隐私保护和版权保护，增加模型透明度

Method: 提出新颖的多任务学习过程，同时训练原始模型（Audited Model）和检测模型（MINT Model），利用中间激活图作为MINT层的输入来增强训练数据检测能力

Result: 在5个公共基准测试中，从MobileNet到Vision Transformers等多种神经网络架构上，Active MINT达到超过80%的训练数据检测准确率，显著优于现有方法

Conclusion: Active MINT方法有效提升了AI模型训练数据的可检测性，为AI部署提供了更强的安全保障，有助于实现更好的安全性、隐私保护和版权保护

Abstract: Active Membership Inference Test (aMINT) is a method designed to detect
whether given data were used during the training of machine learning models. In
Active MINT, we propose a novel multitask learning process that involves
training simultaneously two models: the original or Audited Model, and a
secondary model, referred to as the MINT Model, responsible for identifying the
data used for training the Audited Model. This novel multi-task learning
approach has been designed to incorporate the auditability of the model as an
optimization objective during the training process of neural networks. The
proposed approach incorporates intermediate activation maps as inputs to the
MINT layers, which are trained to enhance the detection of training data. We
present results using a wide range of neural networks, from lighter
architectures such as MobileNet to more complex ones such as Vision
Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT
achieves over 80% accuracy in detecting if given data was used for training,
significantly outperforming previous approaches in the literature. Our aMINT
and related methodological developments contribute to increasing transparency
in AI models, facilitating stronger safeguards in AI deployments to achieve
proper security, privacy, and copyright protection.

</details>


### [65] [Object-level Correlation for Few-Shot Segmentation](https://arxiv.org/abs/2509.07917)
*Chunlin Wen,Yu Zhang,Jie Fan,Hongyuan Zhu,Xiu-Shen Wei,Yijun Wang,Zhiqiang Kou,Shuzhou Sun*

Main category: cs.CV

TL;DR: OCNet通过建立目标对象与查询图像中一般对象的对象级相关性，解决了少样本语义分割中背景噪声过拟合问题，在PASCAL-5i和COCO-20i数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在支持目标对象与整个查询图像之间建立图像级相关性，但这种方法包含难以追踪和抑制的硬像素噪声（无关背景对象），导致背景过拟合

Method: 提出对象级相关网络(OCNet)，包含通用对象挖掘模块(GOMM)和相关构建模块(CCM)。GOMM通过学习显著性和高层相似性线索构建查询通用对象特征，CCM通过分配目标原型来匹配通用对象特征建立对象级相关性

Result: 在PASCAL-5i和COCO-20i数据集上的大量实验表明，该模型达到了最先进的性能

Conclusion: 通过模仿生物视觉过程建立对象级相关性，能够有效挖掘查询目标特征并抑制硬像素噪声，在少样本语义分割任务中表现出色

Abstract: Few-shot semantic segmentation (FSS) aims to segment objects of novel
categories in the query images given only a few annotated support samples.
Existing methods primarily build the image-level correlation between the
support target object and the entire query image. However, this correlation
contains the hard pixel noise, \textit{i.e.}, irrelevant background objects,
that is intractable to trace and suppress, leading to the overfitting of the
background. To address the limitation of this correlation, we imitate the
biological vision process to identify novel objects in the object-level
information. Target identification in the general objects is more valid than in
the entire image, especially in the low-data regime. Inspired by this, we
design an Object-level Correlation Network (OCNet) by establishing the
object-level correlation between the support target object and query general
objects, which is mainly composed of the General Object Mining Module (GOMM)
and Correlation Construction Module (CCM). Specifically, GOMM constructs the
query general object feature by learning saliency and high-level similarity
cues, where the general objects include the irrelevant background objects and
the target foreground object. Then, CCM establishes the object-level
correlation by allocating the target prototypes to match the general object
feature. The generated object-level correlation can mine the query target
feature and suppress the hard pixel noise for the final prediction. Extensive
experiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our model
achieves the state-of-the-art performance.

</details>


### [66] [ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion](https://arxiv.org/abs/2509.07920)
*Ao Li,Jinpeng Liu,Yixuan Zhu,Yansong Tang*

Main category: cs.CV

TL;DR: ScoreHOI是一个基于扩散模型的优化器，通过引入扩散先验和物理约束来精确重建人-物交互，在标准基准测试中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的人-物交互重建方法由于缺乏先验知识，难以获得物理上合理的结果。本文旨在解决这一问题，通过引入扩散先验来提升重建质量。

Method: 提出ScoreHOI扩散优化器，利用分数引导采样的可控性重建条件分布；采用接触驱动的迭代细化方法增强接触合理性；在去噪过程中引入特定物理约束指导。

Result: 在标准基准测试上的广泛评估表明，ScoreHOI在联合人-物交互重建方面实现了精确和稳健的改进，性能优于最先进方法。

Conclusion: ScoreHOI通过扩散先验和物理约束的有效结合，显著提升了人-物交互重建的准确性和物理合理性，为该领域提供了新的解决方案。

Abstract: Joint reconstruction of human-object interaction marks a significant
milestone in comprehending the intricate interrelations between humans and
their surrounding environment. Nevertheless, previous optimization methods
often struggle to achieve physically plausible reconstruction results due to
the lack of prior knowledge about human-object interactions. In this paper, we
introduce ScoreHOI, an effective diffusion-based optimizer that introduces
diffusion priors for the precise recovery of human-object interactions. By
harnessing the controllability within score-guided sampling, the diffusion
model can reconstruct a conditional distribution of human and object pose given
the image observation and object feature. During inference, the ScoreHOI
effectively improves the reconstruction results by guiding the denoising
process with specific physical constraints. Furthermore, we propose a
contact-driven iterative refinement approach to enhance the contact
plausibility and improve the reconstruction accuracy. Extensive evaluations on
standard benchmarks demonstrate ScoreHOI's superior performance over
state-of-the-art methods, highlighting its ability to achieve a precise and
robust improvement in joint human-object interaction reconstruction.

</details>


### [67] [Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation](https://arxiv.org/abs/2509.07923)
*Moo Hyun Son,Juyoung Bae,Zelin Qiu,Jiale Peng,Kai Xin Li,Yifan Lin,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了ToothMCL，首个用于牙齿分割的多模态预训练框架，整合CBCT和IOS数据，在牙齿分割任务上达到最先进性能，DSC指标提升12%（CBCT）和8%（IOS）。


<details>
  <summary>Details</summary>
Motivation: 现有牙齿分割方法缺乏严格验证，性能和临床适用性有限，需要更准确的多模态数字牙科表示方法。

Method: 提出ToothMCL多模态对比学习预训练框架，整合体数据（CBCT）和表面数据（IOS）模态，通过多模态对比学习捕获模态不变表示，精确建模细粒度解剖特征。

Result: 在内部和外部测试中均达到最先进性能，CBCT分割DSC提升12%，IOS分割DSC提升8%，在不同成像条件和临床场景下表现出强大泛化能力。

Conclusion: ToothMCL是首个牙齿分割多模态预训练框架，显著提升了分割精度和临床适用性，为数字牙科提供了更准确的牙齿表示方法。

Abstract: Digital dentistry represents a transformative shift in modern dental
practice. The foundational step in this transformation is the accurate digital
representation of the patient's dentition, which is obtained from segmented
Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the
growing interest in digital dental technologies, existing segmentation
methodologies frequently lack rigorous validation and demonstrate limited
performance and clinical applicability. To the best of our knowledge, this is
the first work to introduce a multimodal pretraining framework for tooth
segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for
pretraining that integrates volumetric (CBCT) and surface-based (IOS)
modalities. By capturing modality-invariant representations through multimodal
contrastive learning, our approach effectively models fine-grained anatomical
features, enabling precise multi-class segmentation and accurate identification
of F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with the
framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to
date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive
collection of independent datasets, representing the largest and most diverse
evaluation to date. Our method achieves state-of-the-art performance in both
internal and external testing, with an increase of 12\% for CBCT segmentation
and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).
Furthermore, ToothMCL consistently surpasses existing approaches in tooth
groups and demonstrates robust generalizability across varying imaging
conditions and clinical scenarios.

</details>


### [68] [Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s](https://arxiv.org/abs/2509.07928)
*Mahmudul Islam Masum,Miad Islam,Arif I. Sarwat*

Main category: cs.CV

TL;DR: 通过两次适配性推理算法解决消费级硬件上的系统瓶颈问题，在RTX 4060等设备上实现了1.85倍速度提升，以少量mAP为代价


<details>
  <summary>Details</summary>
Motivation: 解决对象检测器在消费级硬件上实际性能与基准测试差距过大的问题，识别到性能瓶颈主要在系统层面而非计算能力

Method: 提出两次适配性推理算法，先使用快速低分辨率通道，仅在检测信心度低时才升级到高分辨率模型，无需模型架构改动

Result: 在5000张COCO图片数据集上，方法实现1.85倍速度提升，mAP损失仅5.51%，显著优于PyTorch Early-Exit基线

Conclusion: 通过从纯粹模型优化转向硬件感知的推理策略，为消费级设备部署高性能实时AI提供了可复现的解决方案

Abstract: As local AI grows in popularity, there is a critical gap between the
benchmark performance of object detectors and their practical viability on
consumer-grade hardware. While models like YOLOv10s promise real-time speeds,
these metrics are typically achieved on high-power, desktop-class GPUs. This
paper reveals that on resource-constrained systems, such as laptops with RTX
4060 GPUs, performance is not compute-bound but is instead dominated by
system-level bottlenecks, as illustrated by a simple bottleneck test. To
overcome this hardware-level constraint, we introduce a Two-Pass Adaptive
Inference algorithm, a model-independent approach that requires no
architectural changes. This study mainly focuses on adaptive inference
strategies and undertakes a comparative analysis of architectural early-exit
and resolution-adaptive routing, highlighting their respective trade-offs
within a unified evaluation framework. The system uses a fast, low-resolution
pass and only escalates to a high-resolution model pass when detection
confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x
speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.
This work provides a practical and reproducible blueprint for deploying
high-performance, real-time AI on consumer-grade devices by shifting the focus
from pure model optimization to hardware-aware inference strategies that
maximize throughput.

</details>


### [69] [Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object](https://arxiv.org/abs/2509.07932)
*Bala Prenith Reddy Gopu,Timothy Jacob Huber,George M. Nehma,Patrick Quinn,Madhur Tiwari,Matt Ueckermann,David Hinckley,Christopher McKenna*

Main category: cs.CV

TL;DR: 这篇论文评估了现有动态场景3D重建算法在重建漂浮转动的空间物体时的性能，通过Isaac Sim模拟环境生成物理准确的图像序列，并使用Neuralangelo在静态场景中获得了高精度的重建结果。


<details>
  <summary>Details</summary>
Motivation: 为了支持轨道服务和活性岞埃移除任务，需要准确重建漂浮转动的非合作空间物体的几何形状和运动特性。

Method: 使用Isaac Sim模拟环境生成物理准确的2D图像序列，并采用Neuralangelo算法进行静态场景的3D重建，通过Cloud Compare进行模型对比评估。

Result: 在静态场景中，Neuralangelo能够生成与原始CAD模型高度匹配的3D网格，错误和伪影最小，能够捐捉关键细节信息。

Conclusion: 研究为动态场景重建评估提供了基准，静态场景的成功重建结果表明现有算法在动态场景中也有潜力。

Abstract: Characterization of uncooperative Resident Space Objects (RSO) play a crucial
role in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions to
assess the geometry and motion properties. To address the challenges of
reconstructing tumbling uncooperative targets, this study evaluates the
performance of existing state-of-the-art 3D reconstruction algorithms for
dynamic scenes, focusing on their ability to generate geometrically accurate
models with high-fidelity. To support our evaluation, we developed a simulation
environment using Isaac Sim to generate physics-accurate 2D image sequences of
tumbling satellite under realistic orbital lighting conditions. Our preliminary
results on static scenes using Neuralangelo demonstrate promising
reconstruction quality. The generated 3D meshes closely match the original CAD
models with minimal errors and artifacts when compared using Cloud Compare
(CC). The reconstructed models were able to capture critical fine details for
mission planning. This provides a baseline for our ongoing evaluation of
dynamic scene reconstruction.

</details>


### [70] [Feature Space Analysis by Guided Diffusion Model](https://arxiv.org/abs/2509.07936)
*Kimiaki Shirahama,Miki Yanobu,Kaduki Yamashita,Miho Ohsaki*

Main category: cs.CV

TL;DR: 通过导向潜变模型实现的解码器，生成与指定特征严格匹配的图像，用于解析深度神经网络的黑盒特征空间


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络特征提取过程的黑盒性问题，进行视觉领域的特征空间分析

Method: 使用导向潜变模型实现解码器，在每个反向生成步骤中最小化清漏图像估计特征与用户指定特征的欧几里得距离

Result: 生成的图像特征与指定特征高度相似，揭示了CLIP图像编码器、ResNet-50和视觉Transformer等DNN特征空间的有价值洞察

Conclusion: 该解码器无需额外训练即可分析不同DNN的特征空间，在单台商用GPU上运行，为理解DNN内部特征表示提供了新方法

Abstract: One of the key issues in Deep Neural Networks (DNNs) is the black-box nature
of their internal feature extraction process. Targeting vision-related domains,
this paper focuses on analysing the feature space of a DNN by proposing a
decoder that can generate images whose features are guaranteed to closely match
a user-specified feature. Owing to this guarantee that is missed in past
studies, our decoder allows us to evidence which of various attributes in an
image are encoded into a feature by the DNN, by generating images whose
features are in proximity to that feature. Our decoder is implemented as a
guided diffusion model that guides the reverse image generation of a
pre-trained diffusion model to minimise the Euclidean distance between the
feature of a clean image estimated at each step and the user-specified feature.
One practical advantage of our decoder is that it can analyse feature spaces of
different DNNs with no additional training and run on a single COTS GPU. The
experimental results targeting CLIP's image encoder, ResNet-50 and vision
transformer demonstrate that images generated by our decoder have features
remarkably similar to the user-specified ones and reveal valuable insights into
these DNNs' feature spaces.

</details>


### [71] [Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images](https://arxiv.org/abs/2509.07966)
*Boammani Aser Lompo,Marc Haraoui*

Main category: cs.CV

TL;DR: Visual-TableQA是一个大规模开放域多模态数据集，专门用于评估和增强视觉语言模型对复杂表格数据的视觉推理能力，包含2.5k个LaTeX渲染表格和6k个推理密集型QA对。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在表格数据视觉推理方面的基准测试在规模、多样性和推理深度方面存在局限，特别是对于渲染表格图像的处理能力不足。

Method: 采用模块化、可扩展的全自动生成流水线，使用多个推理LLM在生成、验证和启发三个角色上协作，通过跨模型提示和LLM评审过滤实现多模型协同数据生成。

Result: 实验结果表明，在Visual-TableQA上微调的模型能够很好地泛化到外部基准测试，性能优于多个专有模型，尽管数据集是合成的。

Conclusion: Visual-TableQA为表格视觉推理提供了高质量的大规模基准，其低成本、自动化的生成方法展示了合成数据在提升模型性能方面的潜力。

Abstract: Visual reasoning over structured data such as tables is a critical capability
for modern vision-language models (VLMs), yet current benchmarks remain limited
in scale, diversity, or reasoning depth, especially when it comes to rendered
table images. Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data. Our generation pipeline is modular,
scalable, and fully autonomous, involving multiple reasoning LLMs collaborating
across distinct roles: generation, validation, and inspiration. Visual-TableQA
comprises 2.5k richly structured LaTeX-rendered tables and 6k
reasoning-intensive QA pairs, all produced at a cost of under USD 100. To
promote diversity and creativity, our pipeline performs multi-model
collaborative data generation via cross-model prompting ('inspiration') and
LLM-jury filtering. Stronger models seed layouts and topics that weaker models
elaborate, collectively distilling diverse reasoning patterns and visual
structures into the dataset. Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature. The full
pipeline and resources are publicly available at
https://github.com/AI-4-Everyone/Visual-TableQA.

</details>


### [72] [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)
*Xin Lai,Junyi Li,Wei Li,Tao Liu,Tianjian Li,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 小型o3系统通过扩大工具交互轮数和多轮推理，解决了现有多模态模型在视觉搜索任务中推理模式单调、交互轮数限制的问题，在挑战性视觉搜索任务上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态模型在视觉问题中存在推理模式单调、交互轮数限制的问题，无法满足需要尝试错误和探索的复杂任务需求。

Method: 构建Visual Probe数据集，设计迭代数据收集流水线获取多样化推理轨迹，采用超轮掩码策略防止强化学习中处罚超轮响应，平衡训练效率与测试扩展性。

Result: 模型在仅训练6轮交互的前提下，能够在推理时自然扩展到数十轮，准确性随着轮数增加而提升，产生了丰富的推理模式和深度思考路径。

Conclusion: Mini-o3系统通过扩大工具交互规模和深度推理，有效解决了挑战性视觉搜索问题，为多模态模型的深度推理提供了新的解决方案。

Abstract: Recent advances in large multimodal models have leveraged image-based tools
with reinforcement learning to tackle visual problems. However, existing
open-source approaches often exhibit monotonous reasoning patterns and allow
only a limited number of interaction turns, making them inadequate for
difficult tasks that require trial-and-error exploration. In this work, we
address this limitation by scaling up tool-based interactions and introduce
Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of
steps -- and achieves state-of-the-art performance on challenging visual search
tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key
components. First, we construct the Visual Probe Dataset, a collection of
thousands of challenging visual search problems designed for exploratory
reasoning. Second, we develop an iterative data collection pipeline to obtain
cold-start trajectories that exhibit diverse reasoning patterns, including
depth-first search, trial-and-error, and goal maintenance. Third, we propose an
over-turn masking strategy that prevents penalization of over-turn responses
(those that hit the maximum number of turns) during reinforcement learning,
thereby balancing training-time efficiency with test-time scalability. Despite
training with an upper bound of only six interaction turns, our model generates
trajectories that naturally scale to tens of turns at inference time, with
accuracy improving as the number of turns increases. Extensive experiments
demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking
paths, effectively solving challenging visual search problems.

</details>


### [73] [One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation](https://arxiv.org/abs/2509.07978)
*Zheng Geng,Nan Wang,Shaocong Xu,Chongjie Ye,Bohan Li,Zhaoxi Chen,Sida Peng,Hao Zhao*

Main category: cs.CV

TL;DR: OnePoseViaGen是一个用于单次参考图像6D姿态估计的先进方法，通过粗到精对齐和文本引导域随机化技术，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中任意未见物体的6D姿态估计问题，特别是在3D模型稀缺、单视图重建缺乏度量尺度、生成模型与真实图像存在域差距的挑战性场景。

Method: 采用两阶段方法：1) 粗到精对齐模块结合多视图特征匹配和渲染比较细化来联合优化尺度和姿态；2) 文本引导生成域随机化策略多样化纹理，使合成数据能有效微调姿态估计器。

Result: 在YCBInEOAT、Toyota-Light、LM-O等挑战性基准测试中达到最先进的性能，远超先前方法，并在真实机器人手上展示了稳健的灵巧抓取能力。

Conclusion: 该方法通过高保真单视图3D生成支持可靠的单次6D姿态估计，验证了在实际操作中的实用性，为机器人处理现实世界长尾实例提供了有效解决方案。

Abstract: Estimating the 6D pose of arbitrary unseen objects from a single reference
image is critical for robotics operating in the long-tail of real-world
instances. However, this setting is notoriously challenging: 3D models are
rarely available, single-view reconstructions lack metric scale, and domain
gaps between generated models and real-world images undermine robustness. We
propose OnePoseViaGen, a pipeline that tackles these challenges through two key
components. First, a coarse-to-fine alignment module jointly refines scale and
pose by combining multi-view feature matching with render-and-compare
refinement. Second, a text-guided generative domain randomization strategy
diversifies textures, enabling effective fine-tuning of pose estimators with
synthetic data. Together, these steps allow high-fidelity single-view 3D
generation to support reliable one-shot 6D pose estimation. On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches. We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation. Project page:
https://gzwsama.github.io/OnePoseviaGen.github.io/

</details>


### [74] [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/abs/2509.07979)
*Heeji Yoon,Jaewoo Jung,Junwan Kim,Hyungyu Choi,Heeseong Shin,Sangbeom Lim,Honggyu An,Chaehyun Kim,Jisang Han,Donghyun Kim,Chanho Eom,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: VIRAL是一种通过视觉表示对齐来增强多模态大语言模型视觉推理能力的正则化策略，通过在训练过程中对齐MLLM与预训练视觉基础模型的内部视觉表示，保留关键视觉细节并补充额外视觉知识


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉中心任务（如物体计数、空间推理）上表现有限，主要原因是文本监督范式只能为视觉通路提供间接指导，导致模型在训练过程中丢弃细粒度视觉细节

Method: 提出VIRAL正则化策略，显式地对齐MLLM与预训练视觉基础模型(VFMs)的内部视觉表示，使模型能够保留输入视觉编码器的关键视觉细节，并从VFMs补充额外视觉知识

Result: 在广泛采用的多模态基准测试中，所有任务都取得了持续改进，并通过全面的消融研究验证了关键设计选择的有效性

Conclusion: 这一简单发现为在训练MLLMs中有效整合视觉信息开辟了重要方向，VIRAL策略能够显著增强模型对复杂视觉输入的推理能力

Abstract: Multimodal large language models (MLLMs) trained with visual instruction
tuning have achieved strong performance across diverse tasks, yet they remain
limited in vision-centric tasks such as object counting or spatial reasoning.
We attribute this gap to the prevailing text-only supervision paradigm, which
provides only indirect guidance for the visual pathway and often leads MLLMs to
discard fine-grained visual details during training. In this paper, we present
VIsual Representation ALignment (VIRAL), a simple yet effective regularization
strategy that aligns the internal visual representations of MLLMs with those of
pre-trained vision foundation models (VFMs). By explicitly enforcing this
alignment, VIRAL enables the model not only to retain critical visual details
from the input vision encoder but also to complement additional visual
knowledge from VFMs, thereby enhancing its ability to reason over complex
visual inputs. Our experiments demonstrate consistent improvements across all
tasks on widely adopted multimodal benchmarks. Furthermore, we conduct
comprehensive ablation studies to validate the key design choices underlying
our framework. We believe this simple finding opens up an important direction
for the effective integration of visual information in training MLLMs.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [75] [Undecidability of Tiling with a Tromino](https://arxiv.org/abs/2509.07906)
*MIT-ULB CompGeom Group,:,Zachary Abel,Hugo Akitaya,Lily Chung,Erik D. Demaine,Jenny Diomidova,Della Hendrickson,Stefan Langerman,Jayson Lynch*

Main category: cs.CG

TL;DR: 该论文证明了使用L形或I形三连方块完成周期性初始布局的平面铺砖问题是co-RE完全的（因此不可判定），而有限初始布局或使用多米诺骨牌时该问题可判定。


<details>
  <summary>Details</summary>
Motivation: 研究周期性初始布局下特定形状方块（特别是三连方块）的铺砖问题的计算复杂性，填补了无限铺砖问题复杂性理论的研究空白。

Method: 通过将周期性铺砖问题归约到已知的co-RE完全问题，并利用周期性图论的新结果来证明复杂性结论。同时开发了针对周期性二分图的完美匹配多项式时间算法。

Result: 证明了L形和I形三连方块周期性铺砖的co-RE完全性；发现了2D和3D空间中多方块铺砖的类似复杂性结果；建立了周期性图论中多个问题的复杂性分类（co-RE完全或PSPACE完全）。

Conclusion: 周期性初始布局显著增加了铺砖问题的计算复杂性，特别是对于三连方块而言，这类问题变得不可判定。研究为无限铺砖和周期性图论的复杂性理论提供了重要理论基础。

Abstract: Given a periodic placement of copies of a tromino (either L or I), we prove
co-RE-completeness (and hence undecidability) of deciding whether it can be
completed to a plane tiling. By contrast, the problem becomes decidable if the
initial placement is finite, or if the tile is a domino instead of a tromino
(in any dimension). As a consequence, tiling a given periodic subset of the
plane with a given tromino (L or I) is co-RE-complete.
  We also prove co-RE-completeness of tiling the entire plane with two
polyominoes (one of which is disconnected and the other of which has constant
size), and of tiling 3D space with two connected polycubes (one of which has
constant size). If we restrict to tiling by translation only (no rotation),
then we obtain co-RE-completeness with one more tile: two trominoes for a
periodic subset of 2D, three polyominoes for the 2D plane, and three connected
polycubes for 3D space.
  Along the way, we prove several new complexity and algorithmic results about
periodic (infinite) graphs. Notably, we prove that Periodic Planar
(1-in-)3SAT-3, 3DM, and Graph Orientation are co-RE-complete in 2D and
PSPACE-complete in 1D; we extend basic results in graph drawing to 2D periodic
graphs; and we give a polynomial-time algorithm for perfect matching in
bipartite periodic graphs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [76] [First Plan Then Evaluate: Use a Vectorized Motion Planner for Grasping](https://arxiv.org/abs/2509.07162)
*Martin Matak,Mohanraj Devendran Ashanti,Karl Van Wyk,Tucker Hermans*

Main category: cs.RO

TL;DR: 一种并行化的机器人抓取框架，通过同时规划多个抓取目标的轨迹来提高效率和成功率，免去传统次序方式的缺陷


<details>
  <summary>Details</summary>
Motivation: 传统的生成器-评估器-规划器框架存在两难困境：要么苛费时间找到成功轨迹，要么放宽计算精度导致抓取成功率估计不准

Method: 提出并行化框架，同时规划多个抓取目标的轨迹，评估器估计各轨迹的抓取成功概率，然后执行最可能成功的轨迹，使用向量化运动规划器提高效率

Result: 实验结果显示该方法在不同物体、生成器和运动规划器上都超越传统框架，并成功泛化到真实世界的新环境中

Conclusion: 并行化规划框架有效解决了传统方法的急震问题，在提高效率的同时保证了抓取成功率的准确性

Abstract: Autonomous multi-finger grasping is a fundamental capability in robotic
manipulation. Optimization-based approaches show strong performance, but tend
to be sensitive to initialization and are potentially time-consuming. As an
alternative, the generator-evaluator-planner framework has been proposed. A
generator generates grasp candidates, an evaluator ranks the proposed grasps,
and a motion planner plans a trajectory to the highest-ranked grasp. If the
planner doesn't find a trajectory, a new trajectory optimization is started
with the next-best grasp as the target and so on. However, executing
lower-ranked grasps means a lower chance of grasp success, and multiple
trajectory optimizations are time-consuming. Alternatively, relaxing the
threshold for motion planning accuracy allows for easier computation of a
successful trajectory but implies lower accuracy in estimating grasp success
likelihood. It's a lose-lose proposition: either spend more time finding a
successful trajectory or have a worse estimate of grasp success. We propose a
framework that plans trajectories to a set of generated grasp targets in
parallel, the evaluator estimates the grasp success likelihood of the resulting
trajectories, and the robot executes the trajectory most likely to succeed. To
plan trajectories to different targets efficiently, we propose the use of a
vectorized motion planner. Our experiments show our approach improves over the
traditional generator-evaluator-planner framework across different objects,
generators, and motion planners, and successfully generalizes to novel
environments in the real world, including different shelves and table heights.
Project website https://sites.google.com/view/fpte

</details>


### [77] [Quantum Machine Learning and Grover's Algorithm for Quantum Optimization of Robotic Manipulators](https://arxiv.org/abs/2509.07216)
*Hassen Nigatu,Shi Gaokun,Li Jituo,Wang Jin,Lu Guodong,Howard Li*

Main category: cs.RO

TL;DR: 提出量子原生框架，结合量子机器学习和Grover算法，高效解决机器人运动学优化问题，在复杂高维空间中实现二次复杂度降低和显著加速


<details>
  <summary>Details</summary>
Motivation: 传统方法在高自由度机器人运动学优化中面临高维配置空间搜索的计算挑战，需要更高效的优化方法

Method: 使用参数化量子电路训练前向运动学模型，构建oracle识别最优配置，利用Grover算法实现搜索复杂度的二次降低

Result: 在1-DoF、2-DoF和双臂机械臂任务中验证，相比Nelder Mead等经典优化器获得最高93倍的加速，且随问题维度增加优势更明显

Conclusion: 建立了量子计算与机器人学问题之间的基础性量子原生框架，为机器人运动学优化提供了有效的量子解决方案

Abstract: Optimizing high-degree of freedom robotic manipulators requires searching
complex, high-dimensional configuration spaces, a task that is computationally
challenging for classical methods. This paper introduces a quantum native
framework that integrates quantum machine learning with Grover's algorithm to
solve kinematic optimization problems efficiently. A parameterized quantum
circuit is trained to approximate the forward kinematics model, which then
constructs an oracle to identify optimal configurations. Grover's algorithm
leverages this oracle to provide a quadratic reduction in search complexity.
Demonstrated on 1-DoF, 2-DoF, and dual-arm manipulator tasks, the method
achieves significant speedups-up to 93x over classical optimizers like Nelder
Mead as problem dimensionality increases. This work establishes a foundational,
quantum-native framework for robot kinematic optimization, effectively bridging
quantum computing and robotics problems.

</details>


### [78] [Safe Gap-based Planning in Dynamic Settings](https://arxiv.org/abs/2509.07239)
*Max Asselmeier,Abdel Zaro,Dhruv Ahuja,Ye Zhao,Patricio A. Vela*

Main category: cs.RO

TL;DR: 本文提出了一种动态间隙规划器，通过间隙跟踪、动态估计和间隙传播算法来处理动态环境中的障碍物，实现了在理想条件下可证明无碰撞的轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 现有的感知信息局部规划器在动态环境中往往依赖经验性鲁棒性进行避碰，缺乏对动态障碍物的形式化分析。需要开发能够明确处理动态障碍物的规划方法。

Method: 1) 跟踪自由空间的极坐标区域（间隙）并估计其动态特性；2) 通过新颖的间隙传播算法预测未来可行区域；3) 利用追踪制导理论生成可证明无碰撞的轨迹；4) 在无间隙情况下执行障碍物中心的非间隙处理。

Result: 动态间隙规划器在所有动态环境中都优于其他基准方法（包括经典和学习的运动规划器），并在TurtleBot2平台上通过真实世界实验验证了避碰行为。

Conclusion: 动态间隙规划器通过形式化分析动态障碍物和间隙传播算法，为动态环境中的局部规划提供了有效且可证明安全的解决方案。

Abstract: This chapter extends the family of perception-informed gap-based local
planners to dynamic environments. Existing perception-informed local planners
that operate in dynamic environments often rely on emergent or empirical
robustness for collision avoidance as opposed to performing formal analysis of
dynamic obstacles. This proposed planner, dynamic gap, explicitly addresses
dynamic obstacles through several steps in the planning pipeline. First, polar
regions of free space known as gaps are tracked and their dynamics are
estimated in order to understand how the local environment evolves over time.
Then, at planning time, gaps are propagated into the future through novel gap
propagation algorithms to understand what regions are feasible for passage.
Lastly, pursuit guidance theory is leveraged to generate local trajectories
that are provably collision-free under ideal conditions. Additionally,
obstacle-centric ungap processing is performed in situations where no gaps
exist to robustify the overall planning framework. A set of gap-based planners
are benchmarked against a series of classical and learned motion planners in
dynamic environments, and dynamic gap is shown to outperform all other
baselines in all environments. Furthermore, dynamic gap is deployed on a
TurtleBot2 platform in several real-world experiments to validate collision
avoidance behaviors.

</details>


### [79] [Performance Characterization of a Point-Cloud-Based Path Planner in Off-Road Terrain](https://arxiv.org/abs/2509.07321)
*Casey D. Majhor,Jeremy P. Bos*

Main category: cs.RO

TL;DR: MUONS点云导航系统在越野自主导航中表现出色，仿真测试成功率98%，实地测试零失败，研究发现Bi-RRT扩展半径对性能影响最大，仿真与实地性能高度相关


<details>
  <summary>Details</summary>
Motivation: 评估点云导航系统在越野环境中的性能，通过大规模仿真和实地测试验证系统可靠性，并确定关键性能参数

Method: 使用30,000次规划导航试验进行仿真评估，涵盖3种复杂地形和20种路径规划参数组合，通过统计和相关性分析确定关键参数

Result: 仿真成功率98%，实地测试零失败，Bi-RRT扩展半径与规划时间和路径长度相关性最强，仿真参数变化与实地性能高度相关

Conclusion: 蒙特卡洛仿真可用于性能评估和参数调优，点云导航系统在越野环境中具有高可靠性和实用性

Abstract: We present a comprehensive evaluation of a point-cloud-based navigation
stack, MUONS, for autonomous off-road navigation. Performance is characterized
by analyzing the results of 30,000 planning and navigation trials in simulation
and validated through field testing. Our simulation campaign considers three
kinematically challenging terrain maps and twenty combinations of seven
path-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98
success rate and experienced no failures in the field. By statistical and
correlation analysis we determined that the Bi-RRT expansion radius used in the
initial planning stages is most correlated with performance in terms of
planning time and traversed path length. Finally, we observed that the
proportional variation due to changes in the tuning parameters is remarkably
well correlated to performance in field testing. This finding supports the use
of Monte-Carlo simulation campaigns for performance assessment and parameter
tuning.

</details>


### [80] [Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark](https://arxiv.org/abs/2509.07362)
*Yandi Yang,Jianping Li,Youqi Liao,Yuhao Li,Yizhe Zhang,Zhen Dong,Bisheng Yang,Naser El-Sheimy*

Main category: cs.RO

TL;DR: 基于机载光轩扫描点云的空中-地面跨平台视觉定位新数据集


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中因结构缺失、视点变化和长时间偏移导致的视觉量距困难，利用公开的机载光轩扫描数据作为先验地图提高定位精度

Method: 构建大规模数据集，整合来自移动汇图系统的地面级图像与武汉、香港和旧金山的机载光轩扫描点云数据

Result: 提供了一个包含多平台数据的大规模数据集，以支持空中-地面跨平台设置下的图像到点云对齐算法验证

Conclusion: 该数据集有助于充分发挥机载光轩扫描在精确视觉定位中的潜力，解决了当前领域在数据集多样性、真实地面真值生成和算法验证方面的三大挑战

Abstract: Accurate visual localization in dense urban environments poses a fundamental
task in photogrammetry, geospatial information science, and robotics. While
imagery is a low-cost and widely accessible sensing modality, its effectiveness
on visual odometry is often limited by textureless surfaces, severe viewpoint
changes, and long-term drift. The growing public availability of airborne laser
scanning (ALS) data opens new avenues for scalable and precise visual
localization by leveraging ALS as a prior map. However, the potential of
ALS-based localization remains underexplored due to three key limitations: (1)
the lack of platform-diverse datasets, (2) the absence of reliable ground-truth
generation methods applicable to large-scale urban environments, and (3)
limited validation of existing Image-to-Point Cloud (I2P) algorithms under
aerial-ground cross-platform settings. To overcome these challenges, we
introduce a new large-scale dataset that integrates ground-level imagery from
mobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, and
San Francisco.

</details>


### [81] [TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon](https://arxiv.org/abs/2509.07381)
*Sichao Wu,Jiang Wu,Xingyu Cao,Fawang Zhang,Guangyuan Yu,Junjie Zhao,Yue Qu,Fei Ma,Jingliang Duan*

Main category: cs.RO

TL;DR: TransMPC是一种基于Transformer的显式MPC算法，通过双向自注意力机制实时生成高精度控制序列，解决了传统MPC计算复杂和现有显式MPC精度受限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统在线MPC方法计算复杂度过高，而现有显式MPC方法依赖于简化的系统动力学和成本函数，限制了在复杂系统中的精度。需要一种既能降低在线计算负担又能保持高精度的解决方案。

Method: 使用编码器型Transformer架构，利用双向自注意力机制在单次前向传播中同时推断整个控制序列。采用直接策略优化框架，通过自动微分直接优化真实有限时域成本，结合随机时域采样和回放缓冲区确保训练样本的独立同分布性。

Result: 大量仿真和真实车辆控制实验验证了TransMPC在解精度、时域适应性和计算效率方面的有效性，能够为复杂动态系统实时生成高精度控制序列。

Conclusion: TransMPC成功地将Transformer架构应用于显式MPC，实现了高精度实时控制，同时保持了计算效率，为复杂系统的模型预测控制提供了新的解决方案。

Abstract: Traditional online Model Predictive Control (MPC) methods often suffer from
excessive computational complexity, limiting their practical deployment.
Explicit MPC mitigates online computational load by pre-computing control
policies offline; however, existing explicit MPC methods typically rely on
simplified system dynamics and cost functions, restricting their accuracy for
complex systems. This paper proposes TransMPC, a novel Transformer-based
explicit MPC algorithm capable of generating highly accurate control sequences
in real-time for complex dynamic systems. Specifically, we formulate the MPC
policy as an encoder-only Transformer leveraging bidirectional self-attention,
enabling simultaneous inference of entire control sequences in a single forward
pass. This design inherently accommodates variable prediction horizons while
ensuring low inference latency. Furthermore, we introduce a direct policy
optimization framework that alternates between sampling and learning phases.
Unlike imitation-based approaches dependent on precomputed optimal
trajectories, TransMPC directly optimizes the true finite-horizon cost via
automatic differentiation. Random horizon sampling combined with a replay
buffer provides independent and identically distributed (i.i.d.) training
samples, ensuring robust generalization across varying states and horizon
lengths. Extensive simulations and real-world vehicle control experiments
validate the effectiveness of TransMPC in terms of solution accuracy,
adaptability to varying horizons, and computational efficiency.

</details>


### [82] [Attention and Risk-Aware Decision Framework for Safe Autonomous Driving](https://arxiv.org/abs/2509.07412)
*Zhen Tian,Fujiang Yuan,Yangfan He,Qinghao Li,Changlin Chen,Huilin Chen,Tianxiang Xu,Jianyu Duan,Yanhong Peng,Zhihao Lin*

Main category: cs.RO

TL;DR: 本文提出了一种改进的PPO算法，通过引入风险感知机制、风险注意力决策网络、平衡奖励函数和安全辅助机制，解决了自动驾驶中PPO算法训练效果差、效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中基于模型的方法难以应对意外事件，而现有PPO算法在长序列训练中存在训练效果差、效率低的问题，训练效果差等同于驾驶任务中的碰撞风险。

Method: 引入风险感知机制突出潜在碰撞区域；设计平衡奖励函数根据周围车辆数量调整奖励；构建风险注意力网络对高风险区域进行通道和空间注意力处理；增加安全辅助机制在车道保持和变道时监督和预防碰撞风险动作。

Result: 在物理引擎上的仿真结果表明，该算法在避碰性能上优于基准算法，获得更高的峰值奖励，训练时间更短，在多个测试交通流场景中在风险区域停留时间更短。

Conclusion: 提出的改进PPO算法有效提升了自动驾驶的安全性和训练效率，在复杂交通场景中表现出优越的性能。

Abstract: Autonomous driving has attracted great interest due to its potential
capability in full-unsupervised driving. Model-based and learning-based methods
are widely used in autonomous driving. Model-based methods rely on pre-defined
models of the environment and may struggle with unforeseen events. Proximal
policy optimization (PPO), an advanced learning-based method, can adapt to the
above limits by learning from interactions with the environment. However,
existing PPO faces challenges with poor training results, and low training
efficiency in long sequences. Moreover, the poor training results are
equivalent to collisions in driving tasks. To solve these issues, this paper
develops an improved PPO by introducing the risk-aware mechanism, a
risk-attention decision network, a balanced reward function, and a
safety-assisted mechanism. The risk-aware mechanism focuses on highlighting
areas with potential collisions, facilitating safe-driving learning of the PPO.
The balanced reward function adjusts rewards based on the number of surrounding
vehicles, promoting efficient exploration of the control strategy during
training. Additionally, the risk-attention network enhances the PPO to hold
channel and spatial attention for the high-risk areas of input images.
Moreover, the safety-assisted mechanism supervises and prevents the actions
with risks of collisions during the lane keeping and lane changing. Simulation
results on a physical engine demonstrate that the proposed algorithm
outperforms benchmark algorithms in collision avoidance, achieving higher peak
reward with less training time, and shorter driving time remaining on the risky
areas among multiple testing traffic flow scenarios.

</details>


### [83] [Robust Docking Maneuvers for Autonomous Trolley Collection: An Optimization-Based Visual Servoing Scheme](https://arxiv.org/abs/2509.07413)
*Yuhan Pang,Bingyi Xia,Zhe Zhang,Zhirui Sun,Peijia Xie,Bike Zhu,Wenjun Xu,Jiankun Wang*

Main category: cs.RO

TL;DR: 通过优化基于视觉服务的方案，结合主动红外标记咄干扰观测器，实现了高精度的服务机器人对接扫握操作


<details>
  <summary>Details</summary>
Motivation: 解决服务机器人在公共空间自主收集和重新分配手提车时，面临的高精度对接挑战：环境干扰、光照变化和机器人本身约束

Method: 采用优化基于视觉服务的方案，结合主动红外标记以在多样光照条件下稳健特征提取，显式建模非完整动力学咄可见性约束，并增强干扰抵消观测器

Result: 多样环境实验结果验证了系统的稳健性，定量评估确认了高的对接准确度

Conclusion: 该框架能够有效解决服务机器人对接扫握的挑战，实现了精准稳定的自主对接操作，为公共空间的手提车自主收集咄运输提供了可靠技术支撑

Abstract: Service robots have demonstrated significant potential for autonomous trolley
collection and redistribution in public spaces like airports or warehouses to
improve efficiency and reduce cost. Usually, a fully autonomous system for the
collection and transportation of multiple trolleys is based on a
Leader-Follower formation of mobile manipulators, where reliable docking
maneuvers of the mobile base are essential to align trolleys into organized
queues. However, developing a vision-based robotic docking system faces
significant challenges: high precision requirements, environmental
disturbances, and inherent robot constraints. To address these challenges, we
propose an optimization-based Visual Servoing scheme that incorporates active
infrared markers for robust feature extraction across diverse lighting
conditions. This framework explicitly models nonholonomic kinematics and
visibility constraints within the Hybrid Visual Servoing problem, augmented
with an observer for disturbance rejection to ensure precise and stable
docking. Experimental results across diverse environments demonstrate the
robustness of this system, with quantitative evaluations confirming high
docking accuracy.

</details>


### [84] [Timing the Message: Language-Based Notifications for Time-Critical Assistive Settings](https://arxiv.org/abs/2509.07438)
*Ya-Chuan Hsu,Jonathan DeCastro,Andrew Silva,Guy Rosman*

Main category: cs.RO

TL;DR: 该研究提出了一个结合强化学习和离线分类数据集的框架，用于在时间关键的人机协助中平衡及时性和信息性，相比忽略时间延迟的方法成功率提高40%以上。


<details>
  <summary>Details</summary>
Motivation: 在时间关键场景中，现有语言辅助系统主要关注内容生成而忽略了关键的时间因素（如语言传达时长、人类理解延迟等），这些时间考虑对结果有重大影响。

Method: 将挑战构建为增强状态马尔可夫决策过程的顺序决策问题，设计结合强化学习和生成离线分类数据集的框架，实现可扩展的分类数据集生成流程。

Result: 通过合成人类实验评估，该框架相比忽略时间延迟的方法成功率提高超过40%，有效平衡了及时性和信息性。

Conclusion: 该研究揭示了及时性和信息性之间常被忽视的权衡关系，为优化时间关键人机协助中的通信开辟了新方向。

Abstract: In time-critical settings such as assistive driving, assistants often rely on
alerts or haptic signals to prompt rapid human attention, but these cues
usually leave humans to interpret situations and decide responses
independently, introducing potential delays or ambiguity in meaning.
Language-based assistive systems can instead provide instructions backed by
context, offering more informative guidance. However, current approaches (e.g.,
social assistive robots) largely prioritize content generation while
overlooking critical timing factors such as verbal conveyance duration, human
comprehension delays, and subsequent follow-through duration. These timing
considerations are crucial in time-critical settings, where even minor delays
can substantially affect outcomes. We aim to study this inherent trade-off
between timeliness and informativeness by framing the challenge as a sequential
decision-making problem using an augmented-state Markov Decision Process. We
design a framework combining reinforcement learning and a generated offline
taxonomy dataset, where we balance the trade-off while enabling a scalable
taxonomy dataset generation pipeline. Empirical evaluation with synthetic
humans shows our framework improves success rates by over 40% compared to
methods that ignore time delays, while effectively balancing timeliness and
informativeness. It also exposes an often-overlooked trade-off between these
two factors, opening new directions for optimizing communication in
time-critical human-AI assistance.

</details>


### [85] [Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions](https://arxiv.org/abs/2509.07445)
*Harrison Field,Max Yang,Yijiong Lin,Efi Psomopoulou,David Barton,Nathan F. Lepora*

Main category: cs.RO

TL;DR: Text2Touch利用大语言模型自动设计奖励函数，在配备视觉触觉传感器的灵巧手中实现多轴物体旋转任务，显著优于人工设计的奖励函数


<details>
  <summary>Details</summary>
Motivation: 现有工作未考虑触觉感知在灵巧操作中的关键作用，而触觉对人类灵巧性至关重要。需要探索LLM在触觉感知任务中的奖励设计能力

Method: 采用提示工程策略处理70多个环境变量，通过模拟到真实的蒸馏方法将策略迁移到配备触觉传感器的四指灵巧手机器人

Result: Text2Touch显著优于人工设计的基线方法，表现出更高的旋转速度和稳定性，同时奖励函数长度和复杂度降低了一个数量级

Conclusion: LLM设计的奖励函数可以显著缩短从概念到可部署灵巧触觉技能的时间，支持更快速和可扩展的多模态机器人学习

Abstract: Large language models (LLMs) are beginning to automate reward design for
dexterous manipulation. However, no prior work has considered tactile sensing,
which is known to be critical for human-like dexterity. We present Text2Touch,
bringing LLM-crafted rewards to the challenging task of multi-axis in-hand
object rotation with real-world vision based tactile sensing in palm-up and
palm-down configurations. Our prompt engineering strategy scales to over 70
environment variables, and sim-to-real distillation enables successful policy
transfer to a tactile-enabled fully actuated four-fingered dexterous robot
hand. Text2Touch significantly outperforms a carefully tuned human-engineered
baseline, demonstrating superior rotation speed and stability while relying on
reward functions that are an order of magnitude shorter and simpler. These
results illustrate how LLM-designed rewards can significantly reduce the time
from concept to deployable dexterous tactile skills, supporting more rapid and
scalable multimodal robot learning. Project website:
https://hpfield.github.io/text2touch-website

</details>


### [86] [DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis](https://arxiv.org/abs/2509.07463)
*Sven Kirchner,Nils Purschke,Ross Greer,Alois C. Knoll*

Main category: cs.RO

TL;DR: DepthVision是一个多模态场景理解框架，通过合成LiDAR点云的RGB图像并结合真实RGB数据，在视觉输入退化时提高机器人操作的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人视觉输入退化或不足时的可靠操作问题，特别是在低光照等恶劣条件下确保安全性。

Method: 使用条件生成对抗网络从稀疏LiDAR点云合成RGB图像，通过Luminance-Aware Modality Adaptation动态融合合成图像和真实RGB数据，无需微调下游视觉语言模型。

Result: 在真实和模拟数据集上的评估显示，该方法在低光照条件下显著优于仅使用RGB的基线方法，同时保持与冻结VLMs的兼容性。

Conclusion: LiDAR引导的RGB合成方法具有在真实环境中实现鲁棒机器人操作的潜力，特别是在安全关键任务中。

Abstract: Ensuring reliable robot operation when visual input is degraded or
insufficient remains a central challenge in robotics. This letter introduces
DepthVision, a framework for multimodal scene understanding designed to address
this problem. Unlike existing Vision-Language Models (VLMs), which use only
camera-based visual input alongside language, DepthVision synthesizes RGB
images from sparse LiDAR point clouds using a conditional generative
adversarial network (GAN) with an integrated refiner network. These synthetic
views are then combined with real RGB data using a Luminance-Aware Modality
Adaptation (LAMA), which blends the two types of data dynamically based on
ambient lighting conditions. This approach compensates for sensor degradation,
such as darkness or motion blur, without requiring any fine-tuning of
downstream vision-language models. We evaluate DepthVision on real and
simulated datasets across various models and tasks, with particular attention
to safety-critical tasks. The results demonstrate that our approach improves
performance in low-light conditions, achieving substantial gains over RGB-only
baselines while preserving compatibility with frozen VLMs. This work highlights
the potential of LiDAR-guided RGB synthesis for achieving robust robot
operation in real-world environments.

</details>


### [87] [Safe and Non-Conservative Contingency Planning for Autonomous Vehicles via Online Learning-Based Reachable Set Barriers](https://arxiv.org/abs/2509.07464)
*Rui Yang,Lei Zheng,Shuzhi Sam Ge,Jun Ma*

Main category: cs.RO

TL;DR: 提出实时应急轨迹优化框架，通过事件触发在线学习量化多模态不确定性，使用FRS屏障约束确保安全，在保持安全的同时显著提升驾驶效率和舒适性


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要在动态不确定环境中平衡安全性和驾驶效率，传统方法要么过于保守影响效率，要么确定性方法在遇到突发情况时存在安全风险

Method: 采用事件触发在线学习HV控制意图集，动态量化多模态不确定性并增量精化前向可达集，通过基于FRS的屏障约束确保不变安全性，使用共识ADMM高效求解应急轨迹优化

Result: 在高速公路和城市场景的高保真仿真以及真实世界实验中，在不确定性条件下保持安全的同时显著提高了驾驶效率和乘客舒适性

Conclusion: 该框架能够持续适应HV行为的不确定性，在不依赖过度保守策略的情况下保持可行性和安全性，为自动驾驶在动态不确定环境中的安全导航提供了有效解决方案

Abstract: Autonomous vehicles must navigate dynamically uncertain environments while
balancing the safety and driving efficiency. This challenge is exacerbated by
the unpredictable nature of surrounding human-driven vehicles (HVs) and
perception inaccuracies, which require planners to adapt to evolving
uncertainties while maintaining safe trajectories. Overly conservative planners
degrade driving efficiency, while deterministic approaches may encounter
serious issues and risks of failure when faced with sudden and unexpected
maneuvers. To address these issues, we propose a real-time contingency
trajectory optimization framework in this paper. By employing event-triggered
online learning of HV control-intent sets, our method dynamically quantifies
multi-modal HV uncertainties and refines the forward reachable set (FRS)
incrementally. Crucially, we enforce invariant safety through FRS-based barrier
constraints that ensure safety without reliance on accurate trajectory
prediction of HVs. These constraints are embedded in contingency trajectory
optimization and solved efficiently through consensus alternative direction
method of multipliers (ADMM). The system continuously adapts to the
uncertainties in HV behaviors, preserving feasibility and safety without
resorting to excessive conservatism. High-fidelity simulations on highway and
urban scenarios, as well as a series of real-world experiments demonstrate
significant improvements in driving efficiency and passenger comfort while
maintaining safety under uncertainty. The project page is available at
https://pathetiue.github.io/frscp.github.io/.

</details>


### [88] [Flexible Morphing Aerial Robot with Inflatable Structure for Perching-based Human-Robot Interaction](https://arxiv.org/abs/2509.07496)
*Ayano Miyamichi,Moju Zhao,Kazuki Sugihara,Junichiro Sugihara,Masanori Konishi,Kunio Kojima,Kei Okada,Masayuki Inaba*

Main category: cs.RO

TL;DR: 基于混合变形结构的可变形空中机器人，通过单偶性等条件保证飞行稳定性，实现了在人体上的俯息互动能力


<details>
  <summary>Details</summary>
Motivation: 解决可变形空中机器人飞行稳定性挑战和人机俯息互动的安全性、适应性需求，开发能够在人体上安全俯息的空中机器人

Method: 设计混合变形结构，结合单侧灵活臂和气动吞吐扭，飞行时保持硬性、俯息时转为软性；发展气动控制系统，集成减震和可调抓取力；分析单偶性灵活臂特性，确定标准四旋翼模型控制有效的充分条件

Result: 原型机验证了在人体上的适应性俯息机动，甚至在飞行中推力减少导致臂部变形后仍能稳固恢复，首次实现了能够在人体上俯息互动的空中机器人

Conclusion: 混合变形结构和气动控制系统有效解决了可变形空中机器人的飞行稳定性和人机俯息安全性问题，为人机互动应用开启了新可能

Abstract: Birds in nature perform perching not only for rest but also for interaction
with human such as the relationship with falconers. Recently, researchers
achieve perching-capable aerial robots as a way to save energy, and deformable
structure demonstrate significant advantages in efficiency of perching and
compactness of configuration. However, ensuring flight stability remains
challenging for deformable aerial robots due to the difficulty of controlling
flexible arms. Furthermore, perching for human interaction requires high
compliance along with safety. Thus, this study aims to develop a deformable
aerial robot capable of perching on humans with high flexibility and grasping
ability. To overcome the challenges of stability of both flight and perching,
we propose a hybrid morphing structure that combines a unilateral flexible arm
and a pneumatic inflatable actuators. This design allows the robot's arms to
remain rigid during flight and soft while perching for more effective grasping.
We also develop a pneumatic control system that optimizes pressure regulation
while integrating shock absorption and adjustable grasping forces, enhancing
interaction capabilities and energy efficiency. Besides, we focus on the
structural characteristics of the unilateral flexible arm and identify
sufficient conditions under which standard quadrotor modeling and control
remain effective in terms of flight stability. Finally, the developed prototype
demonstrates the feasibility of compliant perching maneuvers on humans, as well
as the robust recovery even after arm deformation caused by thrust reductions
during flight. To the best of our knowledge, this work is the first to achieve
an aerial robot capable of perching on humans for interaction.

</details>


### [89] [OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics](https://arxiv.org/abs/2509.07500)
*Yinan Deng,Yufeng Yue,Jianyu Dou,Jingyu Zhao,Jiahui Wang,Yujie Tang,Yi Yang,Mengyin Fu*

Main category: cs.RO

TL;DR: OmniMap是首个实时在线3D建图框架，同时捕获光学、几何和语义信息，通过3DGS-Voxel混合表示实现高保真渲染、精确几何和零样本语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能部分满足机器人系统对逼真外观、精确几何和开放词汇语义理解的需求，存在光学模糊、几何不规则和语义模糊等问题。

Method: 采用紧密耦合的3DGS-Voxel混合表示，结合自适应相机建模、带法向约束的混合增量表示和概率融合技术，实现多模态信息的实时处理。

Result: 在多种场景下，OmniMap在渲染保真度、几何精度和零样本语义分割方面均优于最先进方法，并支持多种下游应用。

Conclusion: OmniMap首次实现了实时在线同时捕获光学、几何和语义属性的3D建图，为机器人感知提供了全面解决方案。

Abstract: Robotic systems demand accurate and comprehensive 3D environment perception,
requiring simultaneous capture of photo-realistic appearance (optical), precise
layout shape (geometric), and open-vocabulary scene understanding (semantic).
Existing methods typically achieve only partial fulfillment of these
requirements while exhibiting optical blurring, geometric irregularities, and
semantic ambiguities. To address these challenges, we propose OmniMap. Overall,
OmniMap represents the first online mapping framework that simultaneously
captures optical, geometric, and semantic scene attributes while maintaining
real-time performance and model compactness. At the architectural level,
OmniMap employs a tightly coupled 3DGS-Voxel hybrid representation that
combines fine-grained modeling with structural stability. At the implementation
level, OmniMap identifies key challenges across different modalities and
introduces several innovations: adaptive camera modeling for motion blur and
exposure compensation, hybrid incremental representation with normal
constraints, and probabilistic fusion for robust instance-level understanding.
Extensive experiments show OmniMap's superior performance in rendering
fidelity, geometric accuracy, and zero-shot semantic segmentation compared to
state-of-the-art methods across diverse scenes. The framework's versatility is
further evidenced through a variety of downstream applications, including
multi-domain scene Q&A, interactive editing, perception-guided manipulation,
and map-assisted navigation.

</details>


### [90] [Improving Machine Learning-Based Robot Self-Collision Checking with Input Positional Encoding](https://arxiv.org/abs/2509.07542)
*Bartlomiej Kulecki,Dominik Belter*

Main category: cs.RO

TL;DR: 将位置编码技术集成到自碰撞检测的二元分类模型中，可提高分类精度，使模型能更好地捕捉高频变化，比传统几何方法更快速


<details>
  <summary>Details</summary>
Motivation: 研究位置编码技术在自碰撞检测中的应用，探索机器学习方法相比传统几何方法在碰撞检测中的优势

Method: 使用轻量级多层感知器(MLP)在低维特征空间中操作，将位置编码技术集成到输入向量中

Result: 集成位置编码提高了分类准确性，使模型能更好地捕捉复杂碰撞模式的高频变化，提供了比传统三角形相交测试和包围体层次结构(BVH)更快的碰撞检测方案

Conclusion: 基于机器学习的方法结合位置编码技术，为网格模型的自碰撞检测提供了比传统几何方法更高效准确的解决方案

Abstract: This manuscript investigates the integration of positional encoding -- a
technique widely used in computer graphics -- into the input vector of a binary
classification model for self-collision detection. The results demonstrate the
benefits of incorporating positional encoding, which enhances classification
accuracy by enabling the model to better capture high-frequency variations,
leading to a more detailed and precise representation of complex collision
patterns. The manuscript shows that machine learning-based techniques, such as
lightweight multilayer perceptrons (MLPs) operating in a low-dimensional
feature space, offer a faster alternative for collision checking than
traditional methods that rely on geometric approaches, such as
triangle-to-triangle intersection tests and Bounding Volume Hierarchies (BVH)
for mesh-based models.

</details>


### [91] [Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?](https://arxiv.org/abs/2509.07593)
*Gavin Tao,Yinuo Wang,Jinzhao Zhou*

Main category: cs.RO

TL;DR: 基于SSD-Mamba2状态空间双重性的视觉驱动跨模态加强学习框架，通过近线性扩展实现低延迟、高效的感知融合，在运动控制任务中超越了现有的Transformer基线


<details>
  <summary>Details</summary>
Motivation: 解决绘制控制器中的计算-内存交换问题：循环控制器长期信贵分配困难，Transformer融合在token长度上产生二次成本，限制了时空上下文的利用

Method: 构建基于SSD-Mamba2的视觉驱动跨模态RL框架，利用状态空间双重性实现循环和卷积扫描，将体感和外感观测编码为紧凑token并通过堆叠的SSD-Mamba2层融合，在有限计算下进行稳定训练

Result: 在多样化运动控制场景中，方法在回报、安全性（碰撞和跌落）和样本效率方面一致超越现有最佳基线，同时在相同计算预算下更快收敛

Conclusion: SSD-Mamba2为可扩展、具有预见能力和高效的端到端运动控制提供了一个实用的融合背骨

Abstract: End-to-end reinforcement learning for motion control promises unified
perception-action policies that scale across embodiments and tasks, yet most
deployed controllers are either blind (proprioception-only) or rely on fusion
backbones with unfavorable compute-memory trade-offs. Recurrent controllers
struggle with long-horizon credit assignment, and Transformer-based fusion
incurs quadratic cost in token length, limiting temporal and spatial context.
We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a
selective state-space backbone that applies state-space duality (SSD) to enable
both recurrent and convolutional scanning with hardware-aware streaming and
near-linear scaling. Proprioceptive states and exteroceptive observations
(e.g., depth tokens) are encoded into compact tokens and fused by stacked
SSD-Mamba2 layers. The selective state-space updates retain long-range
dependencies with markedly lower latency and memory use than quadratic
self-attention, enabling longer look-ahead, higher token resolution, and stable
training under limited compute. Policies are trained end-to-end under curricula
that randomize terrain and appearance and progressively increase scene
complexity. A compact, state-centric reward balances task progress, energy
efficiency, and safety. Across diverse motion-control scenarios, our approach
consistently surpasses strong state-of-the-art baselines in return, safety
(collisions and falls), and sample efficiency, while converging faster at the
same compute budget. These results suggest that SSD-Mamba2 provides a practical
fusion backbone for scalable, foresightful, and efficient end-to-end motion
control.

</details>


### [92] [Decoding RobKiNet: Insights into Efficient Training of Robotic Kinematics Informed Neural Network](https://arxiv.org/abs/2509.07646)
*Yanlong Peng,Zhigang Wang,Ziwen He,Pengxu Chang,Chuangchuang Zhou,Yu Yan,Ming Chen*

Main category: cs.RO

TL;DR: RobKiNet是一种基于运动学知识的神经网络方法，用于机器人在多约束配置空间中进行高效采样，相比传统方法和深度强化学习，训练速度提升74.29倍，采样精度达99.25%，实际任务完成率97.33%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人任务与运动规划中传统采样方法在多级约束下效率低下的问题，需要一种能够满足任务级全局约束并提高后续运动规划效率的配置空间采样方法。

Method: 提出RobKiNet运动学信息神经网络，采用端到端方式在连续可行集(CFS)内进行多约束配置空间采样，建立优化期望模型，通过运动学知识注入确保稳定准确的梯度优化。

Result: 在2-DOF空间中验证理论效率，在9-DOF自主移动机械臂上展示优越的全身和解耦控制性能，训练速度比深度强化学习快74.29倍，采样精度达99.25%，电池拆卸任务完成率97.33%。

Conclusion: RobKiNet通过运动学知识注入显著提高了训练效率和采样精度，在复杂机器人任务中表现出色，为多约束配置空间采样提供了有效的解决方案。

Abstract: In robots task and motion planning (TAMP), it is crucial to sample within the
robot's configuration space to meet task-level global constraints and enhance
the efficiency of subsequent motion planning. Due to the complexity of joint
configuration sampling under multi-level constraints, traditional methods often
lack efficiency. This paper introduces the principle of RobKiNet, a
kinematics-informed neural network, for end-to-end sampling within the
Continuous Feasible Set (CFS) under multiple constraints in configuration
space, establishing its Optimization Expectation Model. Comparisons with
traditional sampling and learning-based approaches reveal that RobKiNet's
kinematic knowledge infusion enhances training efficiency by ensuring stable
and accurate gradient optimization.Visualizations and quantitative analyses in
a 2-DOF space validate its theoretical efficiency, while its application on a
9-DOF autonomous mobile manipulator robot(AMMR) demonstrates superior
whole-body and decoupled control, excelling in battery disassembly tasks.
RobKiNet outperforms deep reinforcement learning with a training speed 74.29
times faster and a sampling accuracy of up to 99.25%, achieving a 97.33% task
completion rate in real-world scenarios.

</details>


### [93] [Collaborative Exploration with a Marsupial Ground-Aerial Robot Team through Task-Driven Map Compression](https://arxiv.org/abs/2509.07655)
*Angelos Zacharia,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种地面-空中机器人协作探索框架，利用图基路径规划和带宽高效的地图压缩策略，在有限通信条件下提高大规模未知环境的探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决自主机器人在有限通信的封闭大规模环境中高效探索的挑战，利用地面和空中平台的互补能力来最大化覆盖范围和效率。

Method: 采用图基路径规划算法指导探索，在预期收益显著超过地面机器人的区域部署空中机器人；引入带宽高效的任务驱动地图压缩策略，选择性压缩和共享关键数据。

Result: 仿真和真实世界实验验证了该方法的有效性，在显著减少数据传输的同时提高了探索效率。

Conclusion: 所提出的协作探索框架能够有效解决有限通信条件下的大规模环境探索问题，通过智能部署和高效数据压缩实现了优越的性能。

Abstract: Efficient exploration of unknown environments is crucial for autonomous
robots, especially in confined and large-scale scenarios with limited
communication. To address this challenge, we propose a collaborative
exploration framework for a marsupial ground-aerial robot team that leverages
the complementary capabilities of both platforms. The framework employs a
graph-based path planning algorithm to guide exploration and deploy the aerial
robot in areas where its expected gain significantly exceeds that of the ground
robot, such as large open spaces or regions inaccessible to the ground
platform, thereby maximizing coverage and efficiency. To facilitate large-scale
spatial information sharing, we introduce a bandwidth-efficient, task-driven
map compression strategy. This method enables each robot to reconstruct
resolution-specific volumetric maps while preserving exploration-critical
details, even at high compression rates. By selectively compressing and sharing
key data, communication overhead is minimized, ensuring effective map
integration for collaborative path planning. Simulation and real-world
experiments validate the proposed approach, demonstrating its effectiveness in
improving exploration efficiency while significantly reducing data
transmission.

</details>


### [94] [Temporal Counterfactual Explanations of Behaviour Tree Decisions](https://arxiv.org/abs/2509.07674)
*Tamlin Love,Antonio Andriella,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了一种自动生成行为树反事实解释的新方法，通过构建因果模型来回答机器人决策的"为什么"问题


<details>
  <summary>Details</summary>
Motivation: 行为树是机器人决策控制的流行框架，但现有方法无法生成因果性的反事实解释来详细说明机器人决策和行为的原因

Method: 首先从行为树结构和领域知识自动构建因果模型，然后查询和搜索该模型以找到多样化的反事实解释

Result: 该方法能够正确解释各种行为树结构和状态的行为，能够回答广泛的因果查询

Conclusion: 该方法代表了向更透明、可理解和最终可信赖的机器人系统迈出的一步

Abstract: Explainability is a critical tool in helping stakeholders understand robots.
In particular, the ability for robots to explain why they have made a
particular decision or behaved in a certain way is useful in this regard.
Behaviour trees are a popular framework for controlling the decision-making of
robots and other software systems, and thus a natural question to ask is
whether or not a system driven by a behaviour tree is capable of answering
"why" questions. While explainability for behaviour trees has seen some prior
attention, no existing methods are capable of generating causal, counterfactual
explanations which detail the reasons for robot decisions and behaviour.
Therefore, in this work, we introduce a novel approach which automatically
generates counterfactual explanations in response to contrastive "why"
questions. Our method achieves this by first automatically building a causal
model from the structure of the behaviour tree as well as domain knowledge
about the state and individual behaviour tree nodes. The resultant causal model
is then queried and searched to find a set of diverse counterfactual
explanations. We demonstrate that our approach is able to correctly explain the
behaviour of a wide range of behaviour tree structures and states. By being
able to answer a wide range of causal queries, our approach represents a step
towards more transparent, understandable and ultimately trustworthy robotic
systems.

</details>


### [95] [Robust Radar SLAM for Vehicle Parking Applications](https://arxiv.org/abs/2509.07683)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We address ego-motion estimation for automated parking, where
centimeter-level accuracy is crucial due to tight spaces and nearby obstacles.
Traditional methods using inertial-measurement units and wheel encoders require
calibration, making them costly and time-consuming. To overcome this, we
propose a radar-based simultaneous localization and mapping (SLAM) approach
that leverages the robustness of radar to adverse weather and support for
online calibration. Our robocentric formulation fuses feature positions and
Doppler velocities for robust data association and filter convergence. Key
contributions include a Doppler-augmented radar SLAM method, multi-radar
support and an information-based feature-pruning strategy. Experiments
demonstrate high-accuracy localization and improved robustness over
state-of-the-art methods, meeting the demands of automated parking.

</details>


### [96] [Fault Tolerant Control of a Quadcopter using Reinforcement Learning](https://arxiv.org/abs/2509.07707)
*Muzaffar Habib,Adnan Maqsood,Adnan Fayyaz ud Din*

Main category: cs.RO

TL;DR: 基于强化学习的四旋翼控制框架，专注于单螺旋桨故障时的安全性和鲁棒性，比较了动态规划和DDPG两种方法。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼在飞行中发生螺旋桨故障时保持期望高度的问题，确保硬件和载荷安全，需要鲁棒的控制策略。

Method: 采用动态规划(DP)和深度确定性策略梯度(DDPG)两种强化学习方法，对算法进行修改以处理大维度连续状态和动作空间。

Result: 通过MATLAB环境下的广泛仿真验证了控制框架的鲁棒性，在不同初始条件下都能实现期望状态。

Conclusion: 提出的控制框架在四旋翼故障情况下表现出良好的性能，两种RL算法在故障空中系统中都有应用潜力。

Abstract: This study presents a novel reinforcement learning (RL)-based control
framework aimed at enhancing the safety and robustness of the quadcopter, with
a specific focus on resilience to in-flight one propeller failure. Addressing
the critical need of a robust control strategy for maintaining a desired
altitude for the quadcopter to safe the hardware and the payload in physical
applications. The proposed framework investigates two RL methodologies Dynamic
Programming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the
challenges posed by the rotor failure mechanism of the quadcopter. DP, a
model-based approach, is leveraged for its convergence guarantees, despite high
computational demands, whereas DDPG, a model-free technique, facilitates rapid
computation but with constraints on solution duration. The research challenge
arises from training RL algorithms on large dimensions and action domains. With
modifications to the existing DP and DDPG algorithms, the controllers were
trained not only to cater for large continuous state and action domain and also
achieve a desired state after an inflight propeller failure. To verify the
robustness of the proposed control framework, extensive simulations were
conducted in a MATLAB environment across various initial conditions and
underscoring its viability for mission-critical quadcopter applications. A
comparative analysis was performed between both RL algorithms and their
potential for applications in faulty aerial systems.

</details>


### [97] [Unlocking Stopped-Rotor Flight: Development and Validation of SPERO, a Novel UAV Platform](https://arxiv.org/abs/2509.07812)
*Kristan Hilby,Ian Hunter*

Main category: cs.RO

TL;DR: SPERO是一种创新的停止旋翼无人机，通过翻转锁定机翼、压力中心调节机制、推力矢量平衡等创新设计，首次实现了垂直起降与平飞模式之间的稳定双向转换


<details>
  <summary>Details</summary>
Motivation: 停止旋翼飞机长期以来被认为是垂直起降和平飞时间相等的理想飞行器，但由于气动和稳定性问题，实际应用一直不可行

Method: 采用翻转锁定机翼、主动压力中心机制、推力矢量平衡装置、五旋翼架构，以及11状态机的飞行控制器协调几何和控制重构

Result: 克服了停止旋翼飞行的长期挑战，实现了首个稳定的VTOL与平飞之间的双向转换

Conclusion: SPERO为停止旋翼无人机建立了可推广的设计和控制框架，解决了该领域的关键技术难题

Abstract: Stop-rotor aircraft have long been proposed as the ideal vertical takeoff and
landing (VTOL) aircraft for missions with equal time spent in both flight
regimes, such as agricultural monitoring, search and rescue, and last-mile
delivery. Featuring a central lifting surface that rotates in VTOL to generate
vertical thrust and locks in forward flight to generate passive lift, the
stop-rotor offers the potential for high efficiency across both modes. However,
practical implementation has remained infeasible due to aerodynamic and
stability conflicts between flight modes. In this work, we present SPERO
(Stopped-Penta Rotor), a stop-rotor uncrewed aerial vehicle (UAV) featuring a
flipping and latching wing, an active center of pressure mechanism, thrust
vectored counterbalances, a five-rotor architecture, and an eleven-state
machine flight controller coordinating geometric and controller
reconfiguration. Furthermore, SPERO establishes a generalizable design and
control framework for stopped-rotor UAVs. Together, these innovations overcome
longstanding challenges in stop-rotor flight and enable the first stable,
bidirectional transition between VTOL and forward flight.

</details>


### [98] [Programmable Locking Cells (PLC) for Modular Robots with High Stiffness Tunability and Morphological Adaptability](https://arxiv.org/abs/2509.07916)
*Jianshu Zhou,Wei Chen,Junda Huang,Boyuan Liang,Yunhui Liu,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出了一种模块化的可编程锁定单元(PLC)，通过机械互锁关节实现离散刚度调节，能够在柔性和刚性状态间切换，适用于可重构机器人结构。


<details>
  <summary>Details</summary>
Motivation: 机器人系统在非结构化环境中需要能够在柔性和刚性状态间切换，以执行自适应抓取、高力操作、形状保持等任务。现有可变刚度方案存在结构复杂、需要持续输入功率或整体设计等问题，限制了模块化和可扩展性。

Method: 设计了模块化的肌腱驱动单元PLC，通过电缆张力驱动的机械互锁关节实现离散刚度调节。每个单元通过结构啮合在柔性和坚固状态间转换，多个单元可组装成空间可编程刚度的可重构机器人结构。

Result: 每个单元实现高达950%的刚度变化，在坚固状态下能承受高载荷而不易损坏。开发了两个功能原型：可变刚度夹爪（自适应抓取、牢固保持和手内操作）和管道穿越机器人（在受限环境中实现形状适应性和刚度控制）。

Conclusion: PLC作为一种可扩展的结构中心机制，实现了可编程刚度和运动，使机器人系统具有可重构形态和任务自适应交互能力。

Abstract: Robotic systems operating in unstructured environments require the ability to
switch between compliant and rigid states to perform diverse tasks such as
adaptive grasping, high-force manipulation, shape holding, and navigation in
constrained spaces, among others. However, many existing variable stiffness
solutions rely on complex actuation schemes, continuous input power, or
monolithic designs, limiting their modularity and scalability. This paper
presents the Programmable Locking Cell (PLC)-a modular, tendon-driven unit that
achieves discrete stiffness modulation through mechanically interlocked joints
actuated by cable tension. Each unit transitions between compliant and firm
states via structural engagement, and the assembled system exhibits high
stiffness variation-up to 950% per unit-without susceptibility to damage under
high payload in the firm state. Multiple PLC units can be assembled into
reconfigurable robotic structures with spatially programmable stiffness. We
validate the design through two functional prototypes: (1) a variable-stiffness
gripper capable of adaptive grasping, firm holding, and in-hand manipulation;
and (2) a pipe-traversing robot composed of serial PLC units that achieves
shape adaptability and stiffness control in confined environments. These
results demonstrate the PLC as a scalable, structure-centric mechanism for
programmable stiffness and motion, enabling robotic systems with reconfigurable
morphology and task-adaptive interaction.

</details>


### [99] [RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction](https://arxiv.org/abs/2509.07953)
*Zheyuan Hu,Robyn Wu,Naveen Enock,Jasmine Li,Riya Kadakia,Zackory Erickson,Aviral Kumar*

Main category: cs.RO

TL;DR: RaC通过人类干预回放训练，在模仿学习预训练后加入人类回滚和纠正行为，显著提升机器人长时程任务的效率和鲁棒性，使用10倍少的数据达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类遥操作的专家数据收集方法效率低下，导致接触丰富、可变形物体和长时程任务的性能远低于完美执行，即使有数千条专家演示。

Method: 在模仿学习预训练后，引入RaC阶段：在策略rollout时，人类操作者在失败临近时介入，先回滚机器人到熟悉状态，然后提供纠正段完成当前子任务，基于这种数据组合进行微调。

Result: 在三个真实世界双手机器人任务（挂衬衫、密封容器盖、打包外卖盒）和一个模拟装配任务中，RaC使用10倍少的数据收集时间和样本超越了之前的最先进方法。

Conclusion: RaC通过训练恢复和适应行为扩展了机器人技能库，实现了测试时性能扩展：训练后的RaC策略性能与其展现的恢复操作数量呈线性比例关系。

Abstract: Modern paradigms for robot imitation train expressive policy architectures on
large amounts of human demonstration data. Yet performance on contact-rich,
deformable-object, and long-horizon tasks plateau far below perfect execution,
even with thousands of expert demonstrations. This is due to the inefficiency
of existing ``expert'' data collection procedures based on human teleoperation.
To address this issue, we introduce RaC, a new phase of training on
human-in-the-loop rollouts after imitation learning pre-training. In RaC, we
fine-tune a robotic policy on human intervention trajectories that illustrate
recovery and correction behaviors. Specifically, during a policy rollout, human
operators intervene when failure appears imminent, first rewinding the robot
back to a familiar, in-distribution state and then providing a corrective
segment that completes the current sub-task. Training on this data composition
expands the robotic skill repertoire to include retry and adaptation behaviors,
which we show are crucial for boosting both efficiency and robustness on
long-horizon tasks. Across three real-world bimanual control tasks: shirt
hanging, airtight container lid sealing, takeout box packing, and a simulated
assembly task, RaC outperforms the prior state-of-the-art using 10$\times$ less
data collection time and samples. We also show that RaC enables test-time
scaling: the performance of the trained RaC policy scales linearly in the
number of recovery maneuvers it exhibits. Videos of the learned policy are
available at https://rac-scaling-robot.github.io/.

</details>


### [100] [Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation](https://arxiv.org/abs/2509.07957)
*Shunlei Li,Longsen Gao,Jiuwen Cao,Yingbai Hu*

Main category: cs.RO

TL;DR: GF-VLA是一个从人类视频演示中学习双手机器人技能的统一框架，通过信息论方法提取任务相关线索，构建时序场景图，生成层次行为树和可解释运动基元，在双手机器人装配任务中达到90%以上的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于轨迹复制的方法在泛化到不同物体、空间布局和机械臂配置时存在局限性，需要一种能够进行任务级推理和执行的方法。

Method: 采用信息论方法提取关键的手-物体和物体-物体交互线索，构建时序场景图，结合语言条件变换器生成层次行为树和笛卡尔运动基元，并提出跨臂分配策略。

Result: 在四个双手机器人积木装配基准测试中，图准确率超过95%，子任务分割准确率93%，抓取可靠性94%，放置精度89%，整体任务成功率90%。

Conclusion: GF-VLA框架能够从RGB-D人类演示中提取有效的任务表示，生成鲁棒且可解释的任务策略，在多样化的空间和语义变化下表现出强大的泛化能力和鲁棒性。

Abstract: Acquiring dexterous robotic skills from human video demonstrations remains a
significant challenge, largely due to conventional reliance on low-level
trajectory replication, which often fails to generalize across varying objects,
spatial layouts, and manipulator configurations. To address this limitation, we
introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that
enables dual-arm robotic systems to perform task-level reasoning and execution
directly from RGB-D human demonstrations. GF-VLA employs an
information-theoretic approach to extract task-relevant cues, selectively
highlighting critical hand-object and object-object interactions. These cues
are structured into temporally ordered scene graphs, which are subsequently
integrated with a language-conditioned transformer to produce hierarchical
behavior trees and interpretable Cartesian motion primitives. To enhance
efficiency in bimanual execution, we propose a cross-arm allocation strategy
that autonomously determines gripper assignment without requiring explicit
geometric modeling. We validate GF-VLA on four dual-arm block assembly
benchmarks involving symbolic structure construction and spatial
generalization. Empirical results demonstrate that the proposed representation
achieves over 95% graph accuracy and 93% subtask segmentation, enabling the
language-action planner to generate robust, interpretable task policies. When
deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89%
placement accuracy, and 90% overall task success across stacking,
letter-formation, and geometric reconfiguration tasks, evidencing strong
generalization and robustness under diverse spatial and semantic variations.

</details>


### [101] [TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models](https://arxiv.org/abs/2509.07962)
*Zongzheng Zhang,Haobo Xu,Zhuo Yang,Chenghao Yue,Zehao Lin,Huan-ang Gao,Ziwei Wang,Hao Zhao*

Main category: cs.RO

TL;DR: 本文研究如何在视觉-语言-动作模型中集成转矩信号，提出了转矩调适器和辅助输出预测等方法，在接触丰富的操纵任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作模型缺乏对转矩等力信号的感知能力，而这些信号对于判断任务是否成功完成和闭环控制至关重要。

Method: 系统性研究了将转矩信号集成到VLA架构中的设计空间，包括：1)在解码器中引入转矩调适器；2)受自驾驶预测与规划的启发，预测转矩作为辅助输出。

Result: 实验结果显示：在解码器中插入转矩调适器的方法表现更优；预测转矩作为辅助输出能进一步提升性能，并促进模型建立物理基础的交互动力学内部表征。

Conclusion: 通过系统性的设计空间研究，本文成功将转矩信号集成到VLA模型中，为接触丰富的机器人操纵任务提供了更好的力感知能力。

Abstract: Many robotic manipulation tasks require sensing and responding to force
signals such as torque to assess whether the task has been successfully
completed and to enable closed-loop control. However, current
Vision-Language-Action (VLA) models lack the ability to integrate such subtle
physical feedback. In this work, we explore Torque-aware VLA models, aiming to
bridge this gap by systematically studying the design space for incorporating
torque signals into existing VLA architectures. We identify and evaluate
several strategies, leading to three key findings. First, introducing torque
adapters into the decoder consistently outperforms inserting them into the
encoder.Third, inspired by joint prediction and planning paradigms in
autonomous driving, we propose predicting torque as an auxiliary output, which
further improves performance. This strategy encourages the model to build a
physically grounded internal representation of interaction dynamics. Extensive
quantitative and qualitative experiments across contact-rich manipulation
benchmarks validate our findings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [102] [On design, analysis, and hybrid manufacturing of microstructured blade-like geometries](https://arxiv.org/abs/2509.07044)
*Pablo Antolin,Michael Barton,Georges-Pierre Bonneau,Annalisa Buffa,Amaia Calleja-Ochoa,Gershon Elber,Stefanie Elgeti,Gaizka Gómez Escudero,Alicia Gonzalez,Haizea González Barrio,Stefanie Hahmann,Thibaut Hirschler,Q Youn Honga,Konstantin Key,Myung-Soo Kim,Michael Kofler,Norberto Lopez de Lacalle,Silvia de la Maza,Kanika Rajain,Jacques Zwar*

Main category: cs.GR

TL;DR: 通过异质微规结构设计，在保持功能性的同时大幅减少材料使用量，提出了从设计到检验的统一制造流程


<details>
  <summary>Details</summary>
Motivation: 质疑传统CAD图形学范式，利用多材料3D打印技术创造更轻、更便宜但功能相同的异质微规结构对象

Method: 提出统一制造流程，包括设计、优化、制造和检验阶段，采用异质自由形成内部微规结构

Result: 在空气动力学缆筒叶片工业测试中，在维持压力极限的前提下，对比实心对象大幅节省材料

Conclusion: 异质微规结构CAD能够突破传统图形学范式，实现质量轻、成本低但功能相同的制造新方案

Abstract: With the evolution of new manufacturing technologies such as multi-material
3D printing, one can think of new type of objects that consist of considerably
less, yet heterogeneous, material, consequently being porous, lighter and
cheaper, while having the very same functionality as the original object when
manufactured from one single solid material. We aim at questioning five decades
of traditional paradigms in geometric CAD and focus at new generation of CAD
objects that are not solid, but contain heterogeneous free-form internal
microstructures. We propose a unified manufacturing pipeline that involves all
stages, namely design, optimization, manufacturing, and inspection of
microstructured free-form geometries. We demonstrate our pipeline on an
industrial test case of a blisk blade that sustains the desired pressure
limits, yet requires significantly less material when compared to the solid
counterpart.

</details>


### [103] [SVGauge: Towards Human-Aligned Evaluation for SVG Generation](https://arxiv.org/abs/2509.07127)
*Leonardo Zini,Elia Frigieri,Sebastiano Aloscari,Marcello Generali,Lorenzo Dodi,Robert Dosen,Lorenzo Baraldi*

Main category: cs.GR

TL;DR: SVGauge是第一个针对文本到SVG生成的人类对齐参考指标，通过联合测量视觉保真度和语义一致性，在SVG评估中实现了最高的人类判断相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像评估指标（如FID、LPIPS、CLIPScore）无法满足SVG图像的符号性和矢量性特点，需要专门针对SVG的评估标准。

Method: SVGauge联合测量：(1)视觉保真度：通过提取SigLIP图像嵌入，使用PCA和白化进行域对齐；(2)语义一致性：通过比较BLIP-2生成的SVG标题与原始提示在SBERT和TF-IDF组合空间中的相似度。

Result: 在SHE基准测试中，SVGauge获得了与人类判断的最高相关性，比现有指标更忠实地复现了8个零样本LLM生成器的系统级排名。

Conclusion: 研究结果强调了矢量特定评估的必要性，并为未来文本到SVG生成模型的基准测试提供了实用工具。

Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria
tuned to their symbolic and vectorial nature: criteria that existing metrics
such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce
SVGauge, the first human-aligned, reference based metric for text-to-SVG
generation. SVGauge jointly measures (i) visual fidelity, obtained by
extracting SigLIP image embeddings and refining them with PCA and whitening for
domain alignment, and (ii) semantic consistency, captured by comparing
BLIP-2-generated captions of the SVGs against the original prompts in the
combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark
shows that SVGauge attains the highest correlation with human judgments and
reproduces system-level rankings of eight zero-shot LLM-based generators more
faithfully than existing metrics. Our results highlight the necessity of
vector-specific evaluation and provide a practical tool for benchmarking future
text-to-SVG generation models.

</details>


### [104] [Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests](https://arxiv.org/abs/2509.07175)
*Yanyang Xiao,Yao Li,Juan Cao,Zhonggui Chen*

Main category: cs.GR

TL;DR: 提出一种基于边的搜索策略的高效方法来计算剪切或限制Voronoi图，通过点在单元内测试和有效剪切平面检测，并支持GPU并行计算。


<details>
  <summary>Details</summary>
Motivation: Voronoi图在许多应用中重要，但现有计算方法效率有待提升。需要一种更高效的算法来计算限制域内的Voronoi图。

Method: 使用边基搜索策略，通过点在单元内测试判断空间点是否在目标Voronoi单元中。对中间域单元交叉的每条边，只在两个端点分别在单元内外时才进行剪切，使用少量测试找到剪切平面。

Result: 实验结果显示，该方法在任何站点分布情况下都比现有最优方法表现更好，具有最佳性能。

Conclusion: 该方法只涉及对最终结果有贡献的剪切操作，显著提高了计算效率，并可扩展到GPU并行计算，为Voronoi图计算提供了一种高效的新方法。

Abstract: Since the Voronoi diagram appears in many applications, the topic of
improving its computational efficiency remains attractive. We propose a novel
yet efficient method to compute Voronoi diagrams bounded by a given domain,
i.e., the clipped or restricted Voronoi diagrams. The intersection of the
domain and a Voronoi cell (domain-cell intersection) is generated by removing
the part outside the cell from the domain, which can be accomplished by several
clippings. Different from the existing methods, we present an edge-based search
scheme to find clipping planes (bisectors). A test called point-in-cell is
first set up to tell whether a space point is in a target Voronoi cell or not.
Then, for each edge of the intermediate domain-cell intersection, we will
launch a clipping only if its two endpoints are respectively inside and outside
the corresponding Voronoi cell, where the bisector for the clipping can be
found by using a few times of point-in-cell tests. Therefore, our method only
involves the clippings that contribute to the final results, which is a great
advantage over the state-of-the-art methods. Additionally, because each
domain-cell intersection can be generated independently, we extend the proposed
method to the GPUs for computing Voronoi diagrams in parallel. The experimental
results show the best performance of our method compared to state-of-the-art
ones, regardless of site distribution. This paper was first submitted to
SIGGRAPH Asia 2025.

</details>


### [105] [Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials](https://arxiv.org/abs/2509.07522)
*Jierui Ren,Haojie Jin,Bo Pang,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 通过反射感知的光线锥编码改进神经辐射方法，高效模拟高频出射辐射分布，实现实时高质量渲染


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射方法主要依靠位置特征编码，在模拟高频、强视角依赖的辐射分布时存在显著局限性

Method: 提出反射感知的光线锥编码，使用预滤波的多分辨率哈希网格近似光滑BSDF缝，通过连续空间聚合将视角依赖反射特性嵌入编码过程

Result: 方法显著提升了网络模拟高频反射分布的能力，能够处理从高光滑到低光滑的各种表面，减轻了网络拟合复杂辐射分布的负担

Conclusion: 综合实验结果证明该方法能够在各种光滑条件下产生高质量、无噪声的实时渲染结果，与基准方法相比具有更高的保真度和现实感

Abstract: Modeling of high-frequency outgoing radiance distributions has long been a
key challenge in rendering, particularly for glossy material. Such
distributions concentrate radiative energy within a narrow lobe and are highly
sensitive to changes in view direction. However, existing neural radiosity
methods, which primarily rely on positional feature encoding, exhibit notable
limitations in capturing these high-frequency, strongly view-dependent radiance
distributions. To address this, we propose a highly-efficient approach by
reflectance-aware ray cone encoding based on the neural radiosity framework,
named neural cone radiosity. The core idea is to employ a pre-filtered
multi-resolution hash grid to accurately approximate the glossy BSDF lobe,
embedding view-dependent reflectance characteristics directly into the encoding
process through continuous spatial aggregation. Our design not only
significantly improves the network's ability to model high-frequency reflection
distributions but also effectively handles surfaces with a wide range of
glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our
method reduces the network's burden in fitting complex radiance distributions,
allowing the overall architecture to remain compact and efficient.
Comprehensive experimental results demonstrate that our method consistently
produces high-quality, noise-free renderings in real time under various
glossiness conditions, and delivers superior fidelity and realism compared to
baseline approaches.

</details>


### [106] [Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos](https://arxiv.org/abs/2509.07653)
*Yuheng Jiang,Chengcheng Guo,Yize Wu,Yu Hong,Shengkun Zhu,Zhehao Shen,Yingliang Zhang,Shaohui Jiao,Zhuo Su,Lan Xu,Marc Habermann,Christian Theobalt*

Main category: cs.GR

TL;DR: TaoGS是一种拓扑感知的动态高斯表示方法，通过分离运动和外观来实现长距离跟踪和拓扑适应，支持高效压缩和高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中拓扑变化和长时跟踪的挑战，为沉浸式体验提供鲁棒的体积视频建模方案。

Method: 使用稀疏运动高斯表示场景运动，通过时空跟踪器和光度线索更新；每个运动高斯锚定并动态激活局部外观高斯；引入全局高斯查找表记录高斯生命周期。

Result: 能够处理拓扑变化（如换衣服），支持40倍压缩，显著减少训练时间，保持时间一致性。

Conclusion: TaoGS为拓扑变化下的可扩展体积视频提供了统一的适应性解决方案，实现了运动与静止的和谐统一。

Abstract: Volumetric video is emerging as a key medium for digitizing the dynamic
physical world, creating the virtual environments with six degrees of freedom
to deliver immersive user experiences. However, robustly modeling general
dynamic scenes, especially those involving topological changes while
maintaining long-term tracking remains a fundamental challenge. In this paper,
we present TaoGS, a novel topology-aware dynamic Gaussian representation that
disentangles motion and appearance to support, both, long-range tracking and
topological adaptation. We represent scene motion with a sparse set of motion
Gaussians, which are continuously updated by a spatio-temporal tracker and
photometric cues that detect structural variations across frames. To capture
fine-grained texture, each motion Gaussian anchors and dynamically activates a
set of local appearance Gaussians, which are non-rigidly warped to the current
frame to provide strong initialization and significantly reduce training time.
This activation mechanism enables efficient modeling of detailed textures and
maintains temporal coherence, allowing high-fidelity rendering even under
challenging scenarios such as changing clothes. To enable seamless integration
into codec-based volumetric formats, we introduce a global Gaussian Lookup
Table that records the lifespan of each Gaussian and organizes attributes into
a lifespan-aware 2D layout. This structure aligns naturally with standard video
codecs and supports up to 40 compression. TaoGS provides a unified, adaptive
solution for scalable volumetric video under topological variation, capturing
moments where "elegance in motion" and "Power in Stillness", delivering
immersive experiences that harmonize with the physical world.

</details>
