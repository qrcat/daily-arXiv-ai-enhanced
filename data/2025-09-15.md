<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.RO](#cs.RO) [Total: 22]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS是一个包含50种常见超市物品的高质量3D纹理网格数据集，专为机器人和计算机视觉基准测试设计，具有成本效益和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖合成模型或难以获取的专业物品，缺乏真实世界超市常见物品的高质量3D数据，限制了实际应用的基准测试效果。

Method: 采用运动恢复结构技术，通过高分辨率成像采集物品的3D数据，生成水密网格模型，涵盖10个不同类别的物品。

Result: 创建了一个包含50种易获取超市物品的全面3D数据集，物品具有多样化的形状、尺寸和重量，为基准测试提供了高质量的真实数据。

Conclusion: ASOS数据集以其可访问性和实际应用价值，为物体检测、姿态估计和机器人应用提供了重要的基准测试资源，填补了现有数据集的空白。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态检索增强生成框架(MM-RAG)，用于自然灾害后房屋损坏评估，通过双分支编码器和跨模态交互模块实现图像和文本的语义对齐，在检索准确率和损坏严重程度分类指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自然灾害后准确评估房屋损坏对于保险理赔和资源规划至关重要，需要结合图像特征和文本政策信息进行综合分析。

Method: 基于经典RAG架构设计双分支多模态编码器：图像分支使用ResNet和Transformer提取建筑物损坏特征，文本分支使用BERT检索器处理文本向量化；集成跨模态交互模块通过多头注意力实现语义对齐；引入模态注意力门控机制动态控制生成过程中视觉证据和文本先验信息的作用；采用端到端训练，结合对比损失、检索损失和生成损失进行多任务优化。

Result: 在检索准确率和损坏严重程度分类指标上表现出优异性能，Top-1检索准确率提高了9.6%。

Conclusion: 该MM-RAG框架通过有效的多模态融合和端到端训练，显著提升了自然灾害后房屋损坏评估的准确性和效率，为保险理赔和资源规划提供了可靠的技术支持。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的集成框架，通过多图像增强变体和自定义对齐器提升历史文档转录准确率4个百分点


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档中基于LLM的文本提取不稳定问题，提高转录准确性

Method: 使用Gemini 2.0 Flash对多个增强图像变体进行转录，通过自定义Needleman Wunsch风格对齐器融合输出，生成共识转录和置信度分数

Result: 在622份宾夕法尼亚州死亡记录数据集上，相比单次转录基线准确率提升4个百分点；填充和模糊处理对提升准确率最有效，网格扭曲扰动最适合区分高低置信度案例

Conclusion: 该方法简单、可扩展，可立即部署到其他文档集合和转录模型

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出了首个大规模多模态智能交通监控数据集MITS，包含17万张真实交通监控图像和500万条指令跟随数据，显著提升了主流大模型在交通监控任务上的性能


<details>
  <summary>Details</summary>
Motivation: 通用大模型在智能交通监控领域表现有限，主要缺乏专门的交通监控多模态数据集

Method: 构建MITS数据集，包含170,400张真实交通监控图像，通过系统化数据生成流程产生高质量图像描述和500万条问答对，覆盖5个关键交通监控任务

Result: 在MITS上微调后，LLaVA-1.5性能从0.494提升到0.905(+83.2%)，LLaVA-1.6从0.678到0.921(+35.8%)，Qwen2-VL从0.584到0.926(+58.6%)，Qwen2.5-VL从0.732到0.930(+27.0%)

Conclusion: MITS数据集有效解决了交通监控领域的数据缺失问题，显著提升了大模型在该领域的性能，为ITS和LMM研究提供了高价值资源

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 本文研究了基于决策树的结构化推理是否能提升视觉语言模型在细粒度分类任务中的性能，发现虽然模型能很好理解树状知识，但树基推理始终不如标准零样本提示方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本视觉分类方面表现出色，但在细粒度任务和大规模层次标签空间中的性能尚未充分研究，本文旨在探索结构化树基推理是否能增强VLM性能。

Method: 引入一个框架，使用决策树将分类分解为可解释的决策，在细粒度(GTSRB)和粗粒度(CIFAR-10)数据集上进行评估，并探索用LLM生成的类别和图像描述来增强树提示。

Result: 模型在理解树知识方面达到98.2%准确率，但树基推理始终不如标准零样本提示。添加图像描述后，树基方法和零样本方法的性能都有所提升。

Conclusion: 研究结果揭示了结构化推理在视觉分类中的局限性，为设计更可解释的VLM系统提供了见解。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI是一个从数据中学习可控制、可提示的世界模型的系统，通过概率预测、结构提取和集成三个步骤循环训练，在14万亿视频token上训练后能进行视频预测、理解，并提取光流、深度和物体分割等先进结构。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够从数据中学习丰富可控和灵活提示的世界模型，支持对任何变量集的完整条件分布学习，并提取数据中的底层低维属性作为有意义的中间结构。

Method: 采用三步循环：1）概率预测-构建概率图模型Psi；2）结构提取-通过因果推断零样本提取底层结构；3）集成-将结构转换为新token类型并混合回训练中作为条件信号和预测目标。

Result: 在14万亿互联网视频token上训练PSI实例，成功执行各种视频预测和理解推理，提取了最先进的光流、自监督深度和物体分割，并通过这些结构实现了预测改进的完整循环。

Conclusion: PSI系统通过循环训练不断增强模型能力，既能更好地建模底层数据，又创造了新的控制机制，类似于LLM式的通用提示语言，为世界建模提供了有效的框架。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次分析了联邦学习中视频数据的梯度反演攻击风险，发现特征提取器能提供更好的保护，但攻击者仍可通过超分辨率技术重建高质量视频。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过只交换模型更新而非原始数据来保护隐私，但梯度反演攻击能从中重建敏感数据。虽然这种攻击在图像、文本和表格数据中已有研究，但对视频数据的影响尚未被探索。

Method: 评估两种视频分类方法：使用预训练特征提取器的方法和处理原始视频帧的简单变换方法。测试在不同攻击场景下（零参考帧、一个参考帧、多个参考帧）的梯度反演攻击效果，并使用图像超分辨率技术提升重建质量。

Result: 特征提取器对梯度反演攻击具有更强的抵抗力，但若分类器复杂度不足，数据泄露仍可能发生。超分辨率技术能显著提升攻击者重建视频的质量。

Conclusion: 视频数据在联邦学习中存在可行的泄露威胁，需要进一步研究其发生条件和防护措施。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出了一种用于密集零售环境物体检测的半监督协同训练框架，结合Faster R-CNN和YOLO进行伪标签交换，使用集成分类器增强鲁棒性，并通过元启发式算法优化超参数，在SKU-110k数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决密集零售环境中标注数据有限、遮挡和重叠物体复杂条件下的检测挑战，降低人工标注成本并适应零售场景中频繁的产品和布局变化。

Method: 采用半监督协同训练框架，结合Faster R-CNN（ResNet骨干）进行精确定位和YOLO（Darknet骨干）获取全局上下文，通过伪标签交换提升准确性；使用XGBoost、随机森林和SVM的集成分类器增强分类鲁棒性；应用元启发式算法优化超参数。

Result: 在SKU-110k数据集上表现出强大的性能，证明了该方法在精确度、效率和可扩展性方面的优势。

Conclusion: 该框架具有实际应用价值，可有效用于自动化库存跟踪、产品监控和结账系统等现实零售应用场景，显著降低了标注成本并适应零售环境的变化。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: Token Purging (PG) 是一种无需反向传播的测试时适应方法，通过移除受域偏移影响严重的token来提升3D点云分类性能，在准确率、速度和内存效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分类中由于分布偏移导致的性能下降问题，现有测试时适应方法需要反向传播迭代更新，计算成本高且不适合实时部署。

Method: 提出Token Purging方法，在token级别移除受域偏移影响的token，避免其进入注意力层。包含两个变体：PG-SP（利用源域统计信息）和PG-SF（完全无源域，基于CLS-token驱动适应）。

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C数据集上，PG-SP比最先进的无反向传播方法平均准确率高10.3%，PG-SF在无源域适应方面创下新基准。PG比基线方法快12.4倍，内存效率高5.5倍。

Conclusion: Token Purging是一种高效、轻量级的测试时适应方法，无需反向传播即可有效处理域偏移问题，适合实际部署应用。

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出了一种精确且高度可解释的跨视角细粒度定位方法，通过匹配地面图像与参考航拍图像的局部特征来估计3自由度姿态，避免了传统鸟瞰图转换中的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将地面图像转换为鸟瞰图表示再与航拍图像对齐，这种转换常因透视畸变或高度信息压缩导致信息丢失，从而降低对齐质量。

Method: 直接在原始图像间建立对应关系，仅将匹配的关键点使用单目深度先验提升到鸟瞰空间。支持度量和相对深度，采用尺度感知的Procrustes对齐来估计相机姿态，并在使用相对深度时可选地恢复尺度。

Result: 实验结果表明，仅需相机姿态的弱监督，该方法就能学习准确的局部特征对应关系，在跨区域泛化和未知方向等挑战性条件下实现优越的定位性能，且兼容各种相对深度模型而无需针对每个模型进行微调。

Conclusion: 该方法具有灵活性、强大的定位性能和实际部署适用性，为跨视角定位提供了一种有效解决方案。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 开发基于智能手机的儿童视力筛查应用KidsVisionCheck，使用深度学习分析红眼反射图像，准确率达90%，无需专业设备


<details>
  <summary>Details</summary>
Motivation: 传统Bruckner测试需要眼科医生在临床环境中进行，希望通过智能手机和AI技术让视力筛查更便捷普及

Method: 使用眼科医生收集标注的儿童瞳孔图像训练深度神经网络模型

Result: 在未见测试数据上达到90%的准确率，能够确定最佳数据收集条件并提供即时用户反馈

Conclusion: 这是实现全球儿童视力筛查可及性和早期干预视力异常的重要第一步

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于深度引导的多模态融合方法DGFusion，通过深度感知特征和注意力机制动态调整传感器融合策略，在自动驾驶语义感知任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的传感器融合方法通常在整个输入空间上统一处理传感器数据，在挑战性条件下性能受限。需要一种能够根据空间变化条件动态调整融合策略的方法。

Method: 提出DGFusion网络，将多模态分割作为多任务问题处理。利用LiDAR测量作为输入和深度真值，通过辅助深度头学习深度感知特征，编码为空间变化的局部深度token，结合全局条件token进行注意力跨模态融合。

Result: 在具有挑战性的MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。

Conclusion: 深度引导的多模态融合方法能够有效适应传感器在不同深度条件下的可靠性变化，显著提升了自动驾驶语义感知的鲁棒性。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了基于ResNet-18深度学习框架的补丁基自动蜗红痘检测策略，通过分析不同大小、形状和位置的面部图像补丁来提高检测精度咋敏感性，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 蜗红痘作为一种治疗效果依赖于准确时早检测的治疗性皮肤疾病，需要开发更有效的自动检测方法来提高诊断效果。

Method: 使用ResNet-18深度学习框架，从面部图像中提取多种大小、形状和位置的图像补丁，评估局部视觉信息对模型性能的影响，并与全图基准方法进行比较。

Result: 实验结果显示补丁基策略在准确性咋敏感性方面达到或超过了全图基准方法，同时能够保护患者隐私，提高模型的稳健性咋可解释性。

Conclusion: 该研究提供的补丁基自动蜗红痘检测策略不仅提高了检测性能，还保护了患者隐私，为改善自动化皮肤科诊断提供了实用的见解。

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出了一种基于临床先验知识和合成数据的隐私保护型玫瑰痤疮自动检测方法，通过构建红色通道掩膜聚焦诊断相关区域，使用ResNet-18在合成数据上训练，在真实测试数据上取得了优于全脸基线方法的性能


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮是一种常见但诊断不足的炎症性皮肤病，自动检测面临症状弥散、标注数据稀缺以及面部图像隐私问题的挑战，需要开发隐私保护的自动化检测方法

Method: 首先基于临床观察构建红色通道强度掩膜，聚焦脸颊、鼻子和额头等诊断相关区域并排除身份识别特征；然后在掩膜后的合成图像上训练ResNet-18深度学习模型

Result: 在真实世界测试数据上，该方法在准确率、召回率和F1分数方面均显著优于全脸基线方法，证明了合成数据和临床先验知识的有效性

Conclusion: 合成数据和临床先验知识可以共同实现准确且符合伦理的皮肤病AI系统，特别适用于远程医疗和大规模筛查等隐私敏感应用场景

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文对ULW腹腔镜图像去烟框架进行了全面的消融研究，系统评估了可学习维纳滤波器和复合损失函数中各损失项对整体性能的具体贡献。


<details>
  <summary>Details</summary>
Motivation: 为了严格评估ULW框架中各个组件的有效性和必要性，需要系统地分析每个组成部分对整体性能的具体贡献。

Method: 采用消融研究方法，包括：(1)移除可学习维纳滤波器模块；(2)选择性使用复合损失函数中的单个损失项（MSE、SSIM损失、感知损失）。所有变体在公开的配对腹腔镜图像数据集上进行基准测试。

Result: 研究使用定量指标（SSIM、PSNR、MSE和CIEDE-2000）和定性视觉比较来评估不同变体的性能表现。

Conclusion: 通过系统的消融分析，明确了ULW框架中各个组件的重要性，为腹腔镜图像去烟技术的优化提供了重要见解。

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: WAVE-DETR是一个结合可见光RGB和声学信号的多模态无人机检测器，通过融合视觉和声学特征，在Deformable DETR和Wav2Vec2架构基础上实现了在挑战性环境条件下的强性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂环境条件下无人机检测的鲁棒性问题，研究者希望通过融合视觉和声学两种模态的信息来提升检测性能，特别是在小尺寸无人机检测方面。

Method: 基于Deformable DETR和Wav2Vec2架构，开发了四种不同的融合配置（门控机制、线性层、MLP和交叉注意力），将声学嵌入与多分辨率特征映射融合。使用了Drone-vs-Bird数据集和新生成的包含7500多个同步图像和音频片段的ARDrone数据集。

Result: 最佳性能的门控融合方法将Deformable DETR目标检测器的mAP在ARDrone数据集上提升了11.1%到15.3%（小无人机，IoU阈值0.5-0.9）。中等和大尺寸无人机的mAP也有所提升，所有尺寸无人机的整体增益在3.27%到5.84%之间。

Conclusion: 声学信息能够有效提升Deformable DETR目标检测器的性能，特别是在小尺寸无人机检测方面。多模态融合方法在真实无人机检测场景中表现出色，门控融合是最有效的融合策略。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出了一种称为代理监督的新训练范式，通过将估计的空间变换应用于代理图像来解耦输入域和监督域，从而提升深度学习配准网络对输入图像变化的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习配准方法虽然精度高，但对输入图像特性变化（如伪影、视野不匹配、模态差异）敏感，需要开发能提高网络鲁棒性和泛化性的通用训练方法。

Method: 引入代理监督框架，将输入域与监督域解耦，通过对代理图像应用估计的空间变换来确保在相似性定义良好的域中进行监督计算，支持在异质输入上训练。

Result: 在三个代表性应用（抗伪影脑MR配准、掩码无关肺CT配准、多模态MR配准）中均表现出对输入变化的强鲁棒性，同时在良好处理数据上保持高性能。

Conclusion: 代理监督为训练鲁棒且可泛化的深度学习配准模型提供了原则性框架，不增加复杂度，为医学图像配准在多样化生物医学成像场景中的更广泛应用提供了实用途径。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 提出结合卷积自编码器和Vision Transformer的框架，提升牙齿年龄估计的准确性和可解释性，发现第三磨牙的高类内形态变异是性能限制因素


<details>
  <summary>Details</summary>
Motivation: 深度学习在法医牙科年龄估计等高风险应用中面临'黑盒'问题，需要提升模型性能和透明度

Method: 使用卷积自编码器(AE)与Vision Transformer(ViT)结合的框架，分析潜在空间指标和图像重建来提供多维度诊断

Result: 分类准确率显著提升：第二磨牙从0.712提高到0.815，第三磨牙从0.462提高到0.543，发现第三磨牙数据集的高类内形态变异是主要限制因素

Conclusion: 单一可解释性方法不足，需要多维度分析框架来支持法医年龄估计的专家决策，同时提升准确性和不确定性解释能力

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA是一种无源域自适应方法，通过自监督预训练和几何流形对齐来解决传统SFDA方法丢失几何信息的问题，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统无源域自适应方法依赖于全监督预训练的源模型，通过余弦相似度对齐实例级特征，但这种方法会丢弃源模型潜在流形的关键几何信息。

Method: 提出自监督持续域自适应框架：1) 使用自监督预训练的教师模型而非监督预训练；2) 引入几何流形对齐原则，学生模型通过实例级特征匹配和空间相似性损失的复合目标进行训练；3) 使用指数移动平均更新教师参数以防止灾难性遗忘。

Result: 在多个基准数据集上的广泛实验表明，SCoDA显著优于最先进的无源域自适应方法。

Conclusion: SCoDA通过自监督预训练和几何流形对齐有效解决了传统SFDA方法的局限性，在无源域自适应任务中取得了优异性能。

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出了一种基于Segment Anything 2 (SAM2)的零样本细胞追踪框架，无需训练数据即可在2D和3D显微镜视频中实现竞争性精度的细胞追踪和有丝分裂检测


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的细胞追踪方法依赖昂贵的人工标注数据集，且由于显微镜数据的巨大多样性，其泛化能力有限

Method: 将大型基础模型Segment Anything 2 (SAM2)集成到追踪流程中，构建完全无监督的零样本追踪框架，不依赖任何特定训练数据集

Result: 在2D和大规模3D延时显微镜视频中实现了竞争性的准确度，无需数据集特定的适配

Conclusion: 该方法克服了传统方法的局限性，提供了强大的泛化能力，能够处理细胞分裂、低信噪比、模糊边界、密集集群等挑战

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出了一种将现有2D多摄像头跟踪系统扩展到3D空间的方法，通过深度信息重建目标点云并恢复3D边界框，在AI City Challenge 2025中获得第三名


<details>
  <summary>Details</summary>
Motivation: 现有的多目标多摄像头跟踪系统主要基于2D空间，虽然相机标定和深度信息可以将目标投影到3D空间，但完全重新构建3D跟踪系统成本高昂且不现实

Method: 利用深度信息在点云空间中重建目标，通过聚类和偏航角细化恢复3D边界框，并引入增强的在线数据关联机制，利用目标的局部ID一致性在帧间分配全局ID

Result: 在2025 AI City Challenge的3D MTMC数据集上进行了评估，在排行榜上获得了第三名的成绩

Conclusion: 该方法成功地将现有2D多摄像头跟踪系统扩展到3D空间，无需完全重新构建系统，为大规模监控系统的3D感知提供了可行的解决方案

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 本文提出了一种针对微型和纳米无人机优化的高效视觉惯性里程计（VIO）流水线，通过量化优化和刚性运动模型，在超低功耗RISC-V SoC上实现实时高精度运动估计


<details>
  <summary>Details</summary>
Motivation: 传统高精度VIO流水线需要强大计算系统，而微控制器上的轻量级实现精度不足。本文旨在弥合这一差距，为微型无人机提供既高效又准确的VIO解决方案

Method: 结合SuperPoint、PX4FLOW和ORB等先进特征检测跟踪方法，进行量化优化以适应RISC-V超低功耗并行SoC。采用刚性运动模型减少平面运动场景的估计误差

Result: 在GAP9低功耗SoC上，优化流水线相比基线流水线平均RMSE降低高达3.65倍（使用ORB特征跟踪器）。PX4FLOW在低于24像素/帧的运动速度下，以更低运行时实现与ORB相当的跟踪精度

Conclusion: 该设计成功实现了在超低功耗系统上运行的高精度VIO流水线，为微型无人机应用提供了可行的实时运动估计解决方案，在计算复杂度和跟踪精度之间取得了良好平衡

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [23] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种零样本的指代表达理解方法，通过将REC任务重新定义为基于框的视觉语言验证，使用通用检测器和VLM实现无需特定训练的竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 传统的REC方法需要任务特定的训练，本文旨在探索零样本工作流程是否能够达到竞争性性能，减少对特定预训练的依赖。

Method: 使用YOLO-World检测器生成候选框，然后通过通用VLM对每个区域独立进行True/False查询验证，避免跨框干扰，支持弃权和多重匹配。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上，该方法不仅超越了零样本GroundingDINO基线，还超过了经过REC训练的GroundingDINO及其变体的报告结果。

Conclusion: 工作流程设计而非任务特定的预训练是实现强零样本REC性能的关键因素，验证方法显著优于基于选择的提示方式。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [24] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 提出RPCP数据增强方法解决小麦病虫害分割中的极端像素不平衡问题，通过随机几何变换和投影滤波增强罕见类别的学习效果


<details>
  <summary>Details</summary>
Motivation: 小麦病虫害分割中虫害像素占比极小，导致严重的像素级不平衡，使得模型容易过拟合常见类别而忽略罕见类别，影响整体分割性能

Method: RPCP增强技术：从标注图像提取罕见虫害斑块，应用随机几何变换模拟变异，在避免与现有病变重叠的区域粘贴，并使用随机投影滤波器优化局部特征使其与背景自然融合

Result: 方法显著提升了虫害类别的分割性能，同时保持甚至略微提升了其他类别的准确率

Conclusion: 针对性数据增强能有效缓解极端像素不平衡问题，为农业图像分割提供简单而有效的解决方案

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [25] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 提出了一种结合不确定身份信息和跟踪的HMM框架，用于解决长期多目标跟踪中的身份切换问题，在牲畜跟踪和标准基准数据集上均表现出色


<details>
  <summary>Details</summary>
Motivation: 现有MOT方法在长期跟踪中由于身份切换问题性能下降，而现实应用（如畜牧业）中可以从喂食器等来源获得零星的身份识别信息

Method: 使用隐马尔可夫模型（HMM）框架，结合不确定的身份信息和跟踪数据，为动物提供真实身份识别

Result: 在10分钟猪只跟踪数据集上，相比领先的ByteTrack方法（即使使用重识别）F1分数有所提升；在MOT17和MOT20基准数据集上验证了方法的有效性；性能随身份信息提供频率增加而提升

Conclusion: 提出的HMM框架能够有效利用零星身份信息改善长期多目标跟踪性能，对身份识别的不确定性具有鲁棒性，适用于现实世界的长期跟踪应用

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [26] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 本调查论文系统回顾了事件相机与传统帧相机融合在视频恢复和3D重建领域的研究进展，重点关注深度学习在时空增强方面的应用，并整理了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 事件相机作为新兴的生物启发式传感器，具有低延迟、低功耗和高捕获率等优势，但其与传统帧相机的融合技术发展迅速，需要系统性的综述来总结最新进展和启发未来研究。

Method: 通过系统性地回顾深度学习在图像/视频增强和恢复方面的主要贡献，从时间维度（帧插值、运动去模糊）和空间维度（超分辨率、低光照增强、HDR增强、伪影减少）两个角度进行分析，并探讨事件驱动融合在3D重建领域的演进。

Result: 调查总结了事件相机与传统帧相机融合在各种视频恢复任务中的显著优势，特别是在挑战性条件下的视觉质量提升方面，并编制了全面的开放数据集列表以支持可重复研究和基准测试。

Conclusion: 通过整合最新进展和见解，本调查旨在激发进一步研究，特别是在结合深度学习技术的情况下，利用事件相机系统进行先进的视觉媒体恢复和增强。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [27] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack是首个基于Transformer的ANN-SNN混合跟踪器，通过ISTA适配器实现RGB和事件数据的有效融合，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ANN网络难以充分利用事件流的稀疏和异步特性，而混合ANN-SNN架构在RGB-Event感知中展现出潜力，但跨异构范式的特征融合仍然是一个挑战。

Method: 采用双分支模型：视觉Transformer提取RGB空间上下文，脉冲Transformer捕获事件流时空动态。设计了基于ISTA算法的适配器进行双向特征交互，并加入时序下采样注意力模块对齐特征。

Result: 在FE240hz、VisEvent、COESOT和FELT等RGB-Event跟踪基准测试中实现了最先进的性能，同时保持高能效。

Conclusion: ISTASTrack证明了混合ANN-SNN设计在鲁棒视觉跟踪中的有效性和实用性，为跨模态特征融合提供了新思路。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [28] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出基于多深度状态空间模型的太阳耀斑预测方法，引入FLARE损失函数解决类别不平衡问题，在11年太阳活动周期数据上表现优于基线方法


<details>
  <summary>Details</summary>
Motivation: 当前太阳耀斑预测性能不足，现有方法难以有效处理耀斑类别间的严重不平衡问题，需要更准确可靠的预测方法来减轻对关键基础设施的潜在影响

Method: 基于多深度状态空间模型的太阳耀斑预测模型，引入频率和局部边界感知可靠性损失（FLARE损失）来改善类别不平衡下的预测性能和可靠性

Result: 在覆盖完整11年太阳活动周期的多波长太阳图像数据集上，该方法在Gandin-Murphy-Gerrity评分和真实技能统计量等标准指标上均优于基线方法

Conclusion: 所提出的多深度状态空间模型结合FLARE损失函数能够有效解决太阳耀斑预测中的类别不平衡问题，提高了预测性能和可靠性

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [29] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI是一个RGB-热成像语义分割模型，通过统一的RGB-T编码器同时进行多模态特征提取和跨模态融合，实现了更紧凑的架构和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-T语义分割模型热特征提取有限、跨模态融合不理想以及编码器冗余导致的实时效率问题。

Method: 提出TUNI模型，使用多块堆叠的RGB-T编码器统一进行特征提取和融合；通过RGB和伪热数据的大规模预训练；精简热分支；引入RGB-T局部模块使用自适应余弦相似度选择性地强调跨模态的显著一致和不同局部特征。

Result: 在FMB、PST900和CART数据集上达到与最先进模型竞争的性能，参数更少、计算成本更低；在Jetson Orin NX上实现27 FPS的推理速度。

Conclusion: TUNI通过统一的编码器架构和有效的跨模态融合机制，在保持高性能的同时实现了实时部署能力，为自动驾驶平台在挑战性环境下的感知提供了有效解决方案。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [30] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种基于局部设计元素的少部件字体生成模型，只需部分形状作为输入即可生成完整字体，相比传统少样本字体生成方法更高效。


<details>
  <summary>Details</summary>
Motivation: 传统少样本字体生成需要完整字符形状作为输入，而本文旨在通过仅使用局部设计元素来提高字体创建效率，并探索局部设计细节如何影响整体字符结构。

Method: 设计了一个新颖的少部件字体生成模型，以部分形状（局部设计元素）作为输入，基于这些局部元素生成完整的字体字符。

Result: 该方法不仅提高了字体创建效率，还提供了关于局部设计细节如何影响单个字符整体结构的深入见解。

Conclusion: 提出的少部件字体生成模型为字体设计提供了更高效的解决方案，同时揭示了局部设计元素与整体字符结构之间的关系。

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出基于卷积神经网络的层次化多级注意力网络(MLANet)，从单张野外图像重建3D人脸模型，预测几何、纹理、姿态和光照参数


<details>
  <summary>Details</summary>
Motivation: 从2D野外图像恢复3D人脸模型具有广泛应用，但缺乏标注数据集和复杂真实环境是主要挑战

Method: 使用预训练层次化主干网络，在2D人脸特征提取不同阶段引入多级注意力机制，采用半监督训练策略结合3DMM参数和可微分渲染器

Result: 在AFLW2000-3D和MICC Florence数据集上进行广泛实验，包括对比和消融研究，定量和定性评估显示方法有效性

Conclusion: 提出的MLANet方法能够有效从单张野外图像重建详细的3D人脸模型，解决了缺乏标注数据和复杂环境的问题

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT是一个语言感知的视觉思维链框架，通过多阶段推理流程和奖励优化，显著提升多语言视觉问答性能，在多个基准测试中超越开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本思维链，对多语言多模态推理支持有限，限制了在实际应用中的部署。需要开发能够同时处理视觉和语言信息的推理框架。

Method: 提出多阶段推理流程（文本摘要+边界框、语言识别、空间对象级描述、逐步逻辑推理），采用自动化数据标注方法，结合监督微调和语言感知组相对策略优化的两阶段训练范式。

Result: 在MMMB、Multilingual MMBench和MTVQA等数据集上，比同规模开源基线提升约9.5%准确率，甚至超越规模大2倍的模型约2.6%，优于GPT-4o-0513和Gemini-2.5-flash等专有模型。

Conclusion: LaV-CoT通过语言感知的视觉思维链框架和多方面奖励优化，有效提升了多语言视觉问答的性能和可解释性，具有工业部署的实际价值。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出无需训练的框架，利用大语言模型解析模糊颜色描述，在文本嵌入空间指导颜色混合操作，提升文本到图像生成中的颜色准确性


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在处理复杂颜色术语时存在颜色对齐问题，现有方法无法系统性地解决模糊颜色描述，需要提升颜色渲染的精确度

Method: 使用大语言模型解析文本提示中的模糊颜色术语，基于CIELAB颜色空间的空间关系精炼文本嵌入，直接在嵌入空间指导颜色混合

Result: 实验结果表明该方法在不影响图像质量的情况下提高了颜色对齐准确性，弥合了文本语义与视觉生成之间的差距

Conclusion: 提出的训练免费框架有效解决了文本到图像生成中的颜色模糊问题，无需额外训练或参考图像即可实现精确的颜色渲染

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math是首个专门评估无人机遥感图像中多模态数学推理能力的基准数据集，包含3,773个高质量车辆相关问题，涵盖6个数学学科和20个主题。测试发现当前主流视觉语言模型在数学推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在无人机遥感领域的数学推理能力（如距离计算、轨迹估计、空间分析）尚未得到充分测试，需要专门的基准来评估和改进这方面的能力。

Method: 创建AVI-Math基准数据集，包含从不同高度和角度采集的无人机图像，构建涵盖几何、逻辑、代数等领域的数学问题。对14个主流视觉语言模型进行全面评估，并探索思维链提示和微调技术的应用。

Result: 尽管这些模型在以往的多模态基准测试中表现成功，但在AVI-Math的推理任务上表现挣扎，显示出当前视觉语言模型在数学推理能力方面存在显著局限性。

Conclusion: 研究不仅揭示了视觉语言模型在数学推理方面的局限，还为推进无人机应用中可信赖的视觉语言模型发展提供了宝贵见解，思维链提示和微调技术显示出解决这些推理挑战的潜力。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj是一个新颖的轨迹预测框架，直接在鸟瞰图空间利用实时传感器数据进行轨迹预测，无需依赖预建高清地图，通过可变形注意力和稀疏目标候选提议模块实现端到端预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预建高清地图或实时地图构建模块，但预建地图局限于特定区域且无法适应瞬时变化，而地图构建模块可能遗漏关键场景细节或引入错误，影响预测性能。

Method: 提出BEVTraj框架，在鸟瞰图空间直接操作，使用可变形注意力从密集BEV特征中高效提取相关上下文，并引入稀疏目标候选提议模块实现完全端到端预测，无需后处理步骤。

Result: 大量实验表明，BEVTraj在性能上与最先进的基于高清地图的模型相当，同时通过消除对预建地图的依赖提供了更大的灵活性。

Conclusion: BEVTraj框架成功克服了传统地图依赖方法的局限性，在保持高性能的同时提供了更好的适应性和灵活性，为自动驾驶轨迹预测提供了新的解决方案。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 提出了一种利用多视角信息改进遮挡场景下多人解析的新训练框架，通过弱监督和一致性损失提升模型在人体重叠时的分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在公开数据集上表现良好，但在处理人体重叠遮挡时效果显著下降。直觉是重叠的人体从不同视角看可能是分离的

Method: 提出基于多视角信息的训练框架，包括弱监督人类实例标注和多视角一致性损失。开发半自动标注策略从多视角RGB+D数据和3D人体骨架生成实例分割掩码

Result: 在遮挡场景下相比基线模型实现了最高4.20%的相对性能提升

Conclusion: 多视角信息能有效改善多人解析模型在遮挡情况下的表现，提出的训练框架和标注策略为解决人体重叠问题提供了有效方案

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是一个开源的韩英双语视觉语言模型，相比前代模型有显著提升，支持多图像理解、布局感知OCR，并在多模态对齐和安全性方面进行了优化。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时处理韩语和英语的双语视觉语言模型，支持复杂多图像输入和空间定位，同时保持核心语言能力并提高安全性。

Method: 采用四阶段课程训练和内存高效技术，通过偏好优化提升安全性，支持文档、图表和表格等多图像理解，并能预测文本内容及其空间位置。

Result: 在广泛基准评估中展现出强大的空间定位能力和双语竞争性表现，14B模型在OpenCompass VLM排行榜上位列同规模模型第8名。同时发布了1.7B轻量级版本用于设备端部署。

Conclusion: VARCO-VISION-2.0模型推动了双语视觉语言模型的发展及其实际应用，提供了14B完整版和1.7B轻量版两个变体。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出了一种轻量级的人脸图像质量评估方法，使用MobileNetV3-Small和ShuffleNetV2的集成网络，结合MSECorrLoss损失函数，在VQualA基准测试中达到SRCC 0.9829和PLCC 0.9894的高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像质量评估方法要么无法捕捉人脸特有的退化特征，要么计算复杂度高，限制了在实际应用中的部署。需要开发既准确又高效的FIQA方法。

Method: 集成两个紧凑卷积神经网络（MobileNetV3-Small和ShuffleNetV2），通过简单平均进行预测级融合，使用结合MSE和Pearson相关正则化的MSECorrLoss损失函数来增强与人类感知判断的一致性。

Result: 在VQualA FIQA基准测试中取得了SRCC 0.9829和PLCC 0.9894的优异性能，同时满足了计算效率的约束条件。

Conclusion: 该方法在准确性和计算成本之间取得了良好平衡，适合在实际环境中部署，为人脸识别和验证系统提供了高效的图像质量评估解决方案。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出RCOD框架，通过潜在域分组策略和退化感知采样，实现单步扩散模型中保真度与真实感的灵活权衡控制


<details>
  <summary>Details</summary>
Motivation: 传统单步扩散方法在真实图像超分辨率任务中缺乏灵活的保真度与真实感权衡控制机制，而多步方法可以通过调整采样步数来实现这种控制

Method: 提出RCOD框架，包括潜在域分组策略、退化感知采样策略和视觉提示注入模块，实现单步扩散模型的灵活控制

Result: 在定量指标和视觉质量上均优于现有单步扩散方法，同时保持计算效率

Conclusion: RCOD框架成功解决了单步扩散模型在保真度与真实感权衡方面的局限性，为真实图像超分辨率提供了高效且可控的解决方案

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL是一种无需源数据的域适应框架，通过梯度引导的伪标签细化和对比学习策略，在眼底图像分割中实现了优异的跨域性能


<details>
  <summary>Details</summary>
Motivation: 解决眼底图像分割模型在不同成像协议或条件下性能显著下降的问题，特别是在无法访问原始源数据的情况下实现鲁棒的域适应

Method: 结合梯度引导的伪标签细化模块和基于余弦相似度的对比学习策略。第一阶段通过梯度机制提取显著类别特征，准确量化不确定性并估计原型来细化噪声伪标签；第二阶段使用基于余弦相似度的对比损失显式增强视杯和视盘梯度特征之间的类间可分性

Result: 在具有挑战性的跨域眼底成像数据集上的大量实验表明，Grad-CL优于最先进的无监督和无需源数据的域适应方法，实现了卓越的分割精度和改进的边界描绘

Conclusion: Grad-CL框架有效解决了眼底图像分割中的域适应问题，无需访问源数据即可实现鲁棒的跨域性能，为眼科疾病早期诊断提供了可靠的技术支持

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge方法解决了VQ训练中的不稳定问题，通过压缩-处理-恢复管道实现100%码本使用率，显著提升图像重建和生成性能


<details>
  <summary>Details</summary>
Motivation: 解决向量量化(VQ)训练中的三个核心问题：直通估计偏差、一步滞后更新和稀疏码本梯度，这些导致重建性能不佳和码本使用率低

Method: 提出VQBridge投影器，基于映射函数方法，通过compress-process-recover管道优化码向量，结合学习退火实现稳定有效的码本训练

Result: 在多种码本配置下实现100%码本使用率，262k大码本也能达到完全使用，获得SOTA重建性能，与LlamaGen集成后图像生成性能显著提升

Conclusion: 高质量的分词器对强大的自回归图像生成至关重要，FVQ方法有效、可扩展且通用，在不同VQ变体中都保持有效性

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock是一种通过逐层冻结实现从像素预测到潜在预测渐进转换的自监督视觉表示学习方法，可加速MAE训练并避免表示崩溃


<details>
  <summary>Details</summary>
Motivation: 观察到视频掩码自编码(MAE)训练中ViT层按深度顺序收敛的现象，希望利用这一规律来加速训练并实现有效的潜在预测

Method: 通过显式进度表逐步冻结模型层，浅层先冻结深层后冻结，实现从像素预测到潜在预测的渐进过渡

Result: 在高达40亿参数的大模型上应用LayerLock，在4DS感知套件上的表现超越了非潜在掩码预测方法

Conclusion: LayerLock提供了一种简单有效的自监督学习策略，通过层冻结进度表实现了训练加速和避免表示崩溃的双重优势

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文系统比较了隐式和显式新视角合成方法在空间3D物体重建中的表现，重点评估了外观嵌入的作用，发现嵌入虽然能提升光度保真度但对几何精度提升有限，凸面溅射比高斯溅射具有更紧凑和清晰的优势


<details>
  <summary>Details</summary>
Motivation: 研究外观嵌入在空间基3D物体重建中对几何精度的影响，为空间机器人应用提供关键几何精度要求的评估

Method: 使用SPEED+数据集，比较K-Planes、高斯溅射和凸面溅射三种方法，分析外观嵌入对几何精度和表示效率的影响

Result: 外观嵌入主要减少显式方法所需的基元数量而非提升几何保真度；凸面溅射比高斯溅射获得更紧凑和无杂波的表示

Conclusion: 明确了外观嵌入在几何中心任务中的局限性，突出了空间场景中重建质量与表示效率之间的权衡，凸面溅射在安全关键应用中具有优势

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA是一个新的AI生成图像检测框架，通过减少域偏差和增强语义对齐来提升对未知生成模型的泛化能力，在GenImage基准上实现了5.8%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器主要依赖生成特定的伪影（如风格先验和压缩模式），导致对未见生成模型的泛化能力有限。

Method: 提出GAMMA训练框架，引入多样化操作策略（基于修复的操作和语义保持扰动），采用多任务监督（双分割头和分类头），并引入反向交叉注意力机制让分割头指导分类分支。

Result: 在GenImage基准上达到最先进的泛化性能，准确率提升5.8%，并对新发布的GPT-4o等生成模型保持强鲁棒性。

Conclusion: GAMMA通过减少域偏差和增强语义对齐，显著提升了AI生成图像检测器的泛化能力，为应对日益复杂的生成模型提供了有效解决方案。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 比较三种胎儿脑MRI超分辨率重建方法(NiftyMIC、SVRTK、NeSVoR)在140个病例中的表现，发现NeSVoR重建成功率最高(>90%)，虽然体积测量存在差异，但诊断分类性能不受SRR方法选择影响。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI通常采用快速多视角2D切片采集以减少运动伪影，但这些图像分辨率低、可能存在运动损坏且无法充分捕捉3D解剖结构。现有超分辨率重建方法的比较性能，特别是在病理情况下的表现，以及对下游体积分析和诊断任务的影响尚未充分探索。

Method: 应用三种先进的SRR方法(NiftyMIC、SVRTK、NeSVoR)处理140个胎儿脑MRI扫描(包括健康对照和脑室扩大病理病例)，使用BoUNTi算法分割重建后的高分辨率体积以提取9个主要脑结构体积，评估视觉质量、重建成功率、体积测量一致性和诊断分类性能。

Result: NeSVoR在健康组和病理组均表现出最高且最一致的重建成功率(>90%)。虽然不同SRR方法之间的体积估计存在显著差异，但脑室扩大的分类性能不受SRR方法选择的影响。

Conclusion: 研究结果突显了NeSVoR的鲁棒性，以及尽管SRR引起的体积存在变异性，但诊断性能仍具有弹性。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 提出Mask Consistency Regularization (MCR)训练策略，通过掩码扩张和重塑扰动来解决图像修复中物体移除任务的掩码幻觉和掩码形状偏差问题


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在物体移除任务中存在两个关键挑战：掩码幻觉（在掩码区域生成无关内容）和掩码形状偏差（生成与掩码形状相似而非与周围内容匹配的对象）

Method: 提出MCR训练策略，在训练过程中引入两种掩码扰动：扩张和重塑。扩张掩码帮助模型输出与周围内容对齐，重塑掩码鼓励模型打破掩码形状偏差

Result: 实验证明MCR显著减少了幻觉和掩码形状偏差，在物体移除任务中实现了更好的性能

Conclusion: MCR通过掩码一致性正则化有效解决了物体移除中的关键问题，能够产生更鲁棒和上下文一致的修复结果

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror是一个系统性的文本到图像生成伪影评估框架，包含分类体系、人工标注数据集、视觉语言模型评估器和自动化基准测试，揭示了当前顶级T2I模型仍存在严重伪影问题


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成虽然取得了显著进展，但在物理伪影（如解剖和结构缺陷）方面仍存在严重问题，缺乏系统性和细粒度的评估框架

Method: 1) 建立详细的生成图像伪影分类体系；2) 人工标注MagicData340K大规模数据集；3) 训练MagicAssessor视觉语言模型进行评估；4) 设计数据采样策略和多级奖励系统的GRPO方法；5) 构建MagicBench自动化基准测试

Result: 评估发现即使像GPT-image-1这样的顶级模型也持续存在显著伪影问题，表明伪影减少是未来T2I发展的关键前沿

Conclusion: MagicMirror提供了一个全面的伪影评估框架，揭示了当前T2I模型的局限性，伪影减少是未来研究的重要方向

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip是一个新的手语翻译框架，通过融合手势和唇部运动特征，并采用分层对比学习来提高翻译准确性，在PHOENIX14T数据集上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译方法主要关注手势信号，往往忽略了唇部运动等非手动线索，而唇部运动在手语中传递重要语言信息，对区分视觉相似的手势至关重要。

Method: 提出SignClip框架，融合空间手势和唇部运动特征，引入分层对比学习框架，包含多级对齐目标，确保手语-唇部和视觉-文本模态间的语义一致性。

Result: 在PHOENIX14T和How2Sign两个基准数据集上进行了广泛实验。在PHOENIX14T的无注释设置下，BLEU-4从24.32提升到24.71，ROUGE从46.57提升到48.38，超越了之前的SOTA模型SpaMo。

Conclusion: SignClip通过有效融合手动和非手动线索，显著提升了手语翻译的准确性，证明了唇部运动特征在手语翻译中的重要性。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 该论文分析了开源和闭源大型视觉语言模型在文本篡改检测方面的性能，发现开源模型正在接近但仍有差距，同时评估了图像篡改检测专用模型在文本篡改任务中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大型视觉语言模型在图像篡改检测方面的有效性，但文本篡改检测领域存在研究空白，需要填补这一知识差距。

Method: 通过在不同文本篡改数据集上测试闭源和开源VLMs，包括对真实场景文本和模拟现实滥用的伪造ID卡的篡改检测。

Result: 开源模型性能正在提升但仍落后于GPT-4o等闭源模型，图像篡改检测专用模型在文本篡改任务中存在泛化问题。

Conclusion: 文本篡改检测是VLMs的一个重要应用方向，开源模型需要进一步改进，专用模型需要更好的泛化能力来应对现实世界的挑战性应用场景。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD是一个新颖的多模态协作学习框架，通过点云、RGB图像和文本语义的跨模态协作，实现了卓越的零样本3D异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本3D异常检测方法主要关注点云数据，忽略了RGB图像和文本先验等互补模态提供的丰富语义线索。为了解决这个问题，作者提出了一个多模态协作学习框架。

Method: 提出了多模态提示学习机制（MPLM），通过对象无关的解耦文本提示和多模态对比损失增强模态内表示能力和模态间协作学习；同时提出了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导分支来充分利用互补表示。

Result: 大量实验表明，MCL-AD框架在零样本3D异常检测中实现了最先进的性能。

Conclusion: 该研究证明了多模态协作学习在零样本3D异常检测中的有效性，通过整合点云、RGB图像和文本语义信息，显著提升了检测性能。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出一种基于Lipschitz约束的随机深度方法，通过深度相关的DropPath概率来提升ViT模型的对抗鲁棒性，同时保持干净准确率和降低计算量


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和Vision Transformers在计算机视觉中表现优异但对抗扰动脆弱，现有防御方法计算成本高或缺乏形式化保证

Method: Lipschitz引导的随机深度(DropPath)方法，drop概率随深度增加以控制网络的有效Lipschitz常数，正则化深层网络

Result: 在CIFAR-10和ViT-Tiny上的实验表明，该方法保持接近基线的干净准确率，提升了对FGSM、PGD-20和AutoAttack的鲁棒性，并显著减少了FLOPs

Conclusion: 深度相关的DropPath调度能有效平衡模型鲁棒性、准确性和计算效率，为对抗防御提供了新思路

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出基于能量图的概率框架，用于复杂城市环境中街道家具的精确定位，通过随机生死优化算法整合地理空间信息，提高定位精度


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中街道家具的精确定位问题，这对公共基础设施的有效监控和维护至关重要

Method: 基于能量图的概率框架，编码物体位置的空间似然性；采用随机生死优化算法推断最可能的资产配置；整合GIS图层、道路地图等外部地理空间信息

Result: 使用都柏林市中心街道照明基础设施的地理定位数据集进行真实模拟评估，证明了该方法在可扩展和准确的城市资产映射方面的潜力

Conclusion: 提出的概率框架和优化算法能够有效提高城市环境中街道家具的定位精度，为城市资产管理提供了可行的技术方案

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa通过空间聚类减少扩散变换器中的token数量，实现4.96倍加速，同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法只利用时间维度相似性，忽略了空间维度的相似性，导致计算效率仍有提升空间

Method: 在每时间步对token进行空间聚类，每个聚类只计算一个代表性token，然后将信息传播给其他token，减少90%以上token数量

Result: 在DiT、FLUX和HunyuanVideo上验证有效性，FLUX实现4.96倍加速，ImageReward达到99.49%，比原模型提升0.51%

Conclusion: ClusCa提供了一种正交且互补的空间维度加速方法，无需训练即可直接应用于任何扩散变换器，显著提升计算效率

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT分割框架，通过系统替换浮点运算、提出新激活函数和优化解码器，在保持精度的同时显著减少模型大小和加速推理


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但高内存占用和计算成本限制了其在资源受限设备上的部署。量化虽然能提高效率，但ViT分割模型在低精度下表现脆弱

Method: 基于Segmenter架构，系统替换浮点运算为整数运算；提出λ-ShiftGELU激活函数处理长尾分布；移除L2归一化层；将双线性插值替换为最近邻上采样

Result: 在保持与FP32基线合理精度差距（平均5.1%）的同时，模型大小减少3.8倍，推理速度提升1.2倍。即使在单图像校准的一次性PTQ中也保持竞争力

Conclusion: I-Segmenter展示了完全整数化ViT分割的可行性，为实际部署提供了实用解决方案，特别适合资源受限环境

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD是一种基于伽马扩散模型的OCT图像去噪方法，通过噪声减少保真项和加速推理框架，在保持解剖结构的同时有效去除散斑噪声


<details>
  <summary>Details</summary>
Motivation: OCT图像受散斑噪声影响严重，现有去噪方法难以在噪声去除和结构保持之间取得平衡，需要更准确的噪声统计模型和更好的去噪指导机制

Method: 提出GARD方法，使用Denoising Diffusion Gamma Model替代传统高斯假设，引入Noise-Reduced Fidelity Term利用预处理图像指导去噪，并采用DDIM框架加速推理

Result: 在配对噪声和低噪声OCT图像数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统方法和先进深度学习模型，定性结果显示更好的边缘锐度和解剖细节保持

Conclusion: GARD通过伽马扩散模型和噪声减少保真项，成功解决了OCT图像去噪中噪声去除与结构保持的平衡问题，为医学图像处理提供了有效解决方案

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: GLAM模型通过几何引导的全局和局部对齐方法，在乳腺X线摄影多视图分析中实现了更好的视觉语言预训练，显著超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的乳腺X线摄影视觉语言模型大多从自然图像迁移而来，忽略了医学图像特有的多视图关系和几何特征，无法像放射科医生那样同时分析多个视图来处理同侧对应关系。

Method: 提出GLAM模型，利用乳腺X线摄影多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉、视觉-语言的对比学习，学习局部跨视图对齐和细粒度局部特征。

Result: 在EMBED（最大的开放乳腺X线摄影数据集之一）上预训练后，该模型在多个数据集和不同设置下均优于基线方法。

Conclusion: 通过几何引导的多视图对齐方法，GLAM模型能够更好地捕捉乳腺X线摄影中的关键几何上下文信息，提高了乳腺X线摄影解读的准确性和效率。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 这篇综述论文系统回顾了现代通用视觉语言模型中的视觉基础能力，包括其核心组件、实际应用、评估方法以及与其他技术的关系，并分析了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉基础能力使模型能够根据文本描述精确定位视觉输入中的特定区域，这对于多模态理解和应用至关重要。论文旨在全面梳理该领域的研究现状和发展趋势。

Method: 采用文献综述方法，首先阐述视觉基础的重要性，然后分析现代基础模型的核心组件，考察实际应用和评估指标，并探讨视觉基础与多模态思维链、推理之间的关系。

Result: 系统性地总结了视觉基础在视觉语言模型中的关键作用，梳理了当前主流方法和技术框架，识别了该领域的主要应用场景和评估标准。

Conclusion: 视觉基础是多模态AI的核心能力，虽然面临诸多挑战，但在推理、控制和细粒度理解等方面具有广阔的应用前景，需要进一步研究改进。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 提出了一种针对文本图像编辑方法的视觉组件攻击——Attention Attack，通过使用源图像的自动生成描述作为编辑提示的代理，破坏文本提示与视觉表示之间的交叉注意力，无需了解编辑方法或编辑提示即可破坏图像内容与文本描述的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的图像编辑方法容易受到对抗攻击，但现有攻击主要针对文本组件。本文旨在开发一种针对视觉组件的攻击方法，破坏文本提示与图像视觉表示之间的对齐关系。

Method: 提出Attention Attack攻击方法，利用源图像的自动生成描述作为编辑提示的代理，干扰交叉注意力机制。同时提出了Caption Similarity和语义IoU两种新的评估策略来衡量攻击效果。

Result: 在TEDBench++基准测试上的实验表明，该攻击方法显著降低了编辑性能，同时保持攻击的不可感知性。

Conclusion: Attention Attack是一种有效的针对文本图像编辑系统视觉组件的对抗攻击方法，能够在不了解编辑方法细节的情况下成功破坏编辑效果，提出的新评估指标能更好地衡量攻击的成功程度。

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 该研究利用知识蒸馏技术来降低神经网络图像压缩方法的计算资源需求，使小型网络能够通过向大型复杂模型学习而获得更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 虽然基于神经网络的图像压缩方法在性能上优于传统编解码器，但它们需要大量计算资源，不适合在资源受限平台上实时使用，这阻碍了其在实际应用中的部署。

Method: 采用知识蒸馏训练范式，让较小的神经网络部分地基于更大、更复杂模型的输出进行训练，从而在保持压缩性能的同时减少资源消耗。

Result: 研究表明知识蒸馏可有效应用于图像压缩任务：适用于不同架构大小、可实现不同图像质量/比特率权衡、并能节省处理和能源资源。

Conclusion: 知识蒸馏为神经网络图像压缩提供了有效的资源优化方案，未来可探索不同教师模型和替代损失函数的影响，并扩展到基于transformer的模型。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的固有图像分解方法，仅使用可见光和热成像图像对，通过热成像检测吸收的光能来推断反射率和阴影的序数关系，实现自监督学习。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏真实世界场景的大规模地面真实数据，传统固有图像分解方法依赖合成数据或稀疏标注，限制了在室内外场景的应用效果。

Method: 利用可见光和热成像图像对，基于光能被不透明表面吸收后转化为热能的热成像检测原理，建立可见光与热成像强度之间的序数关系，进而推导出阴影和反射率的序数关系，用于密集自监督神经网络优化。

Result: 在自然光和人工光照条件下对已知反射率和阴影进行定量评估，并在多样化户外场景进行定性实验，结果显示该方法优于近期基于学习的模型。

Conclusion: 该方法为获取真实世界序数监督提供了一条可扩展的路径，这是之前通过人工标注无法实现的，展示了在固有图像分解领域的显著优势。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了压缩视频质量增强(CVQE)的系统分类法和统一基准测试框架，解决了现有综述在分类体系、架构比较和基准测试方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有压缩视频质量增强的综述存在系统性分类不足、架构范式对比分析不够、基准测试实践不完善等问题，需要建立更全面的评估体系

Method: 提出新颖的分类法（按架构范式、编码标准、压缩域特征利用分类），设计统一基准测试框架（集成现代压缩协议和标准测试序列），进行系统性能-复杂度权衡分析

Result: 建立了CVQE方法的系统性分类体系，提供了公平的多标准评估框架，分析了现有方法的性能-复杂度权衡关系

Conclusion: 该综述为CVQE研究和部署提供了一致的评估基础和明智的模型选择指导，指明了未来研究的有前景方向

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter是一个新颖的多模态语义分割框架，通过适配器网络将融合的多模态特征注入到Segment Anything Model中，在保持RGB特征强泛化能力的同时选择性地利用辅助模态信息，在多个挑战性基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割方法在恶劣光照、遮挡和恶劣天气等挑战性条件下表现脆弱，需要整合辅助传感器数据（如LiDAR、红外）来提供互补信息以增强鲁棒性。

Method: 提出MM SAM-adapter框架，使用适配器网络将融合的多模态特征注入到SAM的RGB特征中，既能保持RGB特征的强泛化能力，又能选择性地在辅助模态提供额外线索时加以利用。

Result: 在DeLiVER、FMB和MUSES三个挑战性基准测试中实现了state-of-the-art性能。在RGB-easy和RGB-hard子集上的结果表明，该框架在有利和不利条件下均优于竞争方法。

Conclusion: 多模态适应对于鲁棒场景理解非常有效，MM SAM-adapter实现了多模态信息的平衡和高效利用，为语义分割在挑战性条件下的应用提供了有力解决方案。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen是一种基于潜在扩散模型的第二代图像生成方法，通过用一步生成器替换VAE解码器，可以从固定大小的潜在表示生成任意分辨率的图像，显著降低计算复杂度，将4K图像生成时间从100多秒减少到10秒以内。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成高分辨率图像时计算需求呈二次方增长，导致4K图像生成延迟超过100秒，需要一种更高效的方法来实现任意分辨率的图像生成。

Method: 将扩散模型生成的固定潜在表示作为内容表示，提出使用一步生成器来解码任意分辨率的图像，用新的生成器替换VAE解码器，无需重新训练扩散模型。

Result: InfGen能够将许多模型提升到任意高分辨率时代，同时将4K图像生成时间缩短到10秒以内，显著降低了计算复杂度。

Conclusion: InfGen提供了一种高效的任意分辨率图像生成解决方案，简化了生成过程，可应用于使用相同潜在空间的任何模型，为跨设备提供一致的视觉体验。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 该研究将三种先进的时间自监督学习方法应用于3D脑部MRI分析，通过处理变长输入和增强空间特征学习，在阿尔茨海默病预测任务中超越了监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病预测中深度学习模型面临的标注数据缺乏、跨数据集泛化能力差以及对不同扫描数量和间隔时间适应性不足的问题。

Method: 采用时间自监督学习（SSL）方法，包括时间顺序预测和对比学习，处理变长输入并学习鲁棒的空间特征，使用四个公开数据集共3,161名患者进行预训练。

Result: 在七个下游任务中的六个任务上，自监督学习模型性能优于监督学习，展示了跨任务和不同输入图像数量及时间间隔的适应性和泛化能力。

Conclusion: 时间自监督学习方法在阿尔茨海默病预测中表现出色，具有强大的临床适用性和鲁棒性能，代码和模型已公开。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [65] [MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos](https://arxiv.org/abs/2509.09769)
*Rutav Shah,Shuijing Liu,Qi Wang,Zhenyu Jiang,Sateesh Kumar,Mingyo Seo,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: MimicDroid使用人类游戏视频作为训练数据，通过轨迹配对学习和随机补丁掩码技术，使仿人机器人能够从少量视频示例中快速学习新操作任务，在仿真和真实世界中都取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决仿人机器人从少量视频示例学习新操作任务的问题，避免依赖劳动密集型的遥操作数据，利用人类游戏视频作为可扩展和多样化的训练数据源。

Method: 1) 使用人类游戏视频作为唯一训练数据；2) 提取具有相似操作行为的轨迹对；3) 训练策略以预测一个轨迹的动作条件于另一个轨迹；4) 通过运动学相似性将人类手腕姿态重定向到仿人机器人；5) 应用随机补丁掩码减少对人类特定线索的过拟合并提高视觉差异的鲁棒性。

Result: MimicDroid在开源仿真基准测试中优于最先进方法，在真实世界中实现了近两倍的成功率提升。

Conclusion: 人类游戏视频是训练仿人机器人上下文学习能力的有效数据源，MimicDroid方法能够显著提高机器人在新物体和环境中的适应能力。

Abstract: We aim to enable humanoid robots to efficiently solve new manipulation tasks
from a few video examples. In-context learning (ICL) is a promising framework
for achieving this goal due to its test-time data efficiency and rapid
adaptability. However, current ICL methods rely on labor-intensive teleoperated
data for training, which restricts scalability. We propose using human play
videos -- continuous, unlabeled videos of people interacting freely with their
environment -- as a scalable and diverse training data source. We introduce
MimicDroid, which enables humanoids to perform ICL using human play videos as
the only training data. MimicDroid extracts trajectory pairs with similar
manipulation behaviors and trains the policy to predict the actions of one
trajectory conditioned on the other. Through this process, the model acquired
ICL capabilities for adapting to novel objects and environments at test time.
To bridge the embodiment gap, MimicDroid first retargets human wrist poses
estimated from RGB videos to the humanoid, leveraging kinematic similarity. It
also applies random patch masking during training to reduce overfitting to
human-specific cues and improve robustness to visual differences. To evaluate
few-shot learning for humanoids, we introduce an open-source simulation
benchmark with increasing levels of generalization difficulty. MimicDroid
outperformed state-of-the-art methods and achieved nearly twofold higher
success rates in the real world. Additional materials can be found on:
ut-austin-rpl.github.io/MimicDroid

</details>


### [66] [MIMo grows! Simulating body and sensory development in a multimodal infant model](https://arxiv.org/abs/2509.09805)
*Francisco M. López,Miles Lenz,Marco G. Fedozzi,Arthur Aubret,Jochen Triesch*

Main category: cs.RO

TL;DR: MIMo v2是一个多模态婴儿模型，通过模拟从出生到24个月的身体生长、运动能力发展、视觉发育和神经信号延迟，为发育机器人研究提供更真实的婴儿发育模拟平台。


<details>
  <summary>Details</summary>
Motivation: 现有的发育机器人和仿真平台通常只针对特定年龄设计，无法捕捉婴儿快速变化的感知运动能力和身体约束，限制了婴儿发育研究的真实性。

Method: 开发了MIMo v2模型，包含：1）从出生到24个月的身体生长模型和运动能力增强；2）具有发展性视觉敏锐度的中央凹视觉系统；3）模拟神经信号传输延迟的感知运动延迟模型；4）逆向运动学模块和随机环境生成器；5）更新第三方仿真和学习库的兼容性。

Result: 新版本的MIMo模型能够更真实地模拟婴儿感知运动发展的各个方面，为发育研究提供了更准确的仿真平台。代码已在官方仓库开源。

Conclusion: MIMo v2通过综合模拟婴儿身体生长、感知能力和运动控制的发展过程，显著提升了婴儿发育建模的真实性，为发育机器人和认知发展研究提供了重要工具。

Abstract: Infancy is characterized by rapid body growth and an explosive change of
sensory and motor abilities. However, developmental robots and simulation
platforms are typically designed in the image of a specific age, which limits
their ability to capture the changing abilities and constraints of developing
infants. To address this issue, we present MIMo v2, a new version of the
multimodal infant model. It includes a growing body with increasing actuation
strength covering the age range from birth to 24 months. It also features
foveated vision with developing visual acuity as well as sensorimotor delays
modeling finite signal transmission speeds to and from an infant's brain.
Further enhancements of this MIMo version include an inverse kinematics module,
a random environment generator and updated compatiblity with third-party
simulation and learning libraries. Overall, this new MIMo version permits
increased realism when modeling various aspects of sensorimotor development.
The code is available on the official repository
(https://github.com/trieschlab/MIMo).

</details>


### [67] [Using the Pepper Robot to Support Sign Language Communication](https://arxiv.org/abs/2509.09889)
*Giulia Botta,Marco Botta,Cristina Gena,Alessandro Mazzei,Massimo Donini,Alberto Lillo*

Main category: cs.RO

TL;DR: 研究探索商用社交机器人Pepper是否能产生可理解的意大利手语(LIS)手势和短句，通过用户研究发现大部分孤立手势能被正确识别，但完整句子识别率较低。


<details>
  <summary>Details</summary>
Motivation: 社交机器人在公共和辅助环境中应用日益增多，但对聋人用户的可访问性研究不足。意大利手语(LIS)作为成熟自然语言，具有复杂的手势和非手势成分，让机器人使用LIS交流可以促进更包容的人机交互。

Method: 与聋人学生和手语专家合作，通过手动动画技术和MATLAB逆向运动学求解器，在Pepper机器人上实现了52个LIS手势。对12名熟练LIS使用者(包括聋人和听力正常者)进行探索性用户研究，包含15个视频选择题和2个开放性问题。

Result: 大多数孤立手势能被正确识别，但由于机器人关节活动受限和时间约束，完整句子的识别率显著较低。

Conclusion: 即使是商用社交机器人如Pepper也能执行部分可理解的LIS手势，为更包容的交互设计提供机会。未来应关注多模态增强(如屏幕支持或表情丰富的虚拟形象)并让聋人用户参与设计以改进机器人表现力和可用性。

Abstract: Social robots are increasingly experimented in public and assistive settings,
but their accessibility for Deaf users remains quite underexplored. Italian
Sign Language (LIS) is a fully-fledged natural language that relies on complex
manual and non-manual components. Enabling robots to communicate using LIS
could foster more inclusive human robot interaction, especially in social
environments such as hospitals, airports, or educational settings. This study
investigates whether a commercial social robot, Pepper, can produce
intelligible LIS signs and short signed LIS sentences. With the help of a Deaf
student and his interpreter, an expert in LIS, we co-designed and implemented
52 LIS signs on Pepper using either manual animation techniques or a MATLAB
based inverse kinematics solver. We conducted a exploratory user study
involving 12 participants proficient in LIS, both Deaf and hearing.
Participants completed a questionnaire featuring 15 single-choice video-based
sign recognition tasks and 2 open-ended questions on short signed sentences.
Results shows that the majority of isolated signs were recognized correctly,
although full sentence recognition was significantly lower due to Pepper's
limited articulation and temporal constraints. Our findings demonstrate that
even commercially available social robots like Pepper can perform a subset of
LIS signs intelligibly, offering some opportunities for a more inclusive
interaction design. Future developments should address multi-modal enhancements
(e.g., screen-based support or expressive avatars) and involve Deaf users in
participatory design to refine robot expressivity and usability.

</details>


### [68] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: SART框架通过单次人类演示和自主增强，实现高效安全的模仿学习，减少人工干预并提高成功率


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要大量演示或随机探索，存在安全风险和高人工成本，特别是在空间受限任务中容易发生碰撞

Method: 两阶段框架：1）单次人类演示并标注精度边界球体；2）机器人在边界内自主生成多样化无碰撞轨迹并与原始演示重新连接

Result: 在仿真和真实操控任务中，SART相比仅使用人类演示的策略实现了显著更高的成功率

Conclusion: SART通过最小化人工干预同时确保安全，有效提高了数据收集效率，为模仿学习提供了新的解决方案

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [69] [Detection of Anomalous Behavior in Robot Systems Based on Machine Learning](https://arxiv.org/abs/2509.09953)
*Mahfuzul I. Nissan,Sharmin Aktar*

Main category: cs.RO

TL;DR: 本研究提出基于机器学习的系统日志异常检测方法，通过比较逻辑回归、支持向量机和自编码器在不同机器人场景中的表现，发现最优模型选择具有情境依赖性，自编码器在复杂异常检测方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 确保机器人系统的安全可靠运行至关重要，尽管有严格的设计和工程实践，系统仍可能出现故障导致安全风险，因此需要开发有效的异常检测方法来增强机器人系统的安全性和可靠性。

Method: 使用CoppeliaSim收集两个不同场景（四旋翼无人机和Pioneer机器人）的系统日志，并比较评估多种机器学习模型，包括逻辑回归、支持向量机和自编码器。

Result: 在四旋翼无人机场景中逻辑回归表现最佳，而在Pioneer机器人场景中自编码器最为有效，表明最优模型选择取决于具体情境，这可能是由于不同机器人平台异常复杂度的差异所致。

Conclusion: 研究强调了比较方法的价值，并证明了自编码器在检测机器人系统中复杂异常方面的特殊优势，模型选择应根据具体应用场景进行优化。

Abstract: Ensuring the safe and reliable operation of robotic systems is paramount to
prevent potential disasters and safeguard human well-being. Despite rigorous
design and engineering practices, these systems can still experience
malfunctions, leading to safety risks. In this study, we present a machine
learning-based approach for detecting anomalies in system logs to enhance the
safety and reliability of robotic systems. We collected logs from two distinct
scenarios using CoppeliaSim and comparatively evaluated several machine
learning models, including Logistic Regression (LR), Support Vector Machine
(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context
(Context 1) and a Pioneer robot context (Context 2). Results showed that while
LR demonstrated superior performance in Context 1, the Autoencoder model proved
to be the most effective in Context 2. This highlights that the optimal model
choice is context-dependent, likely due to the varying complexity of anomalies
across different robotic platforms. This research underscores the value of a
comparative approach and demonstrates the particular strengths of autoencoders
for detecting complex anomalies in robotic systems.

</details>


### [70] [Gaussian path model library for intuitive robot motion programming by demonstration](https://arxiv.org/abs/2509.10007)
*Samuli Soutukorva,Markku Suomalainen,Martin Kollingbaum,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 提出了一种从教学数据生成高斯路径模型的系统，用于路径形状表示和分类，支持通过演示进行直观的机器人运动编程和路径模型修改。


<details>
  <summary>Details</summary>
Motivation: 为了通过人类演示实现直观的机器人运动编程，需要一种能够表示和分类路径形状的方法，并支持通过演示修改现有路径模型。

Method: 使用高斯路径模型从教学数据中生成路径形状表示，建立多种形状的高斯路径模型库，通过几何分析实现基于演示的路径模型修改。

Result: 开发了一个完整的系统，能够生成高斯路径模型、分类人类演示的路径，并通过演示修改现有模型，为机器人运动编程提供直观的接口。

Conclusion: 高斯路径模型系统有效地支持了基于人类演示的机器人运动编程，提供了路径形状表示、分类和修改的完整解决方案，具有实际应用价值。

Abstract: This paper presents a system for generating Gaussian path models from
teaching data representing the path shape. In addition, methods for using these
path models to classify human demonstrations of paths are introduced. By
generating a library of multiple Gaussian path models of various shapes, human
demonstrations can be used for intuitive robot motion programming. A method for
modifying existing Gaussian path models by demonstration through geometric
analysis is also presented.

</details>


### [71] [Towards simulation-based optimization of compliant fingers for high-speed connector assembly](https://arxiv.org/abs/2509.10012)
*Richard Matthias Hartisch,Alexander Rother,Jörg Krüger,Kevin Haninger*

Main category: cs.RO

TL;DR: 提出基于仿真的柔顺机构设计工具，用于优化柔顺手指的设计参数，提高插入任务中的容错范围和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 柔顺性是动态接触丰富操作的关键设计参数，传统方法要么需要大量硬件迭代时间，要么使用简化模型无法处理复杂操作任务目标

Method: 使用改进的动态仿真（特别是接触和摩擦建模）作为设计工具，基于任务级目标（如成功率）优化柔顺手指的设计参数

Result: 优化参数可将容错范围提高2.29倍，能够补偿高达8.6mm的工作件变化，但趋势具有任务特异性

Conclusion: 柔顺机构设计需要针对特定应用考虑几何和动力学特性，仿真工具能够有效提高设计效率和性能

Abstract: Mechanical compliance is a key design parameter for dynamic contact-rich
manipulation, affecting task success and safety robustness over contact
geometry variation. Design of soft robotic structures, such as compliant
fingers, requires choosing design parameters which affect geometry and
stiffness, and therefore manipulation performance and robustness. Today, these
parameters are chosen through either hardware iteration, which takes
significant development time, or simplified models (e.g. planar), which can't
address complex manipulation task objectives. Improvements in dynamic
simulation, especially with contact and friction modeling, present a potential
design tool for mechanical compliance. We propose a simulation-based design
tool for compliant mechanisms which allows design with respect to task-level
objectives, such as success rate. This is applied to optimize design parameters
of a structured compliant finger to reduce failure cases inside a tolerance
window in insertion tasks. The improvement in robustness is then validated on a
real robot using tasks from the benchmark NIST task board. The finger stiffness
affects the tolerance window: optimized parameters can increase tolerable
ranges by a factor of 2.29, with workpiece variation up to 8.6 mm being
compensated. However, the trends remain task-specific. In some tasks, the
highest stiffness yields the widest tolerable range, whereas in others the
opposite is observed, motivating need for design tools which can consider
application-specific geometry and dynamics.

</details>


### [72] [Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping](https://arxiv.org/abs/2509.10032)
*Marawan Khalil,Fabian Arzberger,Andreas Nüchter*

Main category: cs.RO

TL;DR: 本文研究了两种球形机器人（无驱动和驱动型）在动态运动下的LiDAR-惯性里程计性能，发现现有LIO算法在球形运动的高动态环境下性能下降，导致地图不一致和漂移问题。


<details>
  <summary>Details</summary>
Motivation: 球形机器人在危险或受限环境中具有独特优势，但其球形运动带来的高动态性可能影响LiDAR-惯性里程计的建图精度，需要系统评估现有算法的性能表现。

Method: 开发了两种球形建图系统（轻量无驱动型和内部摆锤驱动型），配备Livox Mid-360固态LiDAR传感器，在资源受限硬件上运行LIO算法，并通过与地面真实地图比较来评估建图精度。

Result: 结果表明，由于球形运动引入的高动态运动，最先进的LIO算法性能恶化，导致全局不一致的地图和有时无法恢复的漂移问题。

Conclusion: 球形机器人的独特运动特性对现有LIO算法提出了挑战，需要开发更适合高动态环境的建图算法来保证建图质量。

Abstract: Spherical robots offer unique advantages for mapping applications in
hazardous or confined environments, thanks to their protective shells and
omnidirectional mobility. This work presents two complementary spherical
mapping systems: a lightweight, non-actuated design and an actuated variant
featuring internal pendulum-driven locomotion. Both systems are equipped with a
Livox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)
algorithms on resource-constrained hardware. We assess the mapping accuracy of
these systems by comparing the resulting 3D point-clouds from the LIO
algorithms to a ground truth map. The results indicate that the performance of
state-of-the-art LIO algorithms deteriorates due to the high dynamic movement
introduced by the spherical locomotion, leading to globally inconsistent maps
and sometimes unrecoverable drift.

</details>


### [73] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: TwinTac系统结合物理触觉传感器及其数字孪生模型，解决了触觉感知在机器人技能学习中的仿真缺失问题，通过真实到仿真的方法实现跨域数据映射，提升物体分类任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器人技能学习通常依赖仿真生成交互数据，但触觉传感器的仿真模型缺失限制了触觉感知在策略学习中的应用，需要弥合这一跨域学习差距。

Method: 设计高灵敏度物理触觉传感器，采用真实到仿真方法收集同步跨域数据（有限元结果和物理传感器输出），训练神经网络将仿真数据映射到真实传感器响应。

Result: 物理传感器灵敏度得到表征，数字孪生模型能一致复现物理传感器输出；仿真数据能有效增强真实数据，提升物体分类任务准确率。

Conclusion: TwinTac系统成功弥合了跨域学习任务中的差距，证明了数字孪生触觉传感器在机器人技能学习中的潜在价值。

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [74] [Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation](https://arxiv.org/abs/2509.10065)
*Hauzi Cao,Jiahao Shen,Zhengzhen Li,Qinquan Ren,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于预设轨迹和二次规划的空中机械臂运动学跟踪控制框架，能够在预设时间内实现末端执行器的精确定位，同时考虑物理约束。


<details>
  <summary>Details</summary>
Motivation: 现有运动学跟踪控制方法（如比例微分反馈或跟踪误差反馈策略）无法在指定时间约束内实现跟踪目标，需要开发新的控制框架来解决这一限制。

Method: 采用包含两个关键组件的控制框架：1）基于用户定义预设轨迹的末端执行器跟踪控制；2）基于二次规划的参考分配方法，同时考虑空中机械臂的物理约束。

Result: 通过三个实验验证了所提方法的有效性，实验结果表明该算法能够保证在预设时间内达到目标位置，且跟踪误差保持在反映任务要求的性能包络内。

Conclusion: 提出的控制方法相比现有技术具有显著优势，能够确保末端执行器在预设时间内到达期望位置，同时通过二次规划避免违反物理限制的解决方案，为空中机械臂的精确控制提供了有效解决方案。

Abstract: This paper studies the kinematic tracking control problem for aerial
manipulators. Existing kinematic tracking control methods, which typically
employ proportional-derivative feedback or tracking-error-based feedback
strategies, may fail to achieve tracking objectives within specified time
constraints. To address this limitation, we propose a novel control framework
comprising two key components: end-effector tracking control based on a
user-defined preset trajectory and quadratic programming-based reference
allocation. Compared with state-of-the-art approaches, the proposed method has
several attractive features. First, it ensures that the end-effector reaches
the desired position within a preset time while keeping the tracking error
within a performance envelope that reflects task requirements. Second,
quadratic programming is employed to allocate the references of the quadcopter
base and the Delta arm, while considering the physical constraints of the
aerial manipulator, thus preventing solutions that may violate physical
limitations. The proposed approach is validated through three experiments.
Experimental results demonstrate the effectiveness of the proposed algorithm
and its capability to guarantee that the target position is reached within the
preset time.

</details>


### [75] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于Transformer的条件去噪扩散模型，用于预测辅助场景中的人体交互运动，提升机器人辅助策略


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但机器人需要准确的人类运动预测来提供安全响应式辅助，这在物理交互场景中具有挑战性

Method: 收集人类-人类交互辅助任务的动作捕捉数据集HHI-Assist，开发基于条件Transformer的去噪扩散模型来预测交互智能体的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动力学，相比基线方法有改进，并在未见场景中表现出强泛化能力

Conclusion: 通过推进交互感知运动预测和引入新数据集，这项工作有潜力显著增强机器人辅助策略

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [76] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: 提出基于强化学习的足式机器人控制方法，通过重力缩放功率优化奖励函数，在月球重力到超地球重力环境下实现高效节能的移动和姿态控制。


<details>
  <summary>Details</summary>
Motivation: 行星探测机器人面临严格的功率和热预算限制，需要能在多种重力环境下高效工作的节能控制方法。

Method: 使用强化学习框架，设计重力缩放功率优化奖励函数，开发移动控制器和基座姿态控制器，并构建恒力弹簧卸载系统进行月球重力实验验证。

Result: 在地球重力下功耗23.4W（比基线提升23%），月球重力下功耗12.2W（比基线降低36%），成功在多种重力环境下实现高效控制。

Conclusion: 该方法为足式机器人提供了跨多种重力环境开发功率高效移动控制器的可扩展解决方案。

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


### [77] [CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion](https://arxiv.org/abs/2509.10139)
*Santiago Montiel-Marín,Angel Llamazares,Miguel Antunes-García,Fabio Sánchez-García,Luis M. Bergasa*

Main category: cs.RO

TL;DR: CaR1是一种新颖的相机-雷达融合架构，用于BEV车辆分割，通过网格化雷达编码和自适应融合机制，在nuScenes数据集上达到57.6 IoU的竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 相机-雷达融合为自动驾驶系统提供了比LiDAR更具成本效益的替代方案，相机提供丰富的语义信息但深度不可靠，雷达提供稀疏但可靠的位置和运动信息。

Method: 基于BEVFusion构建，采用网格化雷达编码将点云离散化为结构化BEV特征，并使用自适应融合机制动态平衡传感器贡献。

Result: 在nuScenes数据集上的实验显示竞争性的分割性能（57.6 IoU），与最先进方法相当。

Conclusion: CaR1展示了相机-雷达融合在BEV车辆分割中的有效性，代码已公开可用。

Abstract: Camera-radar fusion offers a robust and cost-effective alternative to
LiDAR-based autonomous driving systems by combining complementary sensing
capabilities: cameras provide rich semantic cues but unreliable depth, while
radar delivers sparse yet reliable position and motion information. We
introduce CaR1, a novel camera-radar fusion architecture for BEV vehicle
segmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar
encoding that discretizes point clouds into structured BEV features and an
adaptive fusion mechanism that dynamically balances sensor contributions.
Experiments on nuScenes demonstrate competitive segmentation performance (57.6
IoU), on par with state-of-the-art methods. Code is publicly available
\href{https://www.github.com/santimontiel/car1}{online}.

</details>


### [78] [DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning](https://arxiv.org/abs/2509.10247)
*Xinhong Zhang,Runqing Wang,Yunfan Ren,Jian Sun,Hao Fang,Jie Chen,Gang Wang*

Main category: cs.RO

TL;DR: DiffAero是一个轻量级、GPU加速的完全可微分四旋翼飞行器控制策略学习仿真框架，支持环境和代理级并行，集成多种动力学模型和传感器，在消费级硬件上几小时内即可学习到鲁棒的飞行策略。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真器存在CPU-GPU数据传输瓶颈，无法高效支持可微分和混合学习算法的研究，需要开发一个高性能、完全可微分的仿真平台来加速四旋翼控制策略的学习。

Method: 开发了DiffAero框架，完全在GPU上并行化物理和渲染计算，消除CPU-GPU数据传输瓶颈，支持多种动力学模型、可定制传感器堆栈（IMU、深度相机、LiDAR）和多样化飞行任务。

Result: DiffAero实现了数量级的仿真吞吐量提升，结合混合学习算法能够在消费级硬件上几小时内学习到鲁棒的飞行策略，并通过真实飞行实验验证了性能。

Conclusion: DiffAero不仅提供了高性能仿真，还作为研究平台支持可微分和混合学习算法的探索，为四旋翼控制策略学习提供了高效的解决方案。

Abstract: This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully
differentiable simulation framework designed for efficient quadrotor control
policy learning. DiffAero supports both environment-level and agent-level
parallelism and integrates multiple dynamics models, customizable sensor stacks
(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,
GPU-native training interface. By fully parallelizing both physics and
rendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and
delivers orders-of-magnitude improvements in simulation throughput. In contrast
to existing simulators, DiffAero not only provides high-performance simulation
but also serves as a research platform for exploring differentiable and hybrid
learning algorithms. Extensive benchmarks and real-world flight experiments
demonstrate that DiffAero and hybrid learning algorithms combined can learn
robust flight policies in hours on consumer-grade hardware. The code is
available at https://github.com/flyingbitac/diffaero.

</details>


### [79] [GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning](https://arxiv.org/abs/2509.10305)
*Yutong Shen,Ruizhe Xia,Bokai Yan,Shunqi zhang,Pengrui Xiang,Sicheng He,Yixin Xu*

Main category: cs.RO

TL;DR: GundamQ是一个用于机器人路径规划的多尺度时空Q网络，通过时空感知模块和自适应策略优化模块，在动态环境中实现了15.3%的成功率提升和21.7%的路径质量改善


<details>
  <summary>Details</summary>
Motivation: 当前基于深度强化学习的路径规划方法存在两个基本限制：(1) 多尺度时间依赖性建模不足，导致动态场景中的适应性不佳；(2) 探索-利用平衡效率低下，导致路径质量下降

Method: 提出GundamQ框架，包含两个关键模块：(i) 时空感知模块 - 分层提取多粒度空间特征和多尺度时间依赖性；(ii) 自适应策略优化模块 - 平衡探索与利用，通过约束策略更新优化平滑度和碰撞概率

Result: 在动态环境实验中，GundamQ实现了15.3%的成功率提升和21.7%的整体路径质量提高，显著优于现有最先进方法

Conclusion: GundamQ通过有效的多尺度时空建模和自适应策略优化，成功解决了动态环境中机器人路径规划的关键挑战，为复杂环境下的路径规划提供了有效解决方案

Abstract: In dynamic and uncertain environments, robotic path planning demands accurate
spatiotemporal environment understanding combined with robust decision-making
under partial observability. However, current deep reinforcement learning-based
path planning methods face two fundamental limitations: (1) insufficient
modeling of multi-scale temporal dependencies, resulting in suboptimal
adaptability in dynamic scenarios, and (2) inefficient exploration-exploitation
balance, leading to degraded path quality. To address these challenges, we
propose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path
Planning. The framework comprises two key modules: (i) the Spatiotemporal
Perception module, which hierarchically extracts multi-granularity spatial
features and multi-scale temporal dependencies ranging from instantaneous to
extended time horizons, thereby improving perception accuracy in dynamic
environments; and (ii) the Adaptive Policy Optimization module, which balances
exploration and exploitation during training while optimizing for smoothness
and collision probability through constrained policy updates. Experiments in
dynamic environments demonstrate that GundamQ achieves a 15.3\% improvement in
success rate and a 21.7\% increase in overall path quality, significantly
outperforming existing state-of-the-art methods.

</details>


### [80] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 开发了一种混合控制架构，结合多智能体资源管理系统和基于大语言模型的自动行为场景生成，用于人形导游机器人，解决了传统系统手动配置、灵活性低和行为不自然的问题。


<details>
  <summary>Details</summary>
Motivation: 传统导游机器人系统依赖手动调整行为场景，存在配置繁琐、灵活性差和机器人行为不够自然等局限性，需要更自动化和自然的控制方法。

Method: 采用两阶段生成过程：首先生成风格化叙事文本，然后整合非语言动作标签；使用多智能体系统协调并行动作执行和冲突解决，并在主要操作完成后维持默认行为。

Result: 试验结果表明，该方法在自动化社交机器人控制系统方面具有潜力，能够实现更自然的机器人行为。

Conclusion: 提出的混合控制架构成功克服了传统系统的局限性，为社交机器人控制系统的自动化和规模化提供了可行方案。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [81] [Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System](https://arxiv.org/abs/2509.10349)
*Weiyan Lu,Huizhe Li,Yuhao Fang,Zhexuan Zhou,Junda Wu,Yude Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unmanned aerial vehicles (UAVs) with suspended payloads offer significant
advantages for aerial transportation in complex and cluttered environments.
However, existing systems face critical limitations, including unreliable
perception of the cable-payload dynamics, inefficient planning in large-scale
environments, and the inability to guarantee whole-body safety under cable
bending and external disturbances. This paper presents Acetrans, an Autonomous,
Corridor-based, and Efficient UAV suspended transport system that addresses
these challenges through a unified perception, planning, and control framework.
A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and
cable shape under taut and bent modes, enabling robust whole-body state
estimation and real-time filtering of cable point clouds. To enhance planning
scalability, we introduce the Multi-size-Aware Configuration-space Iterative
Regional Inflation (MACIRI) algorithm, which generates safe flight corridors
while accounting for varying UAV and payload geometries. A spatio-temporal,
corridor-constrained trajectory optimization scheme is then developed to ensure
dynamically feasible and collision-free trajectories. Finally, a nonlinear
model predictive controller (NMPC) augmented with cable-bending constraints
provides robust whole-body safety during execution. Simulation and experimental
results validate the effectiveness of Acetrans, demonstrating substantial
improvements in perception accuracy, planning efficiency, and control safety
compared to state-of-the-art methods.

</details>


### [82] [Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States](https://arxiv.org/abs/2509.10405)
*Nicholas Carlotti,Mirko Nava,Alessandro Giusti*

Main category: cs.RO

TL;DR: 提出一种无需位姿标签或机器人形状先验知识的单目RGB相对位姿估计模型，通过LED状态预测任务学习机器人位置、距离和相对方位


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要位姿标签或CAD模型监督的问题，实现无监督的机器人相对位姿估计

Method: 使用配备多个LED的机器人，通过预测图像中LED状态来学习位姿信息，训练时利用已知LED状态和视角方向，推理时LED状态可任意

Result: 与需要监督的方法性能相当，具有良好的泛化能力和多机器人位姿估计能力

Conclusion: 该方法为无监督相对位姿估计提供了有效解决方案，具有实际应用价值

Abstract: We introduce a model for monocular RGB relative pose estimation of a ground
robot that trains from scratch without pose labels nor prior knowledge about
the robot's shape or appearance. At training time, we assume: (i) a robot
fitted with multiple LEDs, whose states are independent and known at each
frame; (ii) knowledge of the approximate viewing direction of each LED; and
(iii) availability of a calibration image with a known target distance, to
address the ambiguity of monocular depth estimation. Training data is collected
by a pair of robots moving randomly without needing external infrastructure or
human supervision. Our model trains on the task of predicting from an image the
state of each LED on the robot. In doing so, it learns to predict the position
of the robot in the image, its distance, and its relative bearing. At inference
time, the state of the LEDs is unknown, can be arbitrary, and does not affect
the pose estimation performance. Quantitative experiments indicate that our
approach: is competitive with SoA approaches that require supervision from pose
labels or a CAD model of the robot; generalizes to different domains; and
handles multi-robot pose estimation.

</details>


### [83] [TASC: Task-Aware Shared Control for Teleoperated Manipulation](https://arxiv.org/abs/2509.10416)
*Ze Fu,Pinhao Song,Yutong Hu,Renaud Detry*

Main category: cs.RO

TL;DR: TASC是一个任务感知共享控制框架，通过视觉输入构建开放词汇交互图来推断用户意图，提供旋转辅助，实现零样本泛化的日常操作任务共享控制。


<details>
  <summary>Details</summary>
Motivation: 解决通用长时程共享控制中的两个关键挑战：理解和推断任务级用户意图，以及在不同对象和任务间泛化辅助功能。

Method: 构建开放词汇交互图表示功能对象关系，使用视觉语言模型预测空间约束，在抓取和对象交互过程中提供旋转辅助的共享控制策略。

Result: 在仿真和真实世界实验中，TASC相比先前方法提高了任务效率并减少了用户输入工作量。

Conclusion: 这是第一个支持零样本泛化的日常操作任务的共享控制框架，代码已公开。

Abstract: We present TASC, a Task-Aware Shared Control framework for teleoperated
manipulation that infers task-level user intent and provides assistance
throughout the task. To support everyday tasks without predefined knowledge,
TASC constructs an open-vocabulary interaction graph from visual input to
represent functional object relationships, and infers user intent accordingly.
A shared control policy then provides rotation assistance during both grasping
and object interaction, guided by spatial constraints predicted by a
vision-language model. Our method addresses two key challenges in
general-purpose, long-horizon shared control: (1) understanding and inferring
task-level user intent, and (2) generalizing assistance across diverse objects
and tasks. Experiments in both simulation and the real world demonstrate that
TASC improves task efficiency and reduces user input effort compared to prior
methods. To the best of our knowledge, this is the first shared control
framework that supports everyday manipulation tasks with zero-shot
generalization. The code that supports our experiments is publicly available at
https://github.com/fitz0401/tasc.

</details>


### [84] [DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training](https://arxiv.org/abs/2509.10426)
*Jianxin Shi,Zengqi Peng,Xiaolong Chen,Tianyu Wo,Jun Ma*

Main category: cs.RO

TL;DR: DECAMP是一个解耦上下文感知的预训练框架，用于多智能体运动预测，通过分离行为模式学习和潜在特征重构，在Argoverse 2基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中轨迹预测的标注数据稀缺问题，以及多智能体预测场景中传统方法性能不佳的挑战。

Method: 提出解耦上下文感知预训练框架DECAMP，分离行为模式学习和潜在特征重构，结合上下文感知表示学习和协作空间运动预训练任务。

Result: 在Argoverse 2基准测试中展示了优越性能，证明其在多智能体运动预测中的有效性。

Conclusion: 这是首个用于自动驾驶多智能体运动预测的上下文自编码器框架，代码和模型将公开提供。

Abstract: Trajectory prediction is a critical component of autonomous driving,
essential for ensuring both safety and efficiency on the road. However,
traditional approaches often struggle with the scarcity of labeled data and
exhibit suboptimal performance in multi-agent prediction scenarios. To address
these challenges, we introduce a disentangled context-aware pre-training
framework for multi-agent motion prediction, named DECAMP. Unlike existing
methods that entangle representation learning with pretext tasks, our framework
decouples behavior pattern learning from latent feature reconstruction,
prioritizing interpretable dynamics and thereby enhancing scene representation
for downstream prediction. Additionally, our framework incorporates
context-aware representation learning alongside collaborative spatial-motion
pretext tasks, which enables joint optimization of structural and intentional
reasoning while capturing the underlying dynamic intentions. Our experiments on
the Argoverse 2 benchmark showcase the superior performance of our method, and
the results attained underscore its effectiveness in multi-agent motion
forecasting. To the best of our knowledge, this is the first context
autoencoder framework for multi-agent motion forecasting in autonomous driving.
The code and models will be made publicly available.

</details>


### [85] [Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction](https://arxiv.org/abs/2509.10444)
*Chaerim Moon,Joohyung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种运动规划层概念，通过修改轨迹来减少超级机器人肢体(SRL)操作时产生的力矩，从而改善人机交互。


<details>
  <summary>Details</summary>
Motivation: 作为可穿戴设备，超级机器人肢体操作时产生的力矩会作为外部扭矩作用于人体，当力矩增大时需要更多肌肉单元来平衡，导致肌肉零空间减少，影响人机交互效果。

Method: 开发了一个运动规划层，在给定的轨迹基础上进行修改，通过限制角加速度和位置偏差来减少产生的力矩。使用简化的人体和机器人系统模型进行仿真验证。

Result: 仿真结果表明该方法能够有效减少操作时产生的力矩。

Conclusion: 提出的运动规划层概念能够通过轨迹优化减少超级机器人肢体产生的力矩，从而增强人机交互性能，为可穿戴机器人系统的力矩控制提供了有效解决方案。

Abstract: Supernumerary Robotic Limbs (SRLs) can enhance human capability within close
proximity. However, as a wearable device, the generated moment from its
operation acts on the human body as an external torque. When the moments
increase, more muscle units are activated for balancing, and it can result in
reduced muscular null space. Therefore, this paper suggests a concept of a
motion planning layer that reduces the generated moment for enhanced
Human-Robot Interaction. It modifies given trajectories with desirable angular
acceleration and position deviation limits. Its performance to reduce the
moment is demonstrated through the simulation, which uses simplified human and
robotic system models.

</details>


### [86] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出了一种无需训练的视觉语言导航框架，通过将导航指令分解为空间约束图并进行约束优化求解，实现零样本连续环境导航


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法主要针对离散环境或在连续模拟器中进行无监督训练，难以泛化到真实世界场景。需要一种无需训练即可在连续环境中工作的框架

Method: 将导航指令分解为显式空间约束，构建空间约束库，建立有向无环图约束，通过约束求解器确定路径点位置，使用导航树和回溯机制处理无解或多解情况

Result: 在标准基准测试中相比最先进的零样本VLN方法显著提高了成功率和导航效率，真实世界实验表明能有效泛化到新环境和指令集

Conclusion: 该框架为构建更鲁棒和自主的导航系统铺平了道路，实现了无需训练即可在连续环境中进行零样本视觉语言导航

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [87] [Minimum Partition of Polygons under Width and Cut Constraints](https://arxiv.org/abs/2509.09981)
*Jaehoon Chung,Kazuo Iwama,Chung-Shou Liao,Hee-Kap Ahn*

Main category: cs.CG

TL;DR: 研究多边形在预定方向切割下的最小分割问题，确保每个子多边形满足宽度约束，分析分割数的单调性并证明Bang猜想的划分版本


<details>
  <summary>Details</summary>
Motivation: 解决多边形分割优化问题，特别是在预定切割方向下找到满足宽度约束的最小分割数，这对于几何分割和覆盖问题有重要理论意义

Method: 分析多边形分割的结构性质，研究分割数在子多边形包含关系下的单调性，证明凸多边形存在最优平行切割方向，并设计线性时间算法

Result: 证明了简单多边形的最小分割数不小于满足特定凸性条件的子多边形分割数，建立了Bang猜想的划分版本，并给出了凸多边形最优分割的线性时间算法

Conclusion: 该研究在多边形分割理论方面取得了重要进展，建立了分割数的单调性性质，证明了经典覆盖猜想的划分对应版本，并为凸多边形提供了高效的最优分割算法

Abstract: We study the problem of partitioning a polygon into the minimum number of
subpolygons using cuts in predetermined directions such that each resulting
subpolygon satisfies a given width constraint. A polygon satisfies the
unit-width constraint for a set of unit vectors if the length of the orthogonal
projection of the polygon on a line parallel to a vector in the set is at most
one. We analyze structural properties of the minimum partition numbers,
focusing on monotonicity under polygon containment. We show that the minimum
partition number of a simple polygon is at least that of any subpolygon,
provided that the subpolygon satisfies a certain orientation-wise convexity
with respect to the polygon. As a consequence, we prove a partition analogue of
Bang's conjecture about coverings of convex regions in the plane: for any
partition of a convex body in the plane, the sum of relative widths of all
parts is at least one. For any convex polygon, there exists a direction along
which an optimal partition is achieved by parallel cuts. Given such a
direction, an optimal partition can be computed in linear time.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [88] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 提出了一种新颖的两阶段生成-估计框架，用于高质量PBR材质生成，通过微调扩散模型生成可平铺纹理，再通过链式分解方案预测SVBRDF通道，实现了高效、高质量且用户可控的材质创建。


<details>
  <summary>Details</summary>
Motivation: 传统材质创建需要艺术家大量时间和专业知识，现有基于视觉基础模型的方法在质量、灵活性和用户控制方面存在不足，需要更好的解决方案。

Method: 两阶段框架：1）生成阶段使用微调扩散模型合成符合用户输入的着色可平铺纹理；2）估计阶段采用链式分解方案，通过单步图像条件扩散模型顺序预测SVBRDF通道。

Result: 方法在质量和性能上优于现有材质生成和估计方法，对生成纹理和真实照片都表现出强鲁棒性，支持文本到材质、图像到材质、结构引导生成和材质编辑等多种应用。

Conclusion: 该框架为PBR材质创建提供了高效、高质量且灵活的解决方案，显著提升了材质生成的用户控制能力和应用多样性。

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>
